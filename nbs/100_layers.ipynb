{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "> Helper function used to build PyTorch timeseries models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai2.torch_core import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def noop(x): return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def same_padding1d(seq_len,ks,stride=1,dilation=1):\n",
    "    assert stride > 0\n",
    "    assert dilation >= 1\n",
    "    effective_ks = (ks - 1) * dilation + 1\n",
    "    out_dim = (seq_len + stride - 1) // stride\n",
    "    p = max(0, (out_dim - 1) * stride + effective_ks - seq_len)\n",
    "    padding_before = p // 2\n",
    "    padding_after = p - padding_before\n",
    "    return padding_before, padding_after\n",
    "\n",
    "class ZeroPad1d(nn.ConstantPad1d):\n",
    "    def __init__(self, padding):\n",
    "        super().__init__(padding, 0.)\n",
    "\n",
    "class ConvSP1d(Module):\n",
    "    \"Conv1d padding='same'\"\n",
    "    def __init__(self,c_in,c_out,ks,stride=1,padding='same',dilation=1,bias=True):\n",
    "        super().__init__()\n",
    "        self.ks, self.stride, self.dilation = ks, stride, dilation\n",
    "        self.conv = nn.Conv1d(c_in,c_out,ks,stride=stride,padding=0,dilation=dilation,bias=bias)\n",
    "        self.zeropad = ZeroPad1d\n",
    "        self.weight = self.conv.weight\n",
    "        self.bias = self.conv.bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        padding = same_padding1d(x.shape[-1],self.ks,stride=self.stride,dilation=self.dilation)\n",
    "        return self.conv(self.zeropad(padding)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Chomp1d(Module):\n",
    "    def __init__(self, chomp_size): \n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x): \n",
    "        return x[:, :, :-self.chomp_size].contiguous()\n",
    "    \n",
    "    \n",
    "class CausalConv1d(Module):\n",
    "    def __init__(self, c_in, c_out, ks, dilation=1, **kwargs):\n",
    "        padding = (ks-1) * dilation\n",
    "        self.conv = nn.Conv1d(c_in, c_out, ks, padding=padding, dilation=dilation, **kwargs)\n",
    "        self.chomp = Chomp1d(padding)\n",
    "    def forward(self, x): \n",
    "        return self.chomp(self.conv(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def convlayer(c_in,c_out,ks=3,padding='same',bias=True,stride=1,\n",
    "              bn_init=False,zero_bn=False,bn_before=True,act_fn=True,**kwargs):\n",
    "    '''conv layer (padding=\"same\") + bn + act'''\n",
    "    if ks % 2 == 1 and padding == 'same': padding = ks // 2\n",
    "    layers = [ConvSP1d(c_in,c_out, ks, bias=bias, stride=stride) if padding == 'same' else \\\n",
    "    nn.Conv1d(c_in,c_out, ks, stride=stride, padding=padding, bias=bias)]\n",
    "    bn = nn.BatchNorm1d(c_out)\n",
    "    if bn_init: nn.init.constant_(bn.weight, 0. if zero_bn else 1.)\n",
    "    if bn_before: layers.append(bn)\n",
    "    if act_fn: layers.append(nn.ReLU())\n",
    "    if not bn_before: layers.append(bn)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CoordConv1D(Module):\n",
    "    def forward(self, x):\n",
    "        bs, _, seq_len = x.size()\n",
    "        cc = torch.arange(seq_len, device=device, dtype=torch.float) / (seq_len - 1)\n",
    "        cc = cc * 2 - 1\n",
    "        cc = cc.repeat(bs, 1, 1)\n",
    "        x = torch.cat([x, cc], dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LambdaPlus(Module):\n",
    "    def __init__(self, func, *args, **kwargs): self.func,self.args,self.kwargs=func,args,kwargs\n",
    "    def forward(self, x): return self.func(x, *self.args, **self.kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Flatten(Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "    \n",
    "class Squeeze(Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "    def forward(self, x): return x.squeeze(dim=self.dim)\n",
    "    \n",
    "class Unsqueeze(Module):\n",
    "    def __init__(self, dim=-1):\n",
    "        self.dim = dim\n",
    "    def forward(self, x): return x.unsqueeze(dim=self.dim)\n",
    "    \n",
    "class YRange(Module):\n",
    "    def __init__(self, y_range:tuple): \n",
    "        self.y_range = y_range\n",
    "    def forward(self, x):\n",
    "        x = F.sigmoid(x)\n",
    "        return x * (self.y_range[1] - self.y_range[0]) + self.y_range[0]\n",
    "    \n",
    "class Temp(Module):\n",
    "    def __init__(self, temp):\n",
    "        self.temp = float(temp)\n",
    "        self.temp = nn.Parameter(torch.Tensor(1).fill_(self.temp).to(device))\n",
    "    def forward(self, x):\n",
    "        return x.div_(self.temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "create_scripts()\n",
    "beep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
