{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.InceptionTimePlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# InceptionTimePlus\n",
    "\n",
    "> This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com modified from:\n",
    "- Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "- Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.models.layers import *\n",
    "from tsai.models.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial PyTorch implementation by Ignacio Oguiza - oguiza@gmail.com modified from:\n",
    "\n",
    "# Fawaz, H. I., Lucas, B., Forestier, G., Pelletier, C., Schmidt, D. F., Weber, J., ... & Petitjean, F. (2019). \n",
    "# InceptionTime: Finding AlexNet for Time Series Classification. arXiv preprint arXiv:1909.04939.\n",
    "# Official InceptionTime tensorflow implementation: https://github.com/hfawaz/InceptionTime\n",
    "\n",
    "    \n",
    "class InceptionModulePlus(Module):\n",
    "    def __init__(self, ni, nf, ks=40, bottleneck=True, padding='same', coord=False, separable=False, dilation=1, stride=1, conv_dropout=0., sa=False, se=None,\n",
    "                 norm='Batch', zero_norm=False, bn_1st=True, act=nn.ReLU, act_kwargs={}):\n",
    "        if isinstance(ks, Integral): ks = [ks // (2**i) for i in range(3)]\n",
    "        ks = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in ks]  # ensure odd ks for padding='same'\n",
    "        bottleneck = False if ni == nf else bottleneck\n",
    "        self.bottleneck = Conv(ni, nf, 1, coord=coord, bias=False) if bottleneck else noop # \n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(len(ks)): self.convs.append(Conv(nf if bottleneck else ni, nf, ks[i], padding=padding, coord=coord, separable=separable,\n",
    "                                                         dilation=dilation**i, stride=stride, bias=False))\n",
    "        self.mp_conv = nn.Sequential(*[nn.MaxPool1d(3, stride=1, padding=1), Conv(ni, nf, 1, coord=coord, bias=False)])\n",
    "        self.concat = Concat()\n",
    "        self.norm = Norm(nf * 4, norm=norm, zero_norm=zero_norm)\n",
    "        self.conv_dropout = nn.Dropout(conv_dropout) if conv_dropout else noop\n",
    "        self.sa = SimpleSelfAttention(nf * 4) if sa else noop\n",
    "        self.act = act(**act_kwargs) if act else noop\n",
    "        self.se = nn.Sequential(SqueezeExciteBlock(nf * 4, reduction=se), BN1d(nf * 4)) if se else noop\n",
    "        \n",
    "        self._init_cnn(self)\n",
    "    \n",
    "    def _init_cnn(self, m):\n",
    "        if getattr(self, 'bias', None) is not None: nn.init.constant_(self.bias, 0)\n",
    "        if isinstance(self, (nn.Conv1d,nn.Conv2d,nn.Conv3d,nn.Linear)): nn.init.kaiming_normal_(self.weight)\n",
    "        for l in m.children(): self._init_cnn(l)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = x\n",
    "        x = self.bottleneck(x)\n",
    "        x = self.concat([l(x) for l in self.convs] + [self.mp_conv(input_tensor)])\n",
    "        x = self.norm(x)\n",
    "        x = self.conv_dropout(x)\n",
    "        x = self.sa(x)\n",
    "        x = self.act(x)\n",
    "        x = self.se(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "@delegates(InceptionModulePlus.__init__)\n",
    "class InceptionBlockPlus(Module):\n",
    "    def __init__(self, ni, nf, residual=True, depth=6, coord=False, norm='Batch', zero_norm=False, act=nn.ReLU, act_kwargs={}, sa=False, se=None, \n",
    "                 keep_prob=1., **kwargs):\n",
    "        self.residual, self.depth = residual, depth\n",
    "        self.inception, self.shortcut, self.act = nn.ModuleList(), nn.ModuleList(), nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.inception.append(InceptionModulePlus(ni if d == 0 else nf * 4, nf, coord=coord, norm=norm, \n",
    "                                                      zero_norm=zero_norm if d % 3 == 2 else False,\n",
    "                                                      act=act if d % 3 != 2 else None, act_kwargs=act_kwargs, \n",
    "                                                      sa=sa if d % 3 == 2 else False,\n",
    "                                                      se=se if d % 3 != 2 else None,\n",
    "                                                      **kwargs))\n",
    "            if self.residual and d % 3 == 2:\n",
    "                n_in, n_out = ni if d == 2 else nf * 4, nf * 4\n",
    "                self.shortcut.append(Norm(n_in, norm=norm) if n_in == n_out else ConvBlock(n_in, n_out, 1, coord=coord, bias=False, norm=norm, act=None))\n",
    "                self.act.append(act(**act_kwargs))\n",
    "        self.add = Add()\n",
    "        self.keep_prob = keep_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = x\n",
    "        for i in range(self.depth):\n",
    "            if self.training and self.keep_prob[i//3] < 1. and self.keep_prob[i//3] < random.random() and self.residual and i % 3 == 2: \n",
    "                res = x = self.act[i//3](self.shortcut[i//3](res))\n",
    "            else: \n",
    "                x = self.inception[i](x)\n",
    "                if self.residual and i % 3 == 2: res = x = self.act[i//3](self.add(x, self.shortcut[i//3](res)))\n",
    "        return x\n",
    "\n",
    "\n",
    "@delegates(InceptionModulePlus.__init__)\n",
    "class InceptionTimePlus(Module):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, nf=32, nb_filters=None, concat_pool=False, fc_dropout=0., depth=6, stoch_depth=1., y_range=None, \n",
    "                 flatten=False, custom_head=None, **kwargs):\n",
    "        \n",
    "        nf = ifnone(nf, nb_filters) # for compatibility\n",
    "        self.fc_dropout, self.c_out, self.y_range = fc_dropout, c_out, y_range\n",
    "        self.c_out = c_out\n",
    "        \n",
    "        if stoch_depth is not 0: keep_prob = np.linspace(1, stoch_depth, depth // 3)\n",
    "        else: keep_prob = np.array([1] * depth // 3)\n",
    "        self.inceptionblock = InceptionBlockPlus(c_in, nf, depth=depth, keep_prob=keep_prob, **kwargs)\n",
    "        \n",
    "        self.head_nf = nf * 4\n",
    "        self.flatten = None\n",
    "        if flatten:  self.head_nf *= seq_len\n",
    "        self.flatten = Flatten() if flatten else None\n",
    "        if custom_head: self.head = custom_head(self.head_nf, c_out)\n",
    "        else: self.head = self.create_head(self.head_nf, c_out, concat_pool=concat_pool, fc_dropout=fc_dropout, y_range=y_range)\n",
    "        \n",
    "    def create_head(self, nf, c_out, concat_pool=False, fc_dropout=0., y_range=None, **kwargs):\n",
    "        if concat_pool: nf = nf * 2\n",
    "        layers = [GACP1d(1) if concat_pool else GAP1d(1)]\n",
    "        if fc_dropout: layers += [nn.Dropout(fc_dropout)]\n",
    "        layers += [nn.Linear(nf, c_out)]\n",
    "        if y_range: layers += [SigmoidRange(*y_range)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.inceptionblock(x)\n",
    "        if self.flatten is not None: x = self.flatten(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class InCoordTime(InceptionTimePlus):\n",
    "    def __init__(self, *args, coord=True, zero_norm=True, **kwargs):\n",
    "        super().__init__(*args, coord=coord, zero_norm=zero_norm, **kwargs)\n",
    "\n",
    "\n",
    "class XCoordTime(InceptionTimePlus):\n",
    "    def __init__(self, *args, coord=True, separable=True, zero_norm=True, **kwargs):\n",
    "        super().__init__(*args, coord=coord, separable=separable, zero_norm=zero_norm, **kwargs)\n",
    "        \n",
    "InceptionTimePlus17x17 = partial(InceptionTimePlus, nf=17, depth=3)\n",
    "setattr(InceptionTimePlus17x17, '__name__', 'InceptionTimePlus17x17')\n",
    "InceptionTimePlus32x32 = InceptionTimePlus\n",
    "InceptionTimePlus47x47 = partial(InceptionTimePlus, nf=47, depth=9)\n",
    "setattr(InceptionTimePlus47x47, '__name__', 'InceptionTimePlus47x47')\n",
    "InceptionTimePlus62x62 = partial(InceptionTimePlus, nf=62, depth=9)\n",
    "setattr(InceptionTimePlus62x62, '__name__', 'InceptionTimePlus62x62')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@delegates(InceptionTimePlus.__init__)\n",
    "class MultiInceptionTimePlus(Module):\n",
    "    _arch = InceptionTimePlus\n",
    "    def __init__(self, feat_mask, c_out, **kwargs):\n",
    "        r\"\"\"\n",
    "        MultiInceptionTimePlus is a class that allows you to create a model with multiple branches of InceptionTimePlus.\n",
    "        \n",
    "        Args:\n",
    "            - feat_mask: list with number of features that will be passed to each body.\n",
    "        \"\"\"\n",
    "        self.feat_mask = [feat_mask] if isinstance(feat_mask, int) else feat_mask \n",
    "        self.c_out, self.kwargs = c_out, kwargs\n",
    "        \n",
    "        # Body\n",
    "        self.branches = nn.ModuleList()\n",
    "        self.head_nf = 0\n",
    "        for feat in self.feat_mask:\n",
    "            m = create_model(self._arch, c_in=feat, c_out=c_out, **kwargs)\n",
    "            self.head_nf += m.head_nf\n",
    "            m.head = Noop\n",
    "            self.branches.append(m)\n",
    "        \n",
    "        # Head\n",
    "        self.head = self._arch.create_head(self, self.head_nf, c_out, **kwargs)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.split(x, self.feat_mask, dim=1)\n",
    "        for i, branch in enumerate(self.branches):\n",
    "            out = branch(x[i]) if i == 0 else torch.cat([out, branch(x[i])], dim=1)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "\n",
    "test_eq(InceptionTimePlus(n_vars,c_out)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out,concat_pool=True)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, bottleneck=False)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, residual=False)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, conv_dropout=.5)(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True, \n",
    "                          norm='Instance', zero_norm=True, bn_1st=False, fc_dropout=.5, sa=True, se=True, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\n",
    "test_eq(InceptionTimePlus(n_vars,c_out, coord=True, separable=True,\n",
    "                          norm='Instance', zero_norm=True, bn_1st=False, act=nn.PReLU, act_kwargs={})(xb).shape, [bs, c_out])\n",
    "test_eq(total_params(InceptionTimePlus(3, 2))[0], 455490)\n",
    "test_eq(total_params(InceptionTimePlus(6, 2, **{'coord': True, 'separable': True, 'zero_norm': True}))[0], 77204)\n",
    "test_eq(total_params(InceptionTimePlus(3, 2, ks=40))[0], total_params(InceptionTimePlus(3, 2, ks=[9, 19, 39]))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "InceptionTimePlus(n_vars, c_out, seq_len=seq_len, zero_norm=True, flatten=True, custom_head=create_mlp_head)(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(check_bias(InceptionTimePlus(2,3, zero_norm=True), is_conv)[0].sum(), 0)\n",
    "test_eq(check_weight(InceptionTimePlus(2,3, zero_norm=True), is_bn)[0].sum(), 6)\n",
    "test_eq(check_weight(InceptionTimePlus(2,3), is_bn)[0], np.array([1., 1., 1., 1., 1., 1., 1., 1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(count_parameters(MultiInceptionTimePlus([1,1,1], c_out)) > count_parameters(MultiInceptionTimePlus(3, c_out)), True)\n",
    "test_eq(MultiInceptionTimePlus([1,1,1], c_out)(xb).shape, MultiInceptionTimePlus(3, c_out)(xb).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10): InceptionTimePlus(n_vars,c_out,stoch_depth=0.8,depth=9,zero_norm=True)(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InceptionTimePlus(\n",
       "  (inceptionblock): InceptionBlockPlus(\n",
       "    (inception): ModuleList(\n",
       "      (0): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(3, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (1): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (2): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (4): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (5): InceptionModulePlus(\n",
       "        (bottleneck): ConvBlock(\n",
       "          (0): AddCoords1d()\n",
       "          (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        )\n",
       "        (convs): ModuleList(\n",
       "          (0): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(39,), stride=(1,), padding=(19,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(19,), stride=(1,), padding=(9,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "          (2): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): SeparableConv1d(\n",
       "              (depthwise_conv): Conv1d(33, 33, kernel_size=(9,), stride=(1,), padding=(4,), groups=33, bias=False)\n",
       "              (pointwise_conv): Conv1d(33, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mp_conv): Sequential(\n",
       "          (0): MaxPool1d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
       "          (1): ConvBlock(\n",
       "            (0): AddCoords1d()\n",
       "            (1): Conv1d(129, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "          )\n",
       "        )\n",
       "        (concat): Concat(1)\n",
       "        (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (shortcut): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (0): AddCoords1d()\n",
       "        (1): Conv1d(3, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0): ReLU()\n",
       "      (1): ReLU()\n",
       "    )\n",
       "    (add): Add\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): GAP1d(\n",
       "      (gap): AdaptiveAvgPool1d(output_size=1)\n",
       "      (flatten): Flatten(full=False)\n",
       "    )\n",
       "    (1): Linear(in_features=128, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = InceptionTimePlus(2,3,**{'coord': True, 'separable': True, 'zero_norm': True})\n",
    "test_eq(check_weight(net, is_bn)[0], np.array([1., 1., 0., 1., 1., 0., 1., 1.]))\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AdaptiveAvgPool1d(output_size=1)\n",
       "  (1): Flatten(full=False)\n",
       "  (2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): Linear(in_features=128, out_features=512, bias=False)\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (6): Linear(in_features=512, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "n_vars = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, n_vars, seq_len)\n",
    "net = MultiInceptionTimePlus(n_vars, c_out)\n",
    "change_model_head(net, create_pool_plus_head, concat_pool=False)\n",
    "print(net(xb).shape)\n",
    "net.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
