{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Main Numpy and Times Series functions used throughout the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "# import torch\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from fastai2.imports import *\n",
    "# from fastai2.torch_core import *\n",
    "# from fastai2.data.core import *\n",
    "# from fastai2.learner import Learner\n",
    "# from fastcore.transform import *\n",
    "# from fastai2.vision.data import *\n",
    "# from fastai2.data.transforms import *\n",
    "# from fastai2.metrics import *\n",
    "from tsai.utils import *\n",
    "from tsai.data import *\n",
    "from tsai.models.all import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: NATOPS\n",
      "X_train: (180, 24, 51)\n",
      "y_train: (180,)\n",
      "X_valid: (180, 24, 51)\n",
      "y_valid: (180,) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((#180) [0,1,2,3,4,5,6,7,8,9...],\n",
       " (#180) [180,181,182,183,184,185,186,187,188,189...])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsid = 'NATOPS'\n",
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid, parent_dir='./data/UCR/', verbose=True, on_disk=True)\n",
    "X = np.concatenate((X_train, X_valid))\n",
    "y = np.concatenate((y_train, y_valid))\n",
    "splits = (L(list(np.arange(len(X_train)))), L(list(np.arange(len(X_train), len(X)))))\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyTensor(TensorBase):\n",
    "    \"Returns a `tensor` of type torch.float32 and class `NumpyTensor` that has a show method\"\n",
    "    def __new__(cls, o, **kwargs): \n",
    "        if isinstance(o, (list, L)): o = np.stack(o)\n",
    "        res = tensor(o)\n",
    "        res.__class__ = cls\n",
    "        res._meta = kwargs\n",
    "        return res\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        return type(self)(res)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'NumpyTensor(shape:{list(self.shape)})'\n",
    "\n",
    "    def show(self, ax=None, ctx=None, title=None, title_color='black', **kwargs):\n",
    "        if self.ndim != 2: self = type(self)(To2DTensor(self))\n",
    "        ax = ifnone(ax,ctx)\n",
    "        if ax is None: fig, ax = plt.subplots(**kwargs)\n",
    "        ax.plot(self.T)\n",
    "        ax.axis(xmin=0, xmax=self.shape[-1] - 1)\n",
    "        ax.set_title(title, weight='bold', color=title_color)\n",
    "        plt.tight_layout()\n",
    "        return ax\n",
    "\n",
    "class ToNumpyTensor(Transform):\n",
    "    \"Transforms np.ndarray to NumpyTensor\"\n",
    "    def encodes(self, o:np.ndarray): return NumpyTensor(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "a = np.random.rand(2,3,4)\n",
    "test_eq(NumpyTensor(a).shape, (2,3,4))\n",
    "test_eq(ToNumpyTensor()(a).shape, (2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSTensor(NumpyTensor):\n",
    "    '''Returns a tensor oftype torch.float32 and class TSTensor that has a show method'''\n",
    "\n",
    "    @property\n",
    "    def vars(self): return self.shape[-2]\n",
    "\n",
    "    @property\n",
    "    def len(self): return self.shape[-1]\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.ndim >= 3:   return f'TSTensor(samples:{self.shape[-3]}, vars:{self.shape[-2]}, len:{self.shape[-1]})'\n",
    "        elif self.ndim == 2: return f'TSTensor(vars:{self.shape[-2]}, len:{self.shape[-1]})'\n",
    "        elif self.ndim == 1: return f'TSTensor(len:{self.shape[-1]})'\n",
    "        else: return f'TSTensor({self.dtype})'\n",
    "\n",
    "class ToTSTensor(Transform):\n",
    "    def encodes(self, o:np.ndarray): return TSTensor(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TSTensor(samples:2, vars:3, len:4),\n",
       " TSTensor(vars:3, len:4),\n",
       " TSTensor(len:4),\n",
       " TSTensor(torch.float32))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.rand(2,3,4)\n",
    "test_eq(TSTensor(a).shape, (2,3,4))\n",
    "test_eq(ToTSTensor()(a).shape, (2,3,4))\n",
    "TSTensor(a), TSTensor(a[0]), TSTensor(a[0,0]), TSTensor(a[0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyTensorBlock():\n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "        self.type_tfms  =                 L(type_tfms)\n",
    "        self.item_tfms  = ToNumpyTensor + L(item_tfms)\n",
    "        self.batch_tfms =                 L(batch_tfms)\n",
    "        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n",
    "        \n",
    "class TSTensorBlock():\n",
    "    def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "        self.type_tfms  =              L(type_tfms)\n",
    "        self.item_tfms  = ToTSTensor + L(item_tfms)\n",
    "        self.batch_tfms =              L(batch_tfms)\n",
    "        self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(NumpyTensorBlock().item_tfms[0].__name__, 'ToNumpyTensor')\n",
    "test_eq(TSTensorBlock().item_tfms[0].__name__, 'ToTSTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, X, y): self.X, self.y = X, y\n",
    "    def __getitem__(self, idx): return (self.X[idx], self.y[idx])\n",
    "    def __len__(self): return len(self.X)\n",
    "\n",
    "class NumpyDataset():\n",
    "    def __init__(self, X, y=None, types=None): self.X, self.y, self.types = X, y, types\n",
    "    def __getitem__(self, idx): \n",
    "        if self.types is None: return (self.X[idx], self.y[idx]) if self.y is not None else (self.X[idx])\n",
    "        else: return (self.types[0](self.X[idx]), self.types[1](self.y[idx])) if self.y is not None else (self.types[0](self.X[idx]))\n",
    "    def __len__(self): return len(self.X)\n",
    "    @property\n",
    "    def c(self): return 0 if self.y is None else 1 if isinstance(self.y[0], float) else len(np.unique(self.y)) \n",
    "\n",
    "class TSDataset():\n",
    "    def __init__(self, X, y=None, types=None, sel_vars=None, sel_steps=None): \n",
    "        self.X, self.y, self.types = X, y, types\n",
    "        self.sel_vars = ifnone(sel_vars, slice(None))\n",
    "        self.sel_steps = ifnone(sel_steps,slice(None))\n",
    "    def __getitem__(self, idx): \n",
    "        if self.types is None: return (self.X[idx, self.sel_vars, self.sel_steps], self.y[idx]) if self.y is not None else (self.X[idx])\n",
    "        else: return (self.types[0](self.X[idx, self.sel_vars, self.sel_steps]), self.types[1](self.y[idx])) if self.y is not None else (self.types[0](self.X[idx]))\n",
    "    def __len__(self): return len(self.X)\n",
    "    @property\n",
    "    def c(self): return 0 if self.y is None else 1 if isinstance(self.y[0], float) else len(np.unique(self.y)) \n",
    "    @property\n",
    "    def vars(self): return self[0][0].shape[-2]\n",
    "    @property\n",
    "    def len(self): return self[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(5,6,7)\n",
    "b = np.random.rand(5)\n",
    "ds = NumpyDataset(a,b)\n",
    "xb, yb = ds[[0,4]]\n",
    "test_eq(xb.shape, (2,6,7))\n",
    "test_eq(yb.shape, (2,))\n",
    "test_eq(ds.c, 1)\n",
    "\n",
    "a = np.random.rand(5,6,7)\n",
    "b = np.random.randint(0, 2, 5)\n",
    "ds = TSDataset(a,b)\n",
    "test_eq(ds.c, 2)\n",
    "test_eq(ds.vars, 6)\n",
    "test_eq(ds.len, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyDatasets(Datasets):\n",
    "    \"A dataset that creates tuples from X (and y) and applies `item_tfms`\"\n",
    "    _xtype, _ytype = NumpyTensor, None # Expected X and y output types (must have a show method)\n",
    "    def __init__(self, X=None, y=None, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, inplace=False, **kwargs):\n",
    "        self.inplace = inplace\n",
    "        if tls is None: \n",
    "            X = itemify(X, tup_id=0)\n",
    "            y = itemify(y, tup_id=0) if y is not None else y\n",
    "            items = tuple((X)) if y is None else tuple((X,y))\n",
    "            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))\n",
    "        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "        if len(self.tls[0]) > 0: \n",
    "            # type(tl[0]).__name__ == 'memmap' is added to avoid loading in memory larger than RAM datasets\n",
    "            self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == 'memmap' else stack(tl[:]) for tl in self.tls])\n",
    "            self.types = [ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for tl,_typ in zip(self.tls, [self._xtype, self._ytype])]\n",
    "    \n",
    "    def __getitem__(self, it):\n",
    "        return tuple([typ(ptl[it] if i==0 else ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n",
    "    \n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp, inplace=self.inplace, tfms=self.tfms)\n",
    "    \n",
    "    def _new(self, X, *args, y=None, **kwargs): \n",
    "        items = ifnoneelse(y,tuple((X)),tuple((X, y)))\n",
    "        return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    \n",
    "    def show_at(self, idx, **kwargs):\n",
    "        self.show(self[idx], **kwargs)\n",
    "        plt.show()\n",
    "\n",
    "    @property\n",
    "    def items(self): return tuple([tl.items for tl in self.tls])\n",
    "    @items.setter\n",
    "    def items(self, vs):\n",
    "        for tl,c in zip(self.tls, vs): tl.items = v\n",
    "\n",
    "\n",
    "class TSDatasets(NumpyDatasets):\n",
    "    \"A dataset that creates tuples from X (and y) and applies `item_tfms`\"\n",
    "    _xtype, _ytype = TSTensor, None # Expected X and y output types (torch.Tensor - default - or subclass)\n",
    "    def __init__(self, X=None, y=None, items=None, sel_vars=None, sel_steps=None, tfms=None, tls=None, n_inp=None, dl_type=None, \n",
    "                 inplace=False, **kwargs):\n",
    "        self.inplace = inplace\n",
    "        if tls is None: \n",
    "            X = itemify(X, tup_id=0)\n",
    "            y = itemify(y, tup_id=0) if y is not None else y\n",
    "            items = tuple((X)) if y is None else tuple((X,y))\n",
    "            self.tfms = L(ifnone(tfms,[None]*len(ifnone(tls,items))))\n",
    "        self.sel_vars = ifnone(sel_vars, slice(None))\n",
    "        self.sel_steps = ifnone(sel_steps,slice(None))\n",
    "        self.tls = L(tls if tls else [TfmdLists(item, t, **kwargs) for item,t in zip(items,self.tfms)])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "        if len(self.tls[0]) > 0: \n",
    "            self.ptls = L([tl if not self.inplace else tl[:] if type(tl[0]).__name__ == 'memmap' else stack(tl[:]) for tl in self.tls])\n",
    "            self.types = [ifnone(_typ, type(tl[0]) if isinstance(tl[0], torch.Tensor) else tensor) for tl,_typ in zip(self.tls, [self._xtype, self._ytype])]\n",
    "    \n",
    "    def __getitem__(self, it):\n",
    "        return tuple([typ(ptl[it])[...,self.sel_vars, self.sel_steps] if i==0 else typ(ptl[it]) for i,(ptl,typ) in enumerate(zip(self.ptls,self.types))])\n",
    "    \n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp, \n",
    "                                           inplace=self.inplace, tfms=self.tfms, sel_vars=self.sel_vars, sel_steps=self.sel_steps)\n",
    "    @property\n",
    "    def vars(self): return self[0][0].shape[-2]\n",
    "    @property\n",
    "    def len(self): return self[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(np.stack(itemify(y, tup_id=0)), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = TSDatasets(X, y, tfms=None, splits=splits, inplace=False)\n",
    "test_eq(len(dsets.train), len(X_train))\n",
    "dsets = TSDatasets(X, y, tfms=None, splits=splits, inplace=True)\n",
    "test_eq(len(dsets.train), len(X_train))\n",
    "dsets = TSDatasets(X, y, tfms=[add(1), Categorize()], splits=splits, inplace=True)\n",
    "test_eq(len(dsets.train), len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def add_ds(dsets, X, y=None, test_items=None, rm_tfms=None, with_labels=False):\n",
    "    \"Create test datasets from X (and y) using validation transforms of `dsets`\"\n",
    "    items = ifnoneelse(y,tuple((X,)),tuple((X, y)))\n",
    "    with_labels = ifnoneelse(y,False,True) \n",
    "    if isinstance(dsets, (Datasets, NumpyDatasets, TSDatasets)):\n",
    "        tls = dsets.tls if with_labels else dsets.tls[:dsets.n_inp]\n",
    "        new_tls = L([tl._new(item, split_idx=1) for tl,item in zip(tls, items)])\n",
    "        if rm_tfms is None: rm_tfms = [tl.infer_idx(get_first(item)) for tl,item in zip(new_tls, items)]\n",
    "        else:               rm_tfms = tuplify(rm_tfms, match=new_tls)\n",
    "        for i,j in enumerate(rm_tfms): new_tls[i].tfms.fs = new_tls[i].tfms.fs[j:]\n",
    "        if isinstance(dsets, (NumpyDatasets, TSDatasets)):\n",
    "            cls = dsets.__class__\n",
    "            return cls(tls=new_tls, n_inp=dsets.n_inp, inplace=dsets.inplace, tfms=dsets.tfms, sel_vars=dsets.sel_vars, sel_steps=dsets.sel_steps)\n",
    "        elif isinstance(dsets, Datasets): return Datasets(tls=new_tls)\n",
    "    elif isinstance(dsets, TfmdLists):\n",
    "        new_tl = dsets._new(items, split_idx=1)\n",
    "        if rm_tfms is None: rm_tfms = dsets.infer_idx(get_first(items))\n",
    "        new_tl.tfms.fs = new_tl.tfms.fs[rm_tfms:]\n",
    "        return new_tl\n",
    "    else: raise Exception(f\"This method requires using the fastai library to assemble your data.Expected a `Datasets` or a `TfmdLists` but got {dsets.__class__.__name__}\")\n",
    "\n",
    "@patch\n",
    "def add_test(self:NumpyDatasets, X, y=None, test_items=None, rm_tfms=None, with_labels=False):\n",
    "    return add_ds(self, X, y=y, test_items=test_items, rm_tfms=rm_tfms, with_labels=with_labels)\n",
    "\n",
    "@patch\n",
    "def add_unlabeled(self:NumpyDatasets, X, test_items=None, rm_tfms=None, with_labels=False):\n",
    "    return add_ds(self, X, y=None, test_items=test_items, rm_tfms=rm_tfms, with_labels=with_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = TSDatasets(X, y, tfms=[None, Categorize()], splits=splits, inplace=True)\n",
    "test_eq(len(dsets.add_test(X_train, y_train)), len(X_train))\n",
    "test_eq(len(dsets.add_unlabeled(X_train)), len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class NumpyDataLoader(TfmdDL):\n",
    "    do_item = noops\n",
    "    def create_batch(self, b): return self.dataset[b]\n",
    "\n",
    "    @delegates(plt.subplots)\n",
    "    def show_batch(self, b=None, ctxs=None, max_n=9, nrows=3, ncols=3, figsize=(16, 10), **kwargs):\n",
    "        b = self.one_batch()\n",
    "        db = self.decode_batch(b, max_n=max_n)\n",
    "        if figsize is None: figsize = (ncols*6, max_n//ncols*4)\n",
    "        if ctxs is None: ctxs = get_grid(min(len(db), nrows*ncols), nrows=None, ncols=ncols, figsize=figsize, **kwargs)\n",
    "        for i,ctx in enumerate(ctxs):\n",
    "            show_tuple(db[i], ctx=ctx)\n",
    "\n",
    "    @delegates(plt.subplots)\n",
    "    def show_results(self, b, preds, ctxs=None, max_n=9, nrows=3, ncols=3, figsize=(16, 10), **kwargs):\n",
    "        t = self.decode_batch(b, max_n=max_n)\n",
    "        p = self.decode_batch((b[0],preds), max_n=max_n)\n",
    "        if figsize is None: figsize = (ncols*6, max_n//ncols*4)\n",
    "        if ctxs is None: ctxs = get_grid(min(len(t), nrows*ncols), nrows=None, ncols=ncols, figsize=figsize, **kwargs)\n",
    "        for i,ctx in enumerate(ctxs): \n",
    "            title = f'True: {t[i][1]}\\nPred: {p[i][1]}'\n",
    "            color = 'green' if t[i][1] == p[i][1] else 'red'\n",
    "            t[i][0].show(ctx=ctx, title=title, title_color=color)\n",
    "\n",
    "@delegates(plt.subplots)\n",
    "def show_tuple(tup, **kwargs):\n",
    "    \"Display a timeseries plot from a decoded tuple\"\n",
    "    tup[0].show(title='unlabeled' if len(tup) == 1 else tup[1], **kwargs)\n",
    "    \n",
    "class TSDataLoader(NumpyDataLoader): \n",
    "    @property\n",
    "    def vars(self): return self.dataset[0][0].shape[-2]\n",
    "    \n",
    "    @property\n",
    "    def len(self): return self.dataset[0][0].shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "dsets = TSDatasets(X, y, tfms=[add(1), Categorize()], splits=splits, inplace=True)\n",
    "xb,yb = TSDataLoader(dsets.train, bs=bs).one_batch()\n",
    "test_eq(xb.shape, (bs, X.shape[-2], X.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "_batch_tfms = ('after_item','before_batch','after_batch')\n",
    "\n",
    "class NumpyDataLoaders(DataLoaders):\n",
    "    _xblock = NumpyTensorBlock\n",
    "    _dl_type = NumpyDataLoader \n",
    "    def __init__(self, *loaders, path='.', device=default_device()):\n",
    "        self.loaders,self.path = list(loaders),Path(path)\n",
    "        self.device = device\n",
    "        \n",
    "    @classmethod\n",
    "    @delegates(DataLoaders.from_dblock)\n",
    "    def from_numpy(cls, X, y=None, splitter=None, valid_pct=0.2, seed=0, item_tfms=None, batch_tfms=None, **kwargs):\n",
    "        \"Create timeseries dataloaders from arrays (X and y, unless unlabeled)\"\n",
    "        if splitter is None: splitter = RandomSplitter(valid_pct=valid_pct, seed=seed)\n",
    "        getters = [ItemGetter(0), ItemGetter(1)] if y is not None else [ItemGetter(0)]\n",
    "        dblock = DataBlock(blocks=(cls._xblock, CategoryBlock),\n",
    "                           getters=getters,\n",
    "                           splitter=splitter,\n",
    "                           item_tfms=item_tfms,\n",
    "                           batch_tfms=batch_tfms)\n",
    "\n",
    "        source = itemify(X) if y is None else itemify(X,y)\n",
    "        return cls.from_dblock(dblock, source, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_dsets(cls, *ds, path='.',  bs=64, device=None, **kwargs):\n",
    "        default = (True,) + (False,) * (len(ds)-1)\n",
    "        defaults = {'shuffle': default, 'drop_last': default}\n",
    "        for nm in _batch_tfms:\n",
    "            if nm in kwargs: kwargs[nm] = Pipeline(kwargs[nm])\n",
    "        kwargs = merge(defaults, {k: tuplify(v, match=ds) for k,v in kwargs.items()})\n",
    "        kwargs = [{k: v[i] for k,v in kwargs.items()} for i in range_of(ds)]\n",
    "        if not is_listy(bs): bs = [bs]\n",
    "        if len(bs) != len(ds): bs = bs * len(ds)\n",
    "        assert len(ds) == len(kwargs) == len(bs)\n",
    "        if device == None: device = default_device()\n",
    "        return cls(*[cls._dl_type(d, bs=b, **k) for d,k,b in zip(ds, kwargs, bs)], path=path, device=device)\n",
    "\n",
    "class TSDataLoaders(NumpyDataLoaders):\n",
    "    _xblock = TSTensorBlock\n",
    "    _dl_type = TSDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "dsets = TSDatasets(X, y, tfms=[add(1), Categorize()], splits=RandomSplitter(valid_pct=.3)(y), inplace=True)\n",
    "train_dl = TSDataLoader(dsets.train, bs=bs)\n",
    "valid_dl = TSDataLoader(dsets.valid, bs=bs*2)\n",
    "dls = TSDataLoaders(train_dl, valid_dl)\n",
    "xb,yb = dls.train.one_batch()\n",
    "test_eq(xb.shape, (min(bs, len(dsets.train)), X.shape[-2], X.shape[-1]))\n",
    "xb,yb = dls.valid.one_batch()\n",
    "test_eq(xb.shape, (min(bs*2, len(dsets.valid)), X.shape[-2], X.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_all(self:Learner, path='export', dls_fname='dls', model_fname='model', learner_fname='learner'):\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "\n",
    "    # Save the dls\n",
    "    torch.save(self.dls, path/dls_fname)\n",
    "\n",
    "    # Saves the model along with optimizer\n",
    "    self.model_dir = path\n",
    "    self.save(model_fname)\n",
    "\n",
    "    # Export learn without the items and the optimizer state for inference\n",
    "    self.export(path/f'{learner_fname}.pkl')\n",
    "    \n",
    "    print(f'Learner saved:')\n",
    "    print(f\"path          = '{path}'\")\n",
    "    print(f\"dls_fname     = '{dls_fname}'\")\n",
    "    print(f\"model_fname   = '{model_fname}.pth'\")\n",
    "    print(f\"learner_fname = '{learner_fname}.pkl'\")\n",
    "    \n",
    "Learner.save_all = save_all\n",
    "    \n",
    "    \n",
    "def load_learner_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner', cpu=True):\n",
    "    path = Path(path)\n",
    "    learn = load_learner(path/f'{learner_fname}.pkl', cpu=cpu)\n",
    "    learn.load(f'{model_fname}')\n",
    "    dls = torch.load(path/dls_fname)\n",
    "    learn.dls = dls\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_all(self, path='export', dls_fname='dls', model_fname='model', learner_fname='learner'):\n",
    "    \n",
    "    path = Path(path)\n",
    "    if not os.path.exists(path): os.makedirs(path)\n",
    "    print(path)\n",
    "\n",
    "    # Save the dls\n",
    "    torch.save(self.dls, path/f'{dls_fname}.pth')\n",
    "\n",
    "    # Saves the model along with optimizer\n",
    "    self.model_dir = path\n",
    "    self.save(model_fname)\n",
    "\n",
    "    # Export learn without the items and the optimizer state for inference\n",
    "    self.export(path/f'{learner_fname}.pkl')\n",
    "    \n",
    "    print(f'Learner saved:')\n",
    "    print(f\"path          = '{path}'\")\n",
    "    print(f\"dls_fname     = '{dls_fname}'\")\n",
    "    print(f\"model_fname   = '{model_fname}.pth'\")\n",
    "    print(f\"learner_fname = '{learner_fname}.pkl'\")\n",
    "    \n",
    "Learner.save_all = save_all\n",
    "    \n",
    "    \n",
    "def load_learner_all(path='export', dls_fname='dls', model_fname='model', learner_fname='learner', cpu=True):\n",
    "    path = Path(path)\n",
    "    learn = load_learner(path/f'{learner_fname}.pkl', cpu=cpu)\n",
    "    learn.load(f'{model_fname}')\n",
    "    dls = torch.load(path/f'{dls_fname}.pth')\n",
    "    learn.dls = dls\n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.763685</td>\n",
       "      <td>1.789343</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.390087</td>\n",
       "      <td>1.754120</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.119945</td>\n",
       "      <td>1.723962</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>00:05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits, inplace=True)\n",
    "train_dl = TSDataLoader(dsets.train, bs=64, shuffle=True, drop_last=True, num_workers=0)\n",
    "valid_dl = TSDataLoader(dsets.valid, bs=128, num_workers=0)\n",
    "dls   = TSDataLoaders(train_dl, valid_dl, device=default_device())\n",
    "model = InceptionTime(dls.vars, dls.c)\n",
    "learn = Learner(dls, model, metrics=accuracy)\n",
    "learn.fit_one_cycle(1, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export\n",
      "Learner saved:\n",
      "path          = 'export'\n",
      "dls_fname     = 'dls'\n",
      "model_fname   = 'model.pth'\n",
      "learner_fname = 'learner.pkl'\n"
     ]
    }
   ],
   "source": [
    "learn.save_all()\n",
    "del learn\n",
    "learn = load_learner_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint()"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Current notebook saved.\n",
      "\n",
      "Converted 000_utils.ipynb.\n",
      "Converted 001_data.ipynb.\n",
      "Converted 002_core.ipynb.\n",
      "Converted 100_layers.ipynb.\n",
      "Converted 101_ResNet.ipynb.\n",
      "Converted 102_InceptionTime.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "Correct conversion!\n",
      "Total elapsed time 0 s\n",
      "10-04-2020 13:13:34\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from save_nb import *\n",
    "from nbdev.export import *\n",
    "save_nb()\n",
    "notebook2script()\n",
    "last_saved(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
