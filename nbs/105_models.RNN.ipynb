{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp models.RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">These are RNN, LSTM and GRU PyTorch implementations created by Ignacio Oguiza - timeseriesAI@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from tsai.imports import *\n",
    "from tsai.models.layers import *\n",
    "from tsai.models.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class _RNN_Base(Module):\n",
    "    def __init__(self, c_in, c_out, hidden_size=100, n_layers=1, bias=True, rnn_dropout=0, bidirectional=False, fc_dropout=0.):\n",
    "        self.rnn = self._cell(c_in, hidden_size, num_layers=n_layers, bias=bias, batch_first=True, dropout=rnn_dropout, \n",
    "                              bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(fc_dropout) if fc_dropout else nn.Identity()\n",
    "        self.fc = nn.Linear(hidden_size * (1 + bidirectional), c_out)\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x.transpose(2,1)    # [batch_size x n_vars x seq_len] --> [batch_size x seq_len x n_vars]\n",
    "        output, _ = self.rnn(x) # output from all sequence steps: [batch_size x seq_len x hidden_size * (1 + bidirectional)]\n",
    "        output = output[:, -1]  # output from last sequence step : [batch_size x hidden_size * (1 + bidirectional)]\n",
    "        output = self.fc(self.dropout(output))\n",
    "        return output\n",
    "        \n",
    "class RNN(_RNN_Base):\n",
    "    _cell = nn.RNN\n",
    "    \n",
    "class LSTM(_RNN_Base):\n",
    "    _cell = nn.LSTM\n",
    "    \n",
    "class GRU(_RNN_Base):\n",
    "    _cell = nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "c_in = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(RNN(c_in, c_out, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5)(xb).shape, [bs, c_out])\n",
    "test_eq(RNN(c_in, c_out)(xb).shape, [bs, c_out])\n",
    "test_eq(RNN(c_in, c_out, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5)(xb).shape, [bs, c_out])\n",
    "test_eq(LSTM(c_in, c_out)(xb).shape, [bs, c_out])\n",
    "test_eq(GRU(c_in, c_out)(xb).shape, [bs, c_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.basics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.805793</td>\n",
       "      <td>1.785832</td>\n",
       "      <td>0.183333</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsid = 'NATOPS' \n",
    "bs = 16\n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)\n",
    "tfms  = [None, [TSCategorize()]]\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=splits)\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=bs, num_workers=0, shuffle=False)\n",
    "model = LSTM(dls.vars, dls.c)\n",
    "learn = Learner(dls, model,  metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "81802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNN(c_in, c_out, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5)\n",
    "print(m)\n",
    "print(count_parameters(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (rnn): LSTM(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "326002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LSTM(c_in, c_out, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5)\n",
    "print(m)\n",
    "print(count_parameters(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (rnn): GRU(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=200, out_features=2, bias=True)\n",
      ")\n",
      "244602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = GRU(c_in, c_out, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5)\n",
    "print(m)\n",
    "print(count_parameters(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a model to TorchScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0263, -0.0224]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(c_in, c_out, hidden_size=100, n_layers=2, bidirectional=True, rnn_dropout=.5, fc_dropout=.5)\n",
    "model.eval()\n",
    "inp = torch.rand(1, c_in, 50)\n",
    "output = model(inp)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  original_name=LSTM\n",
      "  (rnn): LSTM(original_name=LSTM)\n",
      "  (dropout): Dropout(original_name=Dropout)\n",
      "  (fc): Linear(original_name=Linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# save to gpu, cpu or both\n",
    "traced_cpu = torch.jit.trace(model.cpu(), inp)\n",
    "print(traced_cpu)\n",
    "torch.jit.save(traced_cpu, \"cpu.pt\")\n",
    "\n",
    "# load cpu or gpu model\n",
    "traced_cpu = torch.jit.load(\"cpu.pt\")\n",
    "test_eq(traced_cpu(inp), output)\n",
    "\n",
    "!rm \"cpu.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scripting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RecursiveScriptModule(\n",
      "  original_name=LSTM\n",
      "  (rnn): RecursiveScriptModule(original_name=LSTM)\n",
      "  (dropout): RecursiveScriptModule(original_name=Dropout)\n",
      "  (fc): RecursiveScriptModule(original_name=Linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# save to gpu, cpu or both\n",
    "scripted_cpu = torch.jit.script(model.cpu())\n",
    "print(scripted_cpu)\n",
    "torch.jit.save(scripted_cpu, \"cpu.pt\")\n",
    "\n",
    "# load cpu or gpu model\n",
    "scripted_cpu = torch.jit.load(\"cpu.pt\")\n",
    "test_eq(scripted_cpu(inp), output)\n",
    "\n",
    "!rm \"cpu.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting a model to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| onnx\n",
    "```python\n",
    "import onnx\n",
    "\n",
    "# Export the model\n",
    "torch.onnx.export(model.cpu(),               # model being run\n",
    "                  inp,                       # model input (or a tuple for multiple inputs)\n",
    "                  \"cpu.onnx\",                # where to save the model (can be a file or file-like object)\n",
    "                  export_params=True,        # store the trained parameter weights inside the model file\n",
    "                  verbose=False,\n",
    "                  opset_version=13,          # the ONNX version to export the model to\n",
    "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
    "                  input_names = ['input'],   # the model's input names\n",
    "                  output_names = ['output'], # the model's output names\n",
    "                  dynamic_axes={\n",
    "                      'input'  : {0 : 'batch_size'}, \n",
    "                      'output' : {0 : 'batch_size'}} # variable length axes\n",
    "                 )\n",
    "\n",
    "# Load the model and check it's ok\n",
    "onnx_model = onnx.load(\"cpu.onnx\")\n",
    "onnx.checker.check_model(onnx_model)\n",
    "\n",
    "# You can ignore the WARNINGS below\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#| onnx\n",
    "```python\n",
    "import onnxruntime as ort\n",
    "\n",
    "ort_sess = ort.InferenceSession('cpu.onnx')\n",
    "out = ort_sess.run(None, {'input': inp.numpy()})\n",
    "\n",
    "# input & output names\n",
    "input_name = ort_sess.get_inputs()[0].name\n",
    "output_name = ort_sess.get_outputs()[0].name\n",
    "\n",
    "# input dimensions\n",
    "input_dims = ort_sess.get_inputs()[0].shape\n",
    "print(input_name, output_name, input_dims)\n",
    "\n",
    "test_close(out, output.detach().numpy())\n",
    "!rm \"cpu.onnx\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from tsai.imports import create_scripts\n",
    "from tsai.export import get_nb_name\n",
    "from nbdev.clean import nbdev_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "nb_name = get_nb_name()\n",
    "create_scripts(nb_name)\n",
    "nbdev_clean(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
