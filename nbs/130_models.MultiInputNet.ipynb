{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.MultiInputNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiInputNet\n",
    "\n",
    "> This is an implementation created by Ignacio Oguiza - oguiza@gmail.com. It can be used to combine different types of deep learning models into a single one that will accept multiple inputs from a MixedDataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MultiInputNet(Module):\n",
    "    def __init__(self, dls, models, device=None, flatten=True, custom_head=None, **kwargs):\n",
    "        r\"\"\"\n",
    "        Args: \n",
    "            dls: DataLoaders of type MixedDataLoaders.\n",
    "            models: list of models (one model per dataloader in dls).\n",
    "            flatten: if True, the output of each model body will be flattend before concatenating and passing to a joint head.\n",
    "            device: cpu or cuda. If None, default_device() will be chosen.\n",
    "            custom_head: type of thead that will be applied. \n",
    "            kwargs: custom_head kwargs\n",
    "        \"\"\"\n",
    "        \n",
    "        head = ifnone(custom_head, mlp_head)\n",
    "        device = ifnone(device, default_device())\n",
    "        self.models = nn.ModuleList()\n",
    "        for m in L(models):\n",
    "            m.head = Identity()\n",
    "            self.models.append(m)\n",
    "        self.flatten = Reshape(-1) if flatten else None\n",
    "        self.concat = Concat(dim=1)\n",
    "        with torch.no_grad():\n",
    "            self.head = Noop\n",
    "            out = self.forward(first(dls.train)[0])\n",
    "            self.head_nf = out.shape[-1] if flatten else out.shape[1]\n",
    "        self.head = head(self.head_nf, dls.c, **kwargs) \n",
    "        self.to(device=device)\n",
    "\n",
    "    def forward(self, xs):\n",
    "        for i, (x,m) in enumerate(zip(xs, self.models)):\n",
    "            _out = m(*x) if isinstance(x, L) else m(x)\n",
    "            if self.flatten is not None and _out.ndim == 3: _out = self.flatten(_out)\n",
    "            out = _out if i==0 else self.concat([out, _out])\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
