{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.MINIROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIROCKET\n",
    "\n",
    "> A Very Fast (Almost) Deterministic Transform for Time Series Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.data.external import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# minirocket_multivariate.py from https://github.com/angus924/minirocket\n",
    "\n",
    "# Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
    "\n",
    "# MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
    "# Classification\n",
    "\n",
    "# https://arxiv.org/abs/2012.08791\n",
    "# Original code: https://github.com/angus924/minirocket\n",
    "\n",
    "from numba import njit, prange, vectorize\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "@njit(\"float32[:](float32[:,:,:],int32[:],int32[:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
    "def _fit_biases(X, num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, quantiles):\n",
    "\n",
    "    num_examples, num_channels, input_length = X.shape\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    biases = np.zeros(num_features, dtype = np.float32)\n",
    "\n",
    "    feature_index_start = 0\n",
    "\n",
    "    combination_index = 0\n",
    "    num_channels_start = 0\n",
    "\n",
    "    for dilation_index in range(num_dilations):\n",
    "\n",
    "        dilation = dilations[dilation_index]\n",
    "        padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "        num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "        for kernel_index in range(num_kernels):\n",
    "\n",
    "            feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "            num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "\n",
    "            num_channels_end = num_channels_start + num_channels_this_combination\n",
    "\n",
    "            channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
    "\n",
    "            _X = X[np.random.randint(num_examples)][channels_this_combination]\n",
    "\n",
    "            A = -_X          # A = alpha * X = -X\n",
    "            G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "            C_alpha = np.zeros((num_channels_this_combination, input_length), dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, num_channels_this_combination, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
    "                C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
    "                C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "            C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "            C = np.sum(C, axis = 0)\n",
    "\n",
    "            biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
    "\n",
    "            feature_index_start = feature_index_end\n",
    "\n",
    "            combination_index += 1\n",
    "            num_channels_start = num_channels_end\n",
    "\n",
    "    return biases\n",
    "\n",
    "def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    num_features_per_kernel = num_features // num_kernels\n",
    "    true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
    "    multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
    "\n",
    "    max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
    "    dilations, num_features_per_dilation = \\\n",
    "    np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
    "    num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
    "\n",
    "    remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
    "    i = 0\n",
    "    while remainder > 0:\n",
    "        num_features_per_dilation[i] += 1\n",
    "        remainder -= 1\n",
    "        i = (i + 1) % len(num_features_per_dilation)\n",
    "\n",
    "    return dilations, num_features_per_dilation\n",
    "\n",
    "# low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
    "def _quantiles(n):\n",
    "    return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
    "\n",
    "def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
    "\n",
    "    _, num_channels, input_length = X.shape\n",
    "\n",
    "    num_kernels = 84\n",
    "\n",
    "    dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
    "\n",
    "    num_features_per_kernel = np.sum(num_features_per_dilation)\n",
    "\n",
    "    quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
    "\n",
    "    num_dilations = len(dilations)\n",
    "    num_combinations = num_kernels * num_dilations\n",
    "\n",
    "    max_num_channels = min(num_channels, 9)\n",
    "    max_exponent = np.log2(max_num_channels + 1)\n",
    "\n",
    "    num_channels_per_combination = (2 ** np.random.uniform(0, max_exponent, num_combinations)).astype(np.int32)\n",
    "\n",
    "    channel_indices = np.zeros(num_channels_per_combination.sum(), dtype = np.int32)\n",
    "\n",
    "    num_channels_start = 0\n",
    "    for combination_index in range(num_combinations):\n",
    "        num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "        num_channels_end = num_channels_start + num_channels_this_combination\n",
    "        channel_indices[num_channels_start:num_channels_end] = np.random.choice(num_channels, num_channels_this_combination, replace = False)\n",
    "\n",
    "        num_channels_start = num_channels_end\n",
    "\n",
    "    biases = _fit_biases(X, num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, quantiles)\n",
    "\n",
    "    return num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases\n",
    "\n",
    "# _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
    "@vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
    "def _PPV(a, b):\n",
    "    if a > b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "@njit(\"float32[:,:](float32[:,:,:],Tuple((int32[:],int32[:],int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
    "def transform(X, parameters):\n",
    "\n",
    "    num_examples, num_channels, input_length = X.shape\n",
    "\n",
    "    num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases = parameters\n",
    "\n",
    "    # equivalent to:\n",
    "    # >>> from itertools import combinations\n",
    "    # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "    indices = np.array((\n",
    "        0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "        0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "        0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "        0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "        1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "        1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "        1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "        2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "        2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "        2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "        3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "        4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "    ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "    num_kernels = len(indices)\n",
    "    num_dilations = len(dilations)\n",
    "\n",
    "    num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "    features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
    "\n",
    "    for example_index in prange(num_examples):\n",
    "\n",
    "        _X = X[example_index]\n",
    "\n",
    "        A = -_X          # A = alpha * X = -X\n",
    "        G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "        feature_index_start = 0\n",
    "\n",
    "        combination_index = 0\n",
    "        num_channels_start = 0\n",
    "\n",
    "        for dilation_index in range(num_dilations):\n",
    "\n",
    "            _padding0 = dilation_index % 2\n",
    "\n",
    "            dilation = dilations[dilation_index]\n",
    "            padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "            num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "            C_alpha = np.zeros((num_channels, input_length), dtype = np.float32)\n",
    "            C_alpha[:] = A\n",
    "\n",
    "            C_gamma = np.zeros((9, num_channels, input_length), dtype = np.float32)\n",
    "            C_gamma[9 // 2] = G\n",
    "\n",
    "            start = dilation\n",
    "            end = input_length - padding\n",
    "\n",
    "            for gamma_index in range(9 // 2):\n",
    "\n",
    "                C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
    "                C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
    "\n",
    "                end += dilation\n",
    "\n",
    "            for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "                C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
    "                C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
    "\n",
    "                start += dilation\n",
    "\n",
    "            for kernel_index in range(num_kernels):\n",
    "\n",
    "                feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "                num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "\n",
    "                num_channels_end = num_channels_start + num_channels_this_combination\n",
    "\n",
    "                channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
    "\n",
    "                _padding1 = (_padding0 + kernel_index) % 2\n",
    "\n",
    "                index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "                C = C_alpha[channels_this_combination] + \\\n",
    "                    C_gamma[index_0][channels_this_combination] + \\\n",
    "                    C_gamma[index_1][channels_this_combination] + \\\n",
    "                    C_gamma[index_2][channels_this_combination]\n",
    "                C = np.sum(C, axis = 0)\n",
    "\n",
    "                if _padding1 == 0:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
    "                else:\n",
    "                    for feature_count in range(num_features_this_dilation):\n",
    "                        features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
    "\n",
    "                feature_index_start = feature_index_end\n",
    "\n",
    "                combination_index += 1\n",
    "                num_channels_start = num_channels_end\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial MINIROCKET implementation in Pytorch developed by Ignacio Oguiza - timeseriesAI@gmail.com based on:\n",
    "# Dempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. \n",
    "# arXiv preprint arXiv:2012.08791.\n",
    "# Official repo: https://github.com/angus924/minirocket\n",
    "\n",
    "class MINIROCKET(nn.Module):\n",
    "    def __init__(self, c_in, c_out, seq_len, bn=True, fc_dropout=0.):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if bn: layers += [nn.BatchNorm1d(seq_len)]\n",
    "        if fc_dropout: layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(seq_len, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        self.head = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.head(x.squeeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial MINIROCKET implementation in Pytorch developed by Ignacio Oguiza - timeseriesAI@gmail.com based on:\n",
    "# Dempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. \n",
    "# arXiv preprint arXiv:2012.08791.\n",
    "# Official repo: https://github.com/angus924/minirocket\n",
    "\n",
    "class MINIROCKETv2(nn.Module):\n",
    "    def __init__(self, c_in, c_out, seq_len, num_features=10_000, max_dilations_per_kernel=32, bn=True, fc_dropout=0., eps=1e-8):\n",
    "        super().__init__()\n",
    "        self.c_in, self.seq_len, self.num_features, self.max_dilations_per_kernel, self.eps = c_in, seq_len, num_features, max_dilations_per_kernel, eps\n",
    "        self.rocket_params = None\n",
    "        self.f_mean, self.f_std = None, None\n",
    "        self.fit = fit\n",
    "        self.tfm = transform\n",
    "        self.head_nf = 84 * (num_features // 84)\n",
    "        layers = []\n",
    "        if bn: layers += [nn.BatchNorm1d(self.head_nf)]\n",
    "        if fc_dropout: layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(self.head_nf, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        self.head = nn.Sequential(*layers)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            if self.rocket_params is None:\n",
    "                self.rocket_params = self.fit(x.cpu().numpy(), self.num_features, self.max_dilations_per_kernel)\n",
    "            x_tfm = x.new(self.tfm(x.cpu().numpy(), self.rocket_params))\n",
    "            if self.f_mean is None:\n",
    "                self.f_mean = x_tfm.mean(0)\n",
    "                self.f_std = x_tfm.std(0) + self.eps\n",
    "            x_tfm = (x_tfm - self.f_mean) / self.f_std\n",
    "            x_tfm.requires_grad = True\n",
    "        return self.head(x_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORM X\n",
    "X, y, splits = get_UCR_data('NATOPS', split_data=False)\n",
    "rocket_params = fit(X, num_features=10_000, max_dilations_per_kernel=32)\n",
    "X_tfm = transform(X, rocket_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With MINIROCKET we use need to:\n",
    "1. Transform X --> X_tfm\n",
    "2. Train a MINIROCKET using previously calculated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.791758</td>\n",
       "      <td>1.639753</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.563619</td>\n",
       "      <td>1.494893</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.376873</td>\n",
       "      <td>1.359884</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.learner import *\n",
    "tfms = [None, TSClassification()]\n",
    "batch_tfms = [TSStandardize(by_sample=True)]\n",
    "dls = get_ts_dls(X_tfm, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "learn = ts_learner(dls, MINIROCKET, metrics=accuracy)\n",
    "learn.fit(3, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In MINIROCKETv2 features are transformed internally with every batch. This makes is slower but allows you to use data augmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.791758</td>\n",
       "      <td>1.357971</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.learner import *\n",
    "tfms = [None, TSClassification()]\n",
    "batch_tfms = [TSStandardize(by_sample=True), TSMagScale()]\n",
    "dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "learn = ts_learner(dls, MINIROCKETv2, metrics=accuracy)\n",
    "learn.fit(1, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 000c_data.preparation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 003_data.preprocessing.ipynb.\n",
      "Converted 003b_data.transforms.ipynb.\n",
      "Converted 003c_data.mixed_augmentation.ipynb.\n",
      "Converted 003d_data.image.ipynb.\n",
      "Converted 003e_data.features.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 006_data.mixed.ipynb.\n",
      "Converted 007_metrics.ipynb.\n",
      "Converted 008_learner.ipynb.\n",
      "Converted 009_optimizer.ipynb.\n",
      "Converted 010_callback.core.ipynb.\n",
      "Converted 011_callback.noisy_student.ipynb.\n",
      "Converted 012_callback.gblend.ipynb.\n",
      "Converted 013_callback.TSBERT.ipynb.\n",
      "Converted 100_models.layers.ipynb.\n",
      "Converted 100b_models.utils.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 111b_models.MINIROCKET.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 140_models.misc.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/Documents/Machine_Learning/Jupyter_Notebooks/tsai/tsai\n",
      "Correct conversion! 😃\n",
      "Total time elapsed 213 s\n",
      "Monday 01/18/21 00:50:12 CET\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
