{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.MINIROCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINIROCKET\n",
    "\n",
    "> A Very Fast (Almost) Deterministic Transform for Time Series Classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.data.external import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# # minirocket_multivariate.py from https://github.com/angus924/minirocket\n",
    "\n",
    "# # Angus Dempster, Daniel F Schmidt, Geoffrey I Webb\n",
    "\n",
    "# # MiniRocket: A Very Fast (Almost) Deterministic Transform for Time Series\n",
    "# # Classification\n",
    "\n",
    "# # https://arxiv.org/abs/2012.08791\n",
    "# # Original code: https://github.com/angus924/minirocket\n",
    "\n",
    "# from numba import njit, prange, vectorize\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# @njit(\"float32[:](float32[:,:,:],int32[:],int32[:],int32[:],int32[:],float32[:])\", fastmath = True, parallel = False, cache = True)\n",
    "# def _fit_biases(X, num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, quantiles):\n",
    "\n",
    "#     num_examples, num_channels, input_length = X.shape\n",
    "\n",
    "#     # equivalent to:\n",
    "#     # >>> from itertools import combinations\n",
    "#     # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "#     indices = np.array((\n",
    "#         0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "#         0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "#         0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "#         0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "#         1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "#         1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "#         1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "#         2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "#         2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "#         2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "#         3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "#         4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "#     ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "#     num_kernels = len(indices)\n",
    "#     num_dilations = len(dilations)\n",
    "\n",
    "#     num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "#     biases = np.zeros(num_features, dtype = np.float32)\n",
    "\n",
    "#     feature_index_start = 0\n",
    "\n",
    "#     combination_index = 0\n",
    "#     num_channels_start = 0\n",
    "\n",
    "#     for dilation_index in range(num_dilations):\n",
    "\n",
    "#         dilation = dilations[dilation_index]\n",
    "#         padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "#         num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "#         for kernel_index in range(num_kernels):\n",
    "\n",
    "#             feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "#             num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "\n",
    "#             num_channels_end = num_channels_start + num_channels_this_combination\n",
    "\n",
    "#             channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
    "\n",
    "#             _X = X[np.random.randint(num_examples)][channels_this_combination]\n",
    "\n",
    "#             A = -_X          # A = alpha * X = -X\n",
    "#             G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "#             C_alpha = np.zeros((num_channels_this_combination, input_length), dtype = np.float32)\n",
    "#             C_alpha[:] = A\n",
    "\n",
    "#             C_gamma = np.zeros((9, num_channels_this_combination, input_length), dtype = np.float32)\n",
    "#             C_gamma[9 // 2] = G\n",
    "\n",
    "#             start = dilation\n",
    "#             end = input_length - padding\n",
    "\n",
    "#             for gamma_index in range(9 // 2):\n",
    "\n",
    "#                 C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
    "#                 C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
    "\n",
    "#                 end += dilation\n",
    "\n",
    "#             for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "#                 C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
    "#                 C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
    "\n",
    "#                 start += dilation\n",
    "\n",
    "#             index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "#             C = C_alpha + C_gamma[index_0] + C_gamma[index_1] + C_gamma[index_2]\n",
    "#             C = np.sum(C, axis = 0)\n",
    "\n",
    "#             biases[feature_index_start:feature_index_end] = np.quantile(C, quantiles[feature_index_start:feature_index_end])\n",
    "\n",
    "#             feature_index_start = feature_index_end\n",
    "\n",
    "#             combination_index += 1\n",
    "#             num_channels_start = num_channels_end\n",
    "\n",
    "#     return biases\n",
    "\n",
    "# def _fit_dilations(input_length, num_features, max_dilations_per_kernel):\n",
    "\n",
    "#     num_kernels = 84\n",
    "\n",
    "#     num_features_per_kernel = num_features // num_kernels\n",
    "#     true_max_dilations_per_kernel = min(num_features_per_kernel, max_dilations_per_kernel)\n",
    "#     multiplier = num_features_per_kernel / true_max_dilations_per_kernel\n",
    "\n",
    "#     max_exponent = np.log2((input_length - 1) / (9 - 1))\n",
    "#     dilations, num_features_per_dilation = \\\n",
    "#     np.unique(np.logspace(0, max_exponent, true_max_dilations_per_kernel, base = 2).astype(np.int32), return_counts = True)\n",
    "#     num_features_per_dilation = (num_features_per_dilation * multiplier).astype(np.int32) # this is a vector\n",
    "\n",
    "#     remainder = num_features_per_kernel - np.sum(num_features_per_dilation)\n",
    "#     i = 0\n",
    "#     while remainder > 0:\n",
    "#         num_features_per_dilation[i] += 1\n",
    "#         remainder -= 1\n",
    "#         i = (i + 1) % len(num_features_per_dilation)\n",
    "\n",
    "#     return dilations, num_features_per_dilation\n",
    "\n",
    "# # low-discrepancy sequence to assign quantiles to kernel/dilation combinations\n",
    "# def _quantiles(n):\n",
    "#     return np.array([(_ * ((np.sqrt(5) + 1) / 2)) % 1 for _ in range(1, n + 1)], dtype = np.float32)\n",
    "\n",
    "# def fit(X, num_features = 10_000, max_dilations_per_kernel = 32):\n",
    "\n",
    "#     _, num_channels, input_length = X.shape\n",
    "\n",
    "#     num_kernels = 84\n",
    "\n",
    "#     dilations, num_features_per_dilation = _fit_dilations(input_length, num_features, max_dilations_per_kernel)\n",
    "\n",
    "#     num_features_per_kernel = np.sum(num_features_per_dilation)\n",
    "\n",
    "#     quantiles = _quantiles(num_kernels * num_features_per_kernel)\n",
    "\n",
    "#     num_dilations = len(dilations)\n",
    "#     num_combinations = num_kernels * num_dilations\n",
    "\n",
    "#     max_num_channels = min(num_channels, 9)\n",
    "#     max_exponent = np.log2(max_num_channels + 1)\n",
    "\n",
    "#     num_channels_per_combination = (2 ** np.random.uniform(0, max_exponent, num_combinations)).astype(np.int32)\n",
    "\n",
    "#     channel_indices = np.zeros(num_channels_per_combination.sum(), dtype = np.int32)\n",
    "\n",
    "#     num_channels_start = 0\n",
    "#     for combination_index in range(num_combinations):\n",
    "#         num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "#         num_channels_end = num_channels_start + num_channels_this_combination\n",
    "#         channel_indices[num_channels_start:num_channels_end] = np.random.choice(num_channels, num_channels_this_combination, replace = False)\n",
    "\n",
    "#         num_channels_start = num_channels_end\n",
    "\n",
    "#     biases = _fit_biases(X, num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, quantiles)\n",
    "\n",
    "#     return num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases\n",
    "\n",
    "# # _PPV(C, b).mean() returns PPV for vector C (convolution output) and scalar b (bias)\n",
    "# @vectorize(\"float32(float32,float32)\", nopython = True, cache = True)\n",
    "# def _PPV(a, b):\n",
    "#     if a > b:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "# @njit(\"float32[:,:](float32[:,:,:],Tuple((int32[:],int32[:],int32[:],int32[:],float32[:])))\", fastmath = True, parallel = True, cache = True)\n",
    "# def transform(X, parameters):\n",
    "\n",
    "#     num_examples, num_channels, input_length = X.shape\n",
    "\n",
    "#     num_channels_per_combination, channel_indices, dilations, num_features_per_dilation, biases = parameters\n",
    "\n",
    "#     # equivalent to:\n",
    "#     # >>> from itertools import combinations\n",
    "#     # >>> indices = np.array([_ for _ in combinations(np.arange(9), 3)], dtype = np.int32)\n",
    "#     indices = np.array((\n",
    "#         0,1,2,0,1,3,0,1,4,0,1,5,0,1,6,0,1,7,0,1,8,\n",
    "#         0,2,3,0,2,4,0,2,5,0,2,6,0,2,7,0,2,8,0,3,4,\n",
    "#         0,3,5,0,3,6,0,3,7,0,3,8,0,4,5,0,4,6,0,4,7,\n",
    "#         0,4,8,0,5,6,0,5,7,0,5,8,0,6,7,0,6,8,0,7,8,\n",
    "#         1,2,3,1,2,4,1,2,5,1,2,6,1,2,7,1,2,8,1,3,4,\n",
    "#         1,3,5,1,3,6,1,3,7,1,3,8,1,4,5,1,4,6,1,4,7,\n",
    "#         1,4,8,1,5,6,1,5,7,1,5,8,1,6,7,1,6,8,1,7,8,\n",
    "#         2,3,4,2,3,5,2,3,6,2,3,7,2,3,8,2,4,5,2,4,6,\n",
    "#         2,4,7,2,4,8,2,5,6,2,5,7,2,5,8,2,6,7,2,6,8,\n",
    "#         2,7,8,3,4,5,3,4,6,3,4,7,3,4,8,3,5,6,3,5,7,\n",
    "#         3,5,8,3,6,7,3,6,8,3,7,8,4,5,6,4,5,7,4,5,8,\n",
    "#         4,6,7,4,6,8,4,7,8,5,6,7,5,6,8,5,7,8,6,7,8\n",
    "#     ), dtype = np.int32).reshape(84, 3)\n",
    "\n",
    "#     num_kernels = len(indices)\n",
    "#     num_dilations = len(dilations)\n",
    "\n",
    "#     num_features = num_kernels * np.sum(num_features_per_dilation)\n",
    "\n",
    "#     features = np.zeros((num_examples, num_features), dtype = np.float32)\n",
    "\n",
    "#     for example_index in prange(num_examples):\n",
    "\n",
    "#         _X = X[example_index]\n",
    "\n",
    "#         A = -_X          # A = alpha * X = -X\n",
    "#         G = _X + _X + _X # G = gamma * X = 3X\n",
    "\n",
    "#         feature_index_start = 0\n",
    "\n",
    "#         combination_index = 0\n",
    "#         num_channels_start = 0\n",
    "\n",
    "#         for dilation_index in range(num_dilations):\n",
    "\n",
    "#             _padding0 = dilation_index % 2\n",
    "\n",
    "#             dilation = dilations[dilation_index]\n",
    "#             padding = ((9 - 1) * dilation) // 2\n",
    "\n",
    "#             num_features_this_dilation = num_features_per_dilation[dilation_index]\n",
    "\n",
    "#             C_alpha = np.zeros((num_channels, input_length), dtype = np.float32)\n",
    "#             C_alpha[:] = A\n",
    "\n",
    "#             C_gamma = np.zeros((9, num_channels, input_length), dtype = np.float32)\n",
    "#             C_gamma[9 // 2] = G\n",
    "\n",
    "#             start = dilation\n",
    "#             end = input_length - padding\n",
    "\n",
    "#             for gamma_index in range(9 // 2):\n",
    "\n",
    "#                 C_alpha[:, -end:] = C_alpha[:, -end:] + A[:, :end]\n",
    "#                 C_gamma[gamma_index, :, -end:] = G[:, :end]\n",
    "\n",
    "#                 end += dilation\n",
    "\n",
    "#             for gamma_index in range(9 // 2 + 1, 9):\n",
    "\n",
    "#                 C_alpha[:, :-start] = C_alpha[:, :-start] + A[:, start:]\n",
    "#                 C_gamma[gamma_index, :, :-start] = G[:, start:]\n",
    "\n",
    "#                 start += dilation\n",
    "\n",
    "#             for kernel_index in range(num_kernels):\n",
    "\n",
    "#                 feature_index_end = feature_index_start + num_features_this_dilation\n",
    "\n",
    "#                 num_channels_this_combination = num_channels_per_combination[combination_index]\n",
    "\n",
    "#                 num_channels_end = num_channels_start + num_channels_this_combination\n",
    "\n",
    "#                 channels_this_combination = channel_indices[num_channels_start:num_channels_end]\n",
    "\n",
    "#                 _padding1 = (_padding0 + kernel_index) % 2\n",
    "\n",
    "#                 index_0, index_1, index_2 = indices[kernel_index]\n",
    "\n",
    "#                 C = C_alpha[channels_this_combination] + \\\n",
    "#                     C_gamma[index_0][channels_this_combination] + \\\n",
    "#                     C_gamma[index_1][channels_this_combination] + \\\n",
    "#                     C_gamma[index_2][channels_this_combination]\n",
    "#                 C = np.sum(C, axis = 0)\n",
    "\n",
    "#                 if _padding1 == 0:\n",
    "#                     for feature_count in range(num_features_this_dilation):\n",
    "#                         features[example_index, feature_index_start + feature_count] = _PPV(C, biases[feature_index_start + feature_count]).mean()\n",
    "#                 else:\n",
    "#                     for feature_count in range(num_features_this_dilation):\n",
    "#                         features[example_index, feature_index_start + feature_count] = _PPV(C[padding:-padding], biases[feature_index_start + feature_count]).mean()\n",
    "\n",
    "#                 feature_index_start = feature_index_end\n",
    "\n",
    "#                 combination_index += 1\n",
    "#                 num_channels_start = num_channels_end\n",
    "\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sktime.transformations.panel.rocket._minirocket import _fit as minirocket_fit\n",
    "from sktime.transformations.panel.rocket._minirocket import _transform as minirocket_transform\n",
    "from sktime.transformations.panel.rocket._minirocket_multivariate import _fit_multi as minirocket_fit_multi\n",
    "from sktime.transformations.panel.rocket._minirocket_multivariate import _transform_multi as minirocket_transform_multi\n",
    "from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "from sklearn.linear_model import RidgeCV, RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is a wrapper used to extract features in the format required by the tsai library. \n",
    "\n",
    "class MiniRocketFeatures:\n",
    "    def fit(self, o, num_features=10_000, max_dilations_per_kernel=32):\n",
    "        if o.dtype != 'float32': o = o.astype('float32')\n",
    "        if o.ndim == 2: \n",
    "            o = o[:, np.newaxis]\n",
    "        if o.shape[1] == 1:\n",
    "            parameters = minirocket_fit(o[0, 0][np.newaxis], num_features=num_features, max_dilations_per_kernel=max_dilations_per_kernel)\n",
    "        else: \n",
    "            parameters = minirocket_fit_multi(o[0][np.newaxis], num_features=num_features, max_dilations_per_kernel=max_dilations_per_kernel)\n",
    "        self.parameters = parameters\n",
    "    \n",
    "    def transform(self, o, fname='X_tfm', path='./data/MiniRocketFeatures', on_disk=True, mode='r+', chunksize=10_000):\n",
    "        if o.dtype != 'float32': o = o.astype('float32')\n",
    "        if o.ndim == 2: \n",
    "            o = o[:, np.newaxis]\n",
    "        if chunksize is None:\n",
    "            if o.shape[1] == 1:\n",
    "                o_tfm = minirocket_transform(o[:, 0], self.parameters)[..., np.newaxis]\n",
    "            else:\n",
    "                o_tfm = minirocket_transform_multi(o, self.parameters)[..., np.newaxis]\n",
    "            return o_tfm\n",
    "        else: \n",
    "            start = 0\n",
    "            pb = progress_bar(range(math.ceil(len(o) / chunksize)), leave=False)\n",
    "            for i in pb:\n",
    "                end = start + chunksize\n",
    "                if o.shape[1] == 1:\n",
    "                    _o_tfm = minirocket_transform(o[start:end, 0], self.parameters)[..., np.newaxis]\n",
    "                else:\n",
    "                    _o_tfm = minirocket_transform_multi(o[start:end], self.parameters)[..., np.newaxis]\n",
    "                if i == 0: \n",
    "                    shape = (o.shape[0], _o_tfm.shape[1], _o_tfm.shape[2])\n",
    "                    o_tfm = create_empty_array(shape, fname=fname, path=path, on_disk=on_disk, mode=mode)\n",
    "                o_tfm[start:end] = _o_tfm\n",
    "                start = end\n",
    "                del _o_tfm\n",
    "                gc.collect()\n",
    "            return o_tfm\n",
    "    \n",
    "    def fit_transform(self, o, num_features=10_000, max_dilations_per_kernel=32, \n",
    "                        fname='X_tfm', path='./data/MiniRocketFeatures', on_disk=True, mode='r+', chunksize=10_000):\n",
    "        self.fit(o, num_features=num_features, max_dilations_per_kernel=max_dilations_per_kernel)\n",
    "        return self.transform(o, fname=fname, path=path, on_disk=on_disk, mode=mode, chunksize=chunksize)\n",
    "        \n",
    "    def save(self, fname, path='./models/MiniRocketFeatures'):\n",
    "        path = Path(path)\n",
    "        if not fname.endswith('pkl'): fname = f'{fname}.pkl'\n",
    "        filename = path/fname\n",
    "        filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(filename, 'wb') as output:\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    def load(self, fname, path='./models/MiniRocketFeatures'):\n",
    "        path = Path(path)\n",
    "        if not fname.endswith('pkl'): fname = f'{fname}.pkl'\n",
    "        filename = path/fname\n",
    "        filename.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(filename, 'rb') as input:\n",
    "            output = pickle.load(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# This is an unofficial MINIROCKET implementation in Pytorch developed by Ignacio Oguiza - timeseriesAI@gmail.com based on:\n",
    "# Dempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. \n",
    "# arXiv preprint arXiv:2012.08791.\n",
    "# Official repo: https://github.com/angus924/minirocket\n",
    "\n",
    "\n",
    "class MiniRocket(nn.Sequential):\n",
    "    def __init__(self, c_in, c_out, seq_len=1, fc_dropout=0., **kwargs):\n",
    "        \"\"\"\n",
    "        MINIROCKET implementation where features are previously calculated.\n",
    "        Args:\n",
    "            c_in: number of features per sample. For 10_000 kernels iw will be 9996. \n",
    "            c_out: number of classes.\n",
    "            seq_len: For MINIROCKET this is always 1 as features are previously calculated. Included for compatibility.\n",
    "            fc_dropout: indicates whether dropout should be added to the last fully connected layer\n",
    "\n",
    "        Input shape: [batch_size x c_in x 1]\n",
    "        \"\"\"\n",
    "        backbone = nn.Sequential()\n",
    "        self.head_nf = c_in\n",
    "        layers = [Squeeze()]\n",
    "        if fc_dropout: layers += [nn.Dropout(fc_dropout)]\n",
    "        linear = nn.Linear(c_in, c_out)\n",
    "        nn.init.constant_(linear.weight.data, 0)\n",
    "        nn.init.constant_(linear.bias.data, 0)\n",
    "        layers += [linear]\n",
    "        head = nn.Sequential(*layers)\n",
    "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsid = 'ECGFiveDays'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "mrf = MiniRocketFeatures()\n",
    "mrf.fit(X[splits[0]])\n",
    "X_tfm_train = mrf.transform(X[splits[0]], 'mrf_train', on_disk=True)\n",
    "p1 = mrf.parameters\n",
    "mrf.save(f'mrf_{dsid}')\n",
    "del mrf\n",
    "mrf = MiniRocketFeatures().load(f'mrf_{dsid}')\n",
    "p2 = mrf.parameters\n",
    "test_eq(p1, p2)\n",
    "X_tfm_valid = mrf.transform(X[splits[1]], 'mrf_valid', on_disk=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# #hide\n",
    "# # This is an unofficial MINIROCKET implementation in Pytorch developed by Ignacio Oguiza - timeseriesAI@gmail.com based on:\n",
    "# # Dempster, A., Schmidt, D. F., & Webb, G. I. (2020). MINIROCKET: A Very Fast (Almost) Deterministic Transform for Time Series Classification. \n",
    "# # arXiv preprint arXiv:2012.08791.\n",
    "# # Official repo: https://github.com/angus924/minirocket\n",
    "\n",
    "# class MINIROCKETv2(nn.Module):\n",
    "#     def __init__(self, c_in, c_out, seq_len=None, num_features=10_000, max_dilations_per_kernel=32, fc_dropout=0., \n",
    "#                  eps=1e-8, custom_head=None, **kwargs):\n",
    "#         super().__init__()\n",
    "#         self.c_in, self.num_features, self.max_dilations_per_kernel, self.eps = c_in, num_features, max_dilations_per_kernel, eps\n",
    "#         self.rocket_params = None\n",
    "#         self.f_mean, self.f_std = None, None\n",
    "#         self.fit = minirocket_fit if c_in == 1 else minirocket_fit_multi\n",
    "#         self.tfm = minirocket_transform if c_in == 1 else minirocket_transform_multi\n",
    "#         self.head_nf = 84 * (num_features // 84)\n",
    "#         if custom_head is not None: \n",
    "#             self.head = custom_head(self.head_nf, c_out, 1, fc_dropout=fc_dropout, **kwargs)\n",
    "#         else:\n",
    "#             layers = []\n",
    "#             if fc_dropout: layers += [nn.Dropout(fc_dropout)]\n",
    "#             layers += [Squeeze(-1)]\n",
    "#             linear = nn.Linear(self.head_nf, c_out)\n",
    "#             nn.init.constant_(linear.weight.data, 0)\n",
    "#             nn.init.constant_(linear.bias.data, 0)\n",
    "#             layers += [linear]\n",
    "#             self.head = nn.Sequential(*layers)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         with torch.no_grad():\n",
    "#             if self.rocket_params is None:\n",
    "#                 if self.c_in == 1: \n",
    "#                     self.rocket_params = self.fit(x.cpu().numpy()[:, 0], self.num_features, self.max_dilations_per_kernel)\n",
    "#                 else: \n",
    "#                     self.rocket_params = self.fit(x.cpu().numpy(), self.num_features, self.max_dilations_per_kernel)\n",
    "#             if self.c_in == 1: \n",
    "#                 x_tfm = x.new(self.tfm(x.cpu().numpy()[:, 0], self.rocket_params)).unsqueeze(-1)\n",
    "#             else: \n",
    "#                 x_tfm = x.new(self.tfm(x.cpu().numpy(), self.rocket_params)).unsqueeze(-1)\n",
    "#             if self.f_mean is None:\n",
    "#                 self.f_mean = x_tfm.mean(0)\n",
    "#                 self.f_std = x_tfm.std(0) + self.eps\n",
    "#             x_tfm = (x_tfm - self.f_mean) / self.f_std\n",
    "#         return self.head(x_tfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With MiniRocket we use need to:\n",
    "1. Transform X --> X_tfm\n",
    "2. Train a MiniRocket using previously calculated features (X_tfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.526331</td>\n",
       "      <td>0.754936</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.582386</td>\n",
       "      <td>0.435146</td>\n",
       "      <td>0.796748</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.508790</td>\n",
       "      <td>0.368741</td>\n",
       "      <td>0.828107</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.453322</td>\n",
       "      <td>0.312180</td>\n",
       "      <td>0.861789</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.407587</td>\n",
       "      <td>0.262873</td>\n",
       "      <td>0.893148</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.368117</td>\n",
       "      <td>0.220747</td>\n",
       "      <td>0.914053</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.333499</td>\n",
       "      <td>0.185743</td>\n",
       "      <td>0.944251</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.303106</td>\n",
       "      <td>0.157302</td>\n",
       "      <td>0.959350</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.276519</td>\n",
       "      <td>0.134482</td>\n",
       "      <td>0.973287</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.253316</td>\n",
       "      <td>0.116239</td>\n",
       "      <td>0.979094</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.learner import *\n",
    "dsid = 'ECGFiveDays'\n",
    "X, y, splits = get_UCR_data(dsid, split_data=False)\n",
    "tfms = [None, TSClassification()]\n",
    "batch_tfms = TSStandardize(by_var=True)\n",
    "mrf = MiniRocketFeatures()\n",
    "X_tfm = mrf.fit_transform(X)\n",
    "dls = get_ts_dls(X_tfm, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "learn = ts_learner(dls, MiniRocket, metrics=accuracy)\n",
    "learn.fit(10, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MiniRocketClassifier(sklearn.pipeline.Pipeline):\n",
    "    def __init__(self, num_features=10_000, max_dilations_per_kernel=32, random_state=None, \n",
    "                 alphas=np.logspace(-3, 3, 13), normalize=True, memory=None, verbose=False, scoring=None, class_weight=None, **kwargs):\n",
    "        \"\"\"\n",
    "        MiniRocketClassifier is recommended for up to 10k time series. \n",
    "        For a larger dataset, you can use MINIROCKET (in Pytorch).\n",
    "        scoring = None --> defaults to accuracy.\n",
    "        \"\"\"\n",
    "        self.steps = [('minirocketmultivariate', MiniRocketMultivariate(num_features=num_features, \n",
    "                                                                        max_dilations_per_kernel=max_dilations_per_kernel, \n",
    "                                                                        random_state=random_state)),\n",
    "                      ('ridgeclassifiercv', RidgeClassifierCV(alphas=alphas, normalize=normalize, scoring=scoring, class_weight=class_weight, **kwargs))]\n",
    "        self.num_features, self.max_dilations_per_kernel, self.random_state = num_features, max_dilations_per_kernel, random_state\n",
    "        self.alphas, self.normalize, self.scoring, self.class_weight, self.kwargs = alphas, normalize, scoring, class_weight, kwargs\n",
    "        self.memory = memory\n",
    "        self.verbose = verbose\n",
    "        self._validate_steps()\n",
    "\n",
    "    def save(self, fname='MiniRocket', path='./data'):\n",
    "        path = Path(path)\n",
    "        filename = path/fname\n",
    "        with open(f'{filename}.pkl', 'wb') as output:\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_minirocket(fname='MiniRocket', path='./data'):\n",
    "    path = Path(path)\n",
    "    filename = path/fname\n",
    "    with open(f'{filename}.pkl', 'rb') as input:\n",
    "        output = pickle.load(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class MiniRocketRegressor(sklearn.pipeline.Pipeline):\n",
    "    def __init__(self, num_features=10000, max_dilations_per_kernel=32, random_state=None, \n",
    "                 alphas=np.logspace(-3, 3, 13), *, normalize=True, memory=None, verbose=False, scoring=None, **kwargs):\n",
    "        \"\"\"\n",
    "        MiniRocketRegressor is recommended for up to 10k time series. \n",
    "        For a larger dataset, you can use MINIROCKET (in Pytorch).\n",
    "        scoring = None --> defaults to r2.\n",
    "        \"\"\"\n",
    "        self.steps = [('minirocketmultivariate', MiniRocketMultivariate(num_features=num_features, \n",
    "                                                                        max_dilations_per_kernel=max_dilations_per_kernel, \n",
    "                                                                        random_state=random_state)),\n",
    "                      ('ridgecv', RidgeCV(alphas=alphas, normalize=normalize, scoring=scoring, **kwargs))]\n",
    "        self.num_features, self.max_dilations_per_kernel, self.random_state = num_features, max_dilations_per_kernel, random_state\n",
    "        self.alphas, self.normalize, self.scoring, self.kwargs = alphas, normalize, scoring, kwargs\n",
    "        self.memory = memory\n",
    "        self.verbose = verbose\n",
    "        self._validate_steps()\n",
    "\n",
    "    def save(self, fname='MiniRocket', path='./data'):\n",
    "        path = Path(path)\n",
    "        filename = path/fname\n",
    "        with open(f'{filename}.pkl', 'wb') as output:\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_minirocket(fname='MiniRocket', path='./data'):\n",
    "    path = Path(path)\n",
    "    filename = path/fname\n",
    "    with open(f'{filename}.pkl', 'rb') as input:\n",
    "        output = pickle.load(input)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from tsai.data.all import *\n",
    "# from tsai.learner import *\n",
    "# tfms = [None, TSClassification()]\n",
    "# batch_tfms = [TSMagScale(), TSStandardize(by_sample=True)]\n",
    "# dls = get_ts_dls(X, y, splits=splits, tfms=tfms, batch_tfms=batch_tfms, bs=256)\n",
    "# learn = ts_learner(dls, MINIROCKETv2, metrics=accuracy)\n",
    "# learn.fit(1, 1e-4, cbs=ReduceLROnPlateau(factor=0.5, min_lr=1e-8, patience=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# #hide\n",
    "# from sklearn.pipeline import make_pipeline\n",
    "# from sktime.transformations.panel.rocket import MiniRocketMultivariate\n",
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# class MiniRocketLR(sklearn.pipeline.Pipeline):\n",
    "#     def __init__(self, num_features=10_000, max_dilations_per_kernel=32, random_state=None, memory=None, verbose=False, **kwargs):\n",
    "#         self.steps = [('minirocketmultivariate', MiniRocketMultivariate(num_features=num_features, \n",
    "#                                                                         max_dilations_per_kernel=max_dilations_per_kernel, \n",
    "#                                                                         random_state=random_state)),\n",
    "#                       ('standardscaler', StandardScaler()),\n",
    "#                       ('logisticregressioncv', LogisticRegressionCV(**kwargs))]\n",
    "#         self.num_features, self.max_dilations_per_kernel, self.random_state = num_features, max_dilations_per_kernel, random_state\n",
    "#         self.kwargs = kwargs\n",
    "#         self.memory = memory\n",
    "#         self.verbose = verbose\n",
    "#         self._validate_steps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Univariate classification with sklearn-type API\n",
    "dsid = 'OliveOil'\n",
    "path = './data'\n",
    "fname = 'MRClassifier'\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_UCR_data(dsid)\n",
    "model = MiniRocketClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.save(fname)\n",
    "del model\n",
    "model = load_minirocket(fname)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9111111111111111"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multivariate classification with sklearn-type API\n",
    "dsid = 'NATOPS'\n",
    "path = './data'\n",
    "fname = 'MRClassifier'\n",
    "\n",
    "X_train, y_train, X_test, y_test = get_UCR_data(dsid)\n",
    "model = MiniRocketClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "model.save(fname)\n",
    "del model\n",
    "model = load_minirocket(fname)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041641458324160906"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Univariate regression with sklearn-type API\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dsid = 'Covid3Month'\n",
    "X_train, y_train, X_test, y_test = get_Monash_data(dsid)\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "model = MiniRocketRegressor(scoring=rmse_scorer)\n",
    "model.fit(X_train, y_train)\n",
    "model.save(fname)\n",
    "del model\n",
    "model = load_minirocket(fname)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.30644069648352"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multivariate regression with sklearn-type API\n",
    "from sklearn.metrics import mean_squared_error\n",
    "dsid = 'AppliancesEnergy'\n",
    "X_train, y_train, X_test, y_test = get_Monash_data(dsid)\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "model = MiniRocketRegressor(scoring=rmse_scorer)\n",
    "model.fit(X_train, y_train)\n",
    "model.save(fname)\n",
    "del model\n",
    "model = load_minirocket(fname)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts(); beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
