{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Callbacks that perform data augmentation by mixing samples in different\n",
    "  ways.\n",
    "output-file: data.mixed_augmentation.html\n",
    "title: Label-mixing transforms\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp data.mixed_augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from torch.distributions.beta import Beta\n",
    "from fastai.callback.core import Callback\n",
    "from fastai.layers import NoneReduce\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def _reduce_loss(loss, reduction='mean'):\n",
    "    \"Reduce the loss based on `reduction`\"\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum() if reduction == 'sum' else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MixHandler1d(Callback):\n",
    "    \"A handler class for implementing mixed sample data augmentation\"\n",
    "    run_valid = False\n",
    "\n",
    "    def __init__(self, alpha=0.5):\n",
    "        self.distrib = Beta(alpha, alpha)\n",
    "\n",
    "    def before_train(self):\n",
    "        self.labeled = self.dls.d\n",
    "        if self.labeled:\n",
    "            self.stack_y = getattr(self.learn.loss_func, 'y_int', False)\n",
    "            if self.stack_y: self.old_lf, self.learn.loss_func = self.learn.loss_func, self.lf\n",
    "\n",
    "    def after_train(self):\n",
    "        if self.labeled and self.stack_y: self.learn.loss_func = self.old_lf\n",
    "\n",
    "    def lf(self, pred, *yb):\n",
    "        if not self.training: return self.old_lf(pred, *yb)\n",
    "        with NoneReduce(self.old_lf) as lf: loss = torch.lerp(lf(pred, *self.yb1), lf(pred, *yb), self.lam)\n",
    "        return _reduce_loss(loss, getattr(self.old_lf, 'reduction', 'mean'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class MixUp1d(MixHandler1d):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412\"\n",
    "\n",
    "    def __init__(self, alpha=.4):\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def before_batch(self):\n",
    "        lam = self.distrib.sample((self.x.size(0), ))\n",
    "        self.lam = torch.max(lam, 1 - lam).to(self.x.device)\n",
    "        shuffle = torch.randperm(self.x.size(0))\n",
    "        xb1 = self.x[shuffle]\n",
    "        self.learn.xb = L(xb1, self.xb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.x.ndim - 1))\n",
    "        if self.labeled:\n",
    "            self.yb1 = tuple((self.y[shuffle], ))\n",
    "            if not self.stack_y: self.learn.yb = L(self.yb1, self.yb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.y.ndim - 1))    \n",
    "                \n",
    "MixUp1D = MixUp1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.learner import *\n",
    "from tsai.models.InceptionTime import *\n",
    "from tsai.data.external import get_UCR_data\n",
    "from tsai.data.core import get_ts_dls, TSCategorize\n",
    "from tsai.data.preprocessing import TSStandardize\n",
    "from tsai.learner import ts_learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.9250454902648926, 1.826296329498291, '00:06']\n"
     ]
    }
   ],
   "source": [
    "X, y, splits = get_UCR_data('NATOPS', return_split=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, InceptionTime, cbs=MixUp1d(0.4))\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class CutMix1d(MixHandler1d):\n",
    "    \"Implementation of `https://arxiv.org/abs/1905.04899`\"\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__(alpha)\n",
    "\n",
    "    def before_batch(self):\n",
    "        bs, *_, seq_len = self.x.size()\n",
    "        self.lam = self.distrib.sample((1, ))\n",
    "        shuffle = torch.randperm(bs)\n",
    "        xb1 = self.x[shuffle]\n",
    "        x1, x2 = self.rand_bbox(seq_len, self.lam)\n",
    "        self.learn.xb[0][..., x1:x2] = xb1[..., x1:x2]\n",
    "        self.lam = (1 - (x2 - x1) / float(seq_len)).item()\n",
    "        if self.labeled:\n",
    "            self.yb1 = tuple((self.y[shuffle], ))\n",
    "            if not self.stack_y:\n",
    "                self.learn.yb = tuple(L(self.yb1, self.yb).map_zip(torch.lerp, weight=unsqueeze(self.lam, n=self.y.ndim - 1)))\n",
    "\n",
    "    def rand_bbox(self, seq_len, lam):\n",
    "        cut_seq_len = torch.round(seq_len * (1. - lam)).type(torch.long)\n",
    "        half_cut_seq_len = torch.div(cut_seq_len, 2, rounding_mode='floor')\n",
    "\n",
    "        # uniform\n",
    "        cx = torch.randint(0, seq_len, (1, ))\n",
    "        x1 = torch.clamp(cx - half_cut_seq_len, 0, seq_len)\n",
    "        x2 = torch.clamp(cx + half_cut_seq_len, 0, seq_len)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class IntraClassCutMix1d(Callback):\n",
    "    \"Implementation of CutMix applied to examples of the same class\"\n",
    "    run_valid = False\n",
    "\n",
    "    def __init__(self, alpha=1.):\n",
    "        self.distrib = Beta(tensor(alpha), tensor(alpha))\n",
    "\n",
    "    def before_batch(self):\n",
    "        bs, *_, seq_len = self.x.size()\n",
    "        idxs = torch.arange(bs, device=self.x.device)\n",
    "        y = torch.tensor(self.y)\n",
    "        unique_c = torch.unique(y).tolist()\n",
    "        idxs_by_class = torch.cat([idxs[torch.eq(y, c)] for c in unique_c])\n",
    "        idxs_shuffled_by_class = torch.cat([random_shuffle(idxs[torch.eq(y, c)]) for c in unique_c])\n",
    "        self.lam = self.distrib.sample((1, ))\n",
    "        x1, x2 = self.rand_bbox(seq_len, self.lam)\n",
    "        xb1 = self.x[idxs_shuffled_by_class]\n",
    "        self.learn.xb[0][idxs_by_class, :, x1:x2] = xb1[..., x1:x2]\n",
    "\n",
    "    def rand_bbox(self, seq_len, lam):\n",
    "        cut_seq_len = torch.round(seq_len * (1. - lam)).type(torch.long)\n",
    "        half_cut_seq_len = torch.div(cut_seq_len, 2, rounding_mode='floor')\n",
    "\n",
    "        # uniform\n",
    "        cx = torch.randint(0, seq_len, (1, ))\n",
    "        x1 = torch.clamp(cx - half_cut_seq_len, 0, seq_len)\n",
    "        x2 = torch.clamp(cx + half_cut_seq_len, 0, seq_len)\n",
    "        return x1, x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.781386375427246, 1.7941926717758179, '00:04']\n"
     ]
    }
   ],
   "source": [
    "X, y, splits = get_UCR_data('NATOPS', split_data=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, InceptionTime, cbs=IntraClassCutMix1d())\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1.7089701890945435, 1.777895450592041, '00:05']\n"
     ]
    }
   ],
   "source": [
    "X, y, splits = get_UCR_data('NATOPS', split_data=False)\n",
    "tfms = [None, TSCategorize()]\n",
    "batch_tfms = TSStandardize()\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, batch_tfms=batch_tfms)\n",
    "learn = ts_learner(dls, cbs=CutMix1d(1.))\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "from tsai.imports import create_scripts\n",
    "from tsai.export import get_nb_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|eval: false\n",
    "#|hide\n",
    "nb_name = get_nb_name()\n",
    "create_scripts(nb_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('py37torch110')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
