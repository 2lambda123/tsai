{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "> This contains a set of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "from tsai.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "# #hide\n",
    "# \"\"\" Lookahead Optimizer Wrapper.\n",
    "# Implemented by Mikhail Grankin (Github: mgrankin) in his excellent collection of Pytorch optimizers https://github.com/mgrankin/over9000\n",
    "# Implementation modified from: https://github.com/alphadl/lookahead.pytorch\n",
    "# Paper: `Lookahead Optimizer: k steps forward, 1 step back` - https://arxiv.org/abs/1907.08610\n",
    "# \"\"\"\n",
    "# class LookAhead(torch.optim.Optimizer):\n",
    "#     def __init__(self, base_optimizer, alpha=0.5, k=6):\n",
    "#         if not 0.0 <= alpha <= 1.0: raise ValueError(f'Invalid slow update rate: {alpha}')\n",
    "#         if not 1 <= k: raise ValueError(f'Invalid lookahead steps: {k}')\n",
    "#         defaults = dict(lookahead_alpha=alpha, lookahead_k=k, lookahead_step=0)\n",
    "#         self.base_optimizer = base_optimizer\n",
    "#         self.param_groups = self.base_optimizer.param_groups\n",
    "#         self.defaults = base_optimizer.defaults\n",
    "#         self.defaults.update(defaults)\n",
    "#         self.state = defaultdict(dict)\n",
    "#         # manually add our defaults to the param groups\n",
    "#         for name, default in defaults.items():\n",
    "#             for group in self.param_groups: group.setdefault(name, default)\n",
    "\n",
    "#     def update_slow(self, group):\n",
    "#         for fast_p in group[\"params\"]:\n",
    "#             if fast_p.grad is None: continue\n",
    "#             param_state = self.state[fast_p]\n",
    "#             if 'slow_buffer' not in param_state:\n",
    "#                 param_state['slow_buffer'] = torch.empty_like(fast_p.data)\n",
    "#                 param_state['slow_buffer'].copy_(fast_p.data)\n",
    "#             slow = param_state['slow_buffer']\n",
    "#             slow.add_(group['lookahead_alpha'], fast_p.data - slow)\n",
    "#             fast_p.data.copy_(slow)\n",
    "\n",
    "#     def sync_lookahead(self):\n",
    "#         for group in self.param_groups:\n",
    "#             self.update_slow(group)\n",
    "\n",
    "#     def step(self, closure=None):\n",
    "#         loss = self.base_optimizer.step(closure)\n",
    "#         for group in self.param_groups:\n",
    "#             group['lookahead_step'] += 1\n",
    "#             if group['lookahead_step'] % group['lookahead_k'] == 0: self.update_slow(group)\n",
    "#         return loss\n",
    "\n",
    "#     def state_dict(self):\n",
    "#         fast_state_dict = self.base_optimizer.state_dict()\n",
    "#         slow_state = {(id(k) if isinstance(k, torch.Tensor) else k): v for k, v in self.state.items()}\n",
    "#         fast_state = fast_state_dict['state']\n",
    "#         param_groups = fast_state_dict['param_groups']\n",
    "#         return {'state': fast_state, 'slow_state': slow_state, 'param_groups': param_groups}\n",
    "\n",
    "#     def load_state_dict(self, state_dict):\n",
    "#         fast_state_dict = {'state': state_dict['state'], 'param_groups': state_dict['param_groups']}\n",
    "#         self.base_optimizer.load_state_dict(fast_state_dict)\n",
    "\n",
    "#         # We want to restore the slow state, but share param_groups reference\n",
    "#         # with base_optimizer. This is a bit redundant but least code\n",
    "#         slow_state_new = False\n",
    "#         if 'slow_state' not in state_dict:\n",
    "#             print('Loading state_dict from optimizer without Lookahead applied.')\n",
    "#             state_dict['slow_state'] = defaultdict(dict)\n",
    "#             slow_state_new = True\n",
    "#         slow_state_dict = {\n",
    "#             'state': state_dict['slow_state'],\n",
    "#             'param_groups': state_dict['param_groups'],  # this is pointless but saves code\n",
    "#         }\n",
    "#         super(Lookahead, self).load_state_dict(slow_state_dict)\n",
    "#         self.param_groups = self.base_optimizer.param_groups  # make both ref same container\n",
    "#         if slow_state_new:\n",
    "#             # reapply defaults to catch missing lookahead specific ones\n",
    "#             for name, default in self.defaults.items():\n",
    "#                 for group in self.param_groups:\n",
    "#                     group.setdefault(name, default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def wrap_optimizer(opt, **kwargs): return partial(OptimWrapper, opt=opt, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "'''Implemented by Mikhail Grankin (Github: mgrankin) in his excellent collection of Pytorch optimizers https://github.com/mgrankin/over9000'''\n",
    "\n",
    "# RAdam + LARS\n",
    "class Ralamb(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(Ralamb, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Ralamb, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None: loss = closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse: raise RuntimeError('Ralamb does not support sparse gradients')\n",
    "                p_data_fp32 = p.data.float()\n",
    "                state = self.state[p]\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # Decay the first and second moment running average coefficient\n",
    "                # m_t\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                # v_t\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]: N_sma, radam_step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2**state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        radam_step_size = math.sqrt(\n",
    "                            (1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) *\n",
    "                            (N_sma - 2) / N_sma * N_sma_max /\n",
    "                            (N_sma_max - 2)) / (1 - beta1**state['step'])\n",
    "                    else:\n",
    "                        radam_step_size = 1.0 / (1 - beta1**state['step'])\n",
    "                    buffered[2] = radam_step_size\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "                # more conservative since it's an approximated value\n",
    "                radam_step = p_data_fp32.clone()\n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    radam_step.addcdiv_(-radam_step_size * group['lr'], exp_avg, denom)\n",
    "                else:\n",
    "                    radam_step.add_(-radam_step_size * group['lr'], exp_avg)\n",
    "                radam_norm = radam_step.pow(2).sum().sqrt()\n",
    "                weight_norm = p.data.pow(2).sum().sqrt().clamp(0, 10)\n",
    "                if weight_norm == 0 or radam_norm == 0: trust_ratio = 1\n",
    "                else: trust_ratio = weight_norm / radam_norm\n",
    "                state['weight_norm'] = weight_norm\n",
    "                state['adam_norm'] = radam_norm\n",
    "                state['trust_ratio'] = trust_ratio\n",
    "                if N_sma >= 5:\n",
    "                    p_data_fp32.addcdiv_(-radam_step_size * group['lr'] * trust_ratio, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-radam_step_size * group['lr'] * trust_ratio, exp_avg)\n",
    "                p.data.copy_(p_data_fp32)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "ralamb = wrap_optimizer(Ralamb)\n",
    "\n",
    "def RangerLars(params, *args, alpha=0.5, k=6, **kwargs):\n",
    "    ralamb = Ralamb(params, *args, **kwargs)\n",
    "    return LookAhead(ralamb, alpha, k)\n",
    "\n",
    "rangerlars = wrap_optimizer(RangerLars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# All credit to https://github.com/juntang-zhuang/Adabelief-Optimizer/blob/master/PyTorch_Experiments/AdaBelief.py\n",
    "\n",
    "_version_higher = ( torch.__version__ >= \"1.5.0\" )\n",
    "\n",
    "class AdaBelief(Optimizer):\n",
    "    r\"\"\"Implements AdaBelief algorithm. Modified from Adam in PyTorch\n",
    "    Arguments:\n",
    "        params (iterable): iterable of parameters to optimize or dicts defining\n",
    "            parameter groups\n",
    "        lr (float, optional): learning rate (default: 1e-3)\n",
    "        betas (Tuple[float, float], optional): coefficients used for computing\n",
    "            running averages of gradient and its square (default: (0.9, 0.999))\n",
    "        eps (float, optional): term added to the denominator to improve\n",
    "            numerical stability (default: 1e-8)\n",
    "        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "        amsgrad (boolean, optional): whether to use the AMSGrad variant of this\n",
    "            algorithm from the paper `On the Convergence of Adam and Beyond`_\n",
    "            (default: False)\n",
    "        weight_decouple (boolean, optional): ( default: False) If set as True, then\n",
    "            the optimizer uses decoupled weight decay as in AdamW\n",
    "        fixed_decay (boolean, optional): (default: False) This is used when weight_decouple\n",
    "            is set as True.\n",
    "            When fixed_decay == True, the weight decay is performed as\n",
    "            $W_{new} = W_{old} - W_{old} \\times decay$.\n",
    "            When fixed_decay == False, the weight decay is performed as\n",
    "            $W_{new} = W_{old} - W_{old} \\times decay \\times lr$. Note that in this case, the\n",
    "            weight decay ratio decreases with learning rate (lr).\n",
    "        rectify (boolean, optional): (default: False) If set as True, then perform the rectified\n",
    "            update similar to RAdam\n",
    "    reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients\n",
    "               NeurIPS 2020 Spotlight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,\n",
    "                 weight_decay=0, amsgrad=False, weight_decouple = False, fixed_decay=False, rectify = False ):\n",
    "        if not 0.0 <= lr:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if not 0.0 <= eps:\n",
    "            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n",
    "        if not 0.0 <= betas[0] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n",
    "        if not 0.0 <= betas[1] < 1.0:\n",
    "            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps,\n",
    "                        weight_decay=weight_decay, amsgrad=amsgrad)\n",
    "        super(AdaBelief, self).__init__(params, defaults)\n",
    "\n",
    "        self.weight_decouple = weight_decouple\n",
    "        self.rectify = rectify\n",
    "        self.fixed_decay = fixed_decay\n",
    "        if self.weight_decouple:\n",
    "            print('Weight decoupling enabled in AdaBelief')\n",
    "            if self.fixed_decay:\n",
    "                print('Weight decay fixed')\n",
    "        if self.rectify:\n",
    "            print('Rectification enabled in AdaBelief')\n",
    "        if amsgrad:\n",
    "            print('AMS enabled in AdaBelief')\n",
    "    def __setstate__(self, state):\n",
    "        super(AdaBelief, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('amsgrad', False)\n",
    "\n",
    "    def reset(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                state = self.state[p]\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                # State initialization\n",
    "                state['step'] = 0\n",
    "                # Exponential moving average of gradient values\n",
    "                state['exp_avg'] = torch.zeros_like(p.data,\n",
    "                                   memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "                # Exponential moving average of squared gradient values\n",
    "                state['exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "                if amsgrad:\n",
    "                    # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                    state['max_exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('AdaBelief does not support sparse gradients, please consider SparseAdam instead')\n",
    "                amsgrad = group['amsgrad']\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                # State initialization\n",
    "                if len(state) == 0:\n",
    "                    state['rho_inf'] = 2.0 / (1.0 - beta2) - 1.0\n",
    "                    state['step'] = 0\n",
    "                    # Exponential moving average of gradient values\n",
    "                    state['exp_avg'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "                    # Exponential moving average of squared gradient values\n",
    "                    state['exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "                    if amsgrad:\n",
    "                        # Maintains max of all exp. moving avg. of sq. grad. values\n",
    "                        state['max_exp_avg_var'] = torch.zeros_like(p.data,\n",
    "                                    memory_format=torch.preserve_format) if _version_higher else torch.zeros_like(p.data)\n",
    "\n",
    "                # get current state variable\n",
    "                exp_avg, exp_avg_var = state['exp_avg'], state['exp_avg_var']\n",
    "\n",
    "                state['step'] += 1\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "\n",
    "                # perform weight decay, check if decoupled weight decay\n",
    "                if self.weight_decouple:\n",
    "                    if not self.fixed_decay:\n",
    "                        p.data.mul_(1.0 - group['lr'] * group['weight_decay'])\n",
    "                    else:\n",
    "                        p.data.mul_(1.0 - group['weight_decay'])\n",
    "                else:\n",
    "                    if group['weight_decay'] != 0:\n",
    "                        grad.add_(group['weight_decay'], p.data)\n",
    "\n",
    "                # Update first and second moment running average\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "                grad_residual = grad - exp_avg\n",
    "                exp_avg_var.mul_(beta2).addcmul_(1 - beta2, grad_residual, grad_residual)\n",
    "\n",
    "                if amsgrad:\n",
    "                    max_exp_avg_var = state['max_exp_avg_var']\n",
    "                    # Maintains the maximum of all 2nd moment running avg. till now\n",
    "                    torch.max(max_exp_avg_var, exp_avg_var, out=max_exp_avg_var)\n",
    "\n",
    "                    # Use the max. for normalizing running avg. of gradient\n",
    "                    denom = (max_exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "                else:\n",
    "                    denom = (exp_avg_var.add_(group['eps']).sqrt() / math.sqrt(bias_correction2)).add_(group['eps'])\n",
    "\n",
    "                if not self.rectify:\n",
    "                    # Default update\n",
    "                    step_size = group['lr'] / bias_correction1\n",
    "                    p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                else:# Rectified update\n",
    "                    # calculate rho_t\n",
    "                    state['rho_t'] = state['rho_inf'] - 2 * state['step'] * beta2 ** state['step'] / (\n",
    "                            1.0 - beta2 ** state['step'])\n",
    "\n",
    "                    if state['rho_t'] > 4: # perform Adam style update if variance is small\n",
    "                        rho_inf, rho_t = state['rho_inf'], state['rho_t']\n",
    "                        rt = (rho_t - 4.0) * (rho_t - 2.0) * rho_inf / (rho_inf - 4.0) / (rho_inf - 2.0) / rho_t\n",
    "                        rt = math.sqrt(rt)\n",
    "\n",
    "                        step_size = rt * group['lr'] / bias_correction1\n",
    "\n",
    "                        p.data.addcdiv_(-step_size, exp_avg, denom)\n",
    "\n",
    "                    else: # perform SGD style update\n",
    "                        p.data.add_( -group['lr'], exp_avg)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "adabelief = wrap_optimizer(AdaBelief)\n",
    "adamw = wrap_optimizer(torch.optim.AdamW)\n",
    "optimizers = [Adam, RMSProp, RAdam, Lamb, SGD, adamw, adabelief, ralamb, rangerlars, ranger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.save_checkpoint();"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 000_utils.ipynb.\n",
      "Converted 000b_data.validation.ipynb.\n",
      "Converted 000c_data.preparation.ipynb.\n",
      "Converted 001_data.external.ipynb.\n",
      "Converted 002_data.core.ipynb.\n",
      "Converted 002b_data.unwindowed.ipynb.\n",
      "Converted 002c_data.metadatasets.ipynb.\n",
      "Converted 003_data.preprocessing.ipynb.\n",
      "Converted 003b_data.transforms.ipynb.\n",
      "Converted 003c_data.mixed_augmentation.ipynb.\n",
      "Converted 003d_data.image.ipynb.\n",
      "Converted 003e_data.features.ipynb.\n",
      "Converted 005_data.tabular.ipynb.\n",
      "Converted 006_data.mixed.ipynb.\n",
      "Converted 007_metrics.ipynb.\n",
      "Converted 008_learner.ipynb.\n",
      "Converted 008b_tslearner.ipynb.\n",
      "Converted 009_optimizer.ipynb.\n",
      "Converted 010_callback.core.ipynb.\n",
      "Converted 011_callback.noisy_student.ipynb.\n",
      "Converted 012_callback.gblend.ipynb.\n",
      "Converted 013_callback.MVP.ipynb.\n",
      "Converted 014_callback.PredictionDynamics.ipynb.\n",
      "Converted 100_models.layers.ipynb.\n",
      "Converted 100b_models.utils.ipynb.\n",
      "Converted 100c_models.explainability.ipynb.\n",
      "Converted 101_models.ResNet.ipynb.\n",
      "Converted 101b_models.ResNetPlus.ipynb.\n",
      "Converted 102_models.InceptionTime.ipynb.\n",
      "Converted 102b_models.InceptionTimePlus.ipynb.\n",
      "Converted 103_models.MLP.ipynb.\n",
      "Converted 103b_models.FCN.ipynb.\n",
      "Converted 103c_models.FCNPlus.ipynb.\n",
      "Converted 104_models.ResCNN.ipynb.\n",
      "Converted 105_models.RNN.ipynb.\n",
      "Converted 105_models.RNNPlus.ipynb.\n",
      "Converted 106_models.XceptionTime.ipynb.\n",
      "Converted 106b_models.XceptionTimePlus.ipynb.\n",
      "Converted 107_models.RNN_FCN.ipynb.\n",
      "Converted 107b_models.RNN_FCNPlus.ipynb.\n",
      "Converted 108_models.TransformerModel.ipynb.\n",
      "Converted 108b_models.TST.ipynb.\n",
      "Converted 108c_models.TSTPlus.ipynb.\n",
      "Converted 109_models.OmniScaleCNN.ipynb.\n",
      "Converted 110_models.mWDN.ipynb.\n",
      "Converted 111_models.ROCKET.ipynb.\n",
      "Converted 111b_models.MINIROCKET.ipynb.\n",
      "Converted 112_models.XResNet1d.ipynb.\n",
      "Converted 112b_models.XResNet1dPlus.ipynb.\n",
      "Converted 113_models.TCN.ipynb.\n",
      "Converted 114_models.XCM.ipynb.\n",
      "Converted 114b_models.XCMPlus.ipynb.\n",
      "Converted 120_models.TabModel.ipynb.\n",
      "Converted 121_models.TabTransformer.ipynb.\n",
      "Converted 122_models.TabFusionTransformer.ipynb.\n",
      "Converted 130_models.MultiInputNet.ipynb.\n",
      "Converted 140_models.misc.ipynb.\n",
      "Converted 900_tutorials.ipynb.\n",
      "Converted index.ipynb.\n",
      "\n",
      "\n",
      "Checking folder: /Users/nacho/Documents/Machine_Learning/Jupyter_Notebooks/tsai/tsai\n",
      "Correct conversion! 😃\n",
      "Total time elapsed 232 s\n",
      "Thursday 01/04/21 22:41:05 CEST\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
