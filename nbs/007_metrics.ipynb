{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "> This contains metrics not included in fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from fastai.metrics import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "mk_class('ActivationType', **{o:o.lower() for o in ['No', 'Sigmoid', 'Softmax', 'BinarySoftmax']},\n",
    "         doc=\"All possible activation classes for `AccumMetric\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def MatthewsCorrCoefBinary(thresh=.5, sample_weight=None):\n",
    "    \"Matthews correlation coefficient for single-label classification problems\"\n",
    "    return skm_to_fastai(skm.matthews_corrcoef, activation=ActivationType.BinarySoftmax, thresh=thresh, sample_weight=sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _avg_R(inp, targ):\n",
    "    \"Compute average R\"\n",
    "    return targ[inp > 0].mean()\n",
    "avg_R = AccumMetric(_avg_R, to_np=True)\n",
    "\n",
    "def _perc(inp, targ):\n",
    "    \"Compute percent\"\n",
    "    return (inp > 0).mean()\n",
    "perc = AccumMetric(_perc, to_np=True)\n",
    "\n",
    "def _win_perc(inp, targ):\n",
    "    \"Compute winning percent\"\n",
    "    return (targ[inp > 0] > 0).mean()\n",
    "win_perc = AccumMetric(_win_perc, to_np=True)\n",
    "\n",
    "def _SQN100(inp, targ):\n",
    "    \"Compute SQN-100\"\n",
    "    if (inp > 0).sum() >= 2: sqn100 = 10 * targ[inp > 0].mean() / targ[inp > 0].std()\n",
    "    else: return np.nan\n",
    "    return sqn100\n",
    "SQN100 = AccumMetric(_SQN100, to_np=True)\n",
    "\n",
    "def _SQNyr(inp, targ, per_year=100):\n",
    "    \"Compute SQN-year\"\n",
    "    if (inp > 0).sum() >= 2: return np.sqrt(per_year * (inp > 0).sum() / len(inp)) * targ[inp > 0].mean() / targ[inp > 0].std()\n",
    "    else: return np.nan\n",
    "\n",
    "def SQNyr(per_year=100):\n",
    "    return AccumMetric(_SQNyr, per_year=per_year, to_np=True)\n",
    "\n",
    "def _tstat(inp, targ):\n",
    "    \"Compute t-stat based on 2 independent sample distributions (regression and binary tasks)\"\n",
    "    if inp.ndim > 1: inp = inp[:, -1]\n",
    "    return scipy.stats.ttest_ind(targ[inp > 0], targ[inp <= 0])[0]\n",
    "tstat = AccumMetric(_tstat, to_np=True)\n",
    "\n",
    "def _tstat_zero(inp, targ):\n",
    "    \"Compute t-stat based on 2 independent sample distributions (regression and binary tasks)\"\n",
    "    if inp.ndim > 1: inp = inp[:, -1]\n",
    "    return scipy.stats.ttest_ind(targ[inp > 0], torch.zeros((inp > 0).sum()))[0]\n",
    "tstat_zero = AccumMetric(_tstat, to_np=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _total_est_Rs(inp, targ, avg_win=1, avg_loss=-1, per_year=100, thresh=0.5):\n",
    "    \"Compute total Rs / year\"\n",
    "    win_perc = targ[inp].mean() if inp.sum() > 0 else 0\n",
    "    total = per_year * inp.mean() \n",
    "    return int(win_perc * total * avg_win + (1 - win_perc) * total * avg_loss)\n",
    "\n",
    "def total_est_Rs(avg_win=1, avg_loss=-1, per_year=100, thresh=0.5):\n",
    "    return AccumMetric(_total_est_Rs, flatten=False, avg_win=avg_win, avg_loss=avg_loss, per_year=per_year, \n",
    "                       to_np=True, activation=ActivationType.BinarySoftmax, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _ΔR_bin_perc(inp, targ, value=None, thresh=0.5):\n",
    "    \"Τotal incremental Rs relative to optimal (1) and average (0)\"\n",
    "    ΔR_target = value[(targ == 1)].mean() - value.mean()\n",
    "    ΔR_pred = value[(inp == 1)].mean() - value.mean()\n",
    "    return min(1, ΔR_pred/ΔR_target) * min(1, inp.sum()/targ.sum())\n",
    "\n",
    "def ΔR_bin_perc(value=None, thresh=0.5):\n",
    "    return AccumMetric(_ΔR_bin_perc, value=value, flatten=False, to_np=True, activation=ActivationType.BinarySoftmax, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _avgR_class(inp, targ, value=None):\n",
    "    \"Compute average R in classification tasks - unshuffled\"\n",
    "    return value[inp].mean()\n",
    "\n",
    "def avgR_class(value, thresh=.5):\n",
    "    return AccumMetric(_avgR_class, value=value, flatten=False, activation=ActivationType.BinarySoftmax, to_np=True, thresh=thresh)\n",
    "\n",
    "def _perc_class(inp, targ):\n",
    "    \"Compute percent > .5\"\n",
    "    return inp.mean()\n",
    "\n",
    "def perc_class(thresh=.5):\n",
    "    return AccumMetric(_perc_class, flatten=False, activation=ActivationType.BinarySoftmax, to_np=True, thresh=thresh)\n",
    "\n",
    "def _win_perc_class(inp, targ, value=None):\n",
    "    \"Compute winning percent in classification tasks - unshuffled\"\n",
    "    return (value[inp] > 0).mean()\n",
    "\n",
    "def win_perc_class(value, thresh=.5):\n",
    "    return AccumMetric(_win_perc_class, value=value, flatten=False, activation=ActivationType.BinarySoftmax, to_np=True, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def _tstat_bin_probas(inp, targ):\n",
    "    \"Compute t-stat based on binary probas\"\n",
    "    return scipy.stats.ttest_ind(inp[targ == 1], inp[targ == 0])[0]\n",
    "t_stat_binary = AccumMetric(_tstat_bin_probas, to_np=True, activation=ActivationType.BinarySoftmax)\n",
    "\n",
    "def _tstat_bin_ext(inp, targ, ext=None):\n",
    "    \"Compute t-stat for an external variable based on binary probas\"\n",
    "    return scipy.stats.ttest_ind(ext[inp == 1], ext[inp == 0])[0]\n",
    "\n",
    "def tstat_bin_ext(ext, thresh=.5):\n",
    "    return AccumMetric(_tstat_bin_ext, ext=ext, flatten=False, activation=ActivationType.BinarySoftmax, to_np=True, thresh=thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_task_metrics(dls, binary_metrics=None, multi_class_metrics=None, regression_metrics=None, verbose=True): \n",
    "    if dls.c == 2: \n",
    "        pv('binary-classification task', verbose)\n",
    "        return binary_metrics\n",
    "    elif dls.c > 2: \n",
    "        pv('multi-class task', verbose)\n",
    "        return multi_class_metrics\n",
    "    else:\n",
    "        pv('regression task', verbose)\n",
    "        return regression_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "create_scripts()\n",
    "beep()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
