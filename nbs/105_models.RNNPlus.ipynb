{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.RNNPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNPlus\n",
    "\n",
    "> These are RNN, LSTM and GRU PyTorch implementations created by Ignacio Oguiza - timeseriesAI@gmail.com based on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.data.core import *\n",
    "from tsai.models.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# example\n",
    "# rnn = nn.LSTM(10, 20, 2)\n",
    "# input = torch.randn(5, 3, 10)\n",
    "# h0 = torch.randn(2, 3, 20)\n",
    "# c0 = torch.randn(2, 3, 20)\n",
    "# output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": true
   },
   "outputs": [],
   "source": [
    "# #hide\n",
    "# class _RNNPlus_Base(Module):\n",
    "#     def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, n_layers=1, bias=True, rnn_dropout=0, bidirectional=False, fc_dropout=0., \n",
    "#                  last_step=True, bn=False, custom_head=None, y_range=None, **kwargs):\n",
    "#         if not last_step: assert seq_len, 'you need to enter a seq_len to use flatten=True'\n",
    "#         self.rnn = self._cell(c_in, hidden_size, num_layers=n_layers, bias=bias, batch_first=True, dropout=rnn_dropout, bidirectional=bidirectional)\n",
    "#         self.transpose = Transpose(-1, -2, contiguous=True)\n",
    "        \n",
    "#         # Head\n",
    "#         self.head_nf = hidden_size * (1 + bidirectional)\n",
    "#         if custom_head: self.head = custom_head(self.head_nf, c_out, seq_len) # custom head must have all required kwargs\n",
    "#         else: self.head = self.create_head(self.head_nf, c_out, seq_len, last_step=last_step, fc_dropout=fc_dropout, bn=bn, y_range=y_range)\n",
    "\n",
    "#     def forward(self, x): \n",
    "#         x = x.transpose(2,1)                               # [batch_size x n_vars x seq_len] --> [batch_size x seq_len x n_vars]\n",
    "#         output, _ = self.rnn(x)                            # [batch_size x seq_len x hidden_size * (1 + bidirectional)]\n",
    "#         output = self.transpose(output)                    # [batch_size x hidden_size * (1 + bidirectional) x seq_len]\n",
    "#         return self.head(output)\n",
    "    \n",
    "#     def create_head(self, nf, c_out, seq_len, last_step=True, fc_dropout=0., bn=False, y_range=None):\n",
    "#         if last_step: \n",
    "#             layers = [LastStep()]\n",
    "#         else: \n",
    "#             layers = [Flatten()]\n",
    "#             nf *= seq_len\n",
    "#         layers += [LinBnDrop(nf, c_out, bn=bn, p=fc_dropout)]\n",
    "#         if y_range: layers += [SigmoidRange(*y_range)]\n",
    "#         return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "# class RNNPlus(_RNNPlus_Base):\n",
    "#     _cell = nn.RNN\n",
    "    \n",
    "# class LSTMPlus(_RNNPlus_Base):\n",
    "#     _cell = nn.LSTM\n",
    "    \n",
    "# class GRUPlus(_RNNPlus_Base):\n",
    "#     _cell = nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class _RNNPlus_Base(Module):\n",
    "    def __init__(self, c_in, c_out, seq_len=None, hidden_size=100, n_layers=1, bias=True, rnn_dropout=0, bidirectional=False, fc_dropout=0., \n",
    "                 last_step=True, bn=False, custom_head=None, y_range=None, **kwargs):\n",
    "        \n",
    "        if not last_step: assert seq_len, 'you need to enter a seq_len to use flatten=True'\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = _RNN_Backbone(self._cell, c_in, c_out, seq_len=seq_len, hidden_size=hidden_size, n_layers=n_layers, bias=bias, \n",
    "                                      rnn_dropout=rnn_dropout,  bidirectional=bidirectional)\n",
    "        \n",
    "        # Head\n",
    "        self.head_nf = hidden_size * (1 + bidirectional)\n",
    "        if custom_head: self.head = custom_head(self.head_nf, c_out, seq_len) # custom head must have all required kwargs\n",
    "        else: self.head = self.create_head(self.head_nf, c_out, seq_len, last_step=last_step, fc_dropout=fc_dropout, bn=bn, y_range=y_range)\n",
    "\n",
    "    def forward(self, x): \n",
    "        output = self.backbone(x)        # [batch_size x n_vars x seq_len] --> [batch_size x hidden_size * (1 + bidirectional) x seq_len]\n",
    "        return self.head(output)\n",
    "    \n",
    "    def create_head(self, nf, c_out, seq_len, last_step=True, fc_dropout=0., bn=False, y_range=None):\n",
    "        if last_step: \n",
    "            layers = [LastStep()]\n",
    "        else: \n",
    "            layers = [Flatten()]\n",
    "            nf *= seq_len\n",
    "        layers += [LinBnDrop(nf, c_out, bn=bn, p=fc_dropout)]\n",
    "        if y_range: layers += [SigmoidRange(*y_range)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    \n",
    "class _RNN_Backbone(Module):\n",
    "    def __init__(self, cell, c_in, c_out, seq_len=None, hidden_size=100, n_layers=1, bias=True, rnn_dropout=0, bidirectional=False):\n",
    "        self.rnn = cell(c_in, hidden_size, num_layers=n_layers, bias=bias, batch_first=True, dropout=rnn_dropout, bidirectional=bidirectional)\n",
    "        self.transpose = Transpose(-1, -2, contiguous=True)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        x = x.transpose(2,1)                               # [batch_size x n_vars x seq_len] --> [batch_size x seq_len x n_vars]\n",
    "        output, _ = self.rnn(x)                            # [batch_size x seq_len x hidden_size * (1 + bidirectional)]\n",
    "        output = self.transpose(output)                    # [batch_size x hidden_size * (1 + bidirectional) x seq_len]\n",
    "        return output\n",
    "\n",
    "class RNNPlus(_RNNPlus_Base):\n",
    "    _cell = nn.RNN\n",
    "    \n",
    "class LSTMPlus(_RNNPlus_Base):\n",
    "    _cell = nn.LSTM\n",
    "    \n",
    "class GRUPlus(_RNNPlus_Base):\n",
    "    _cell = nn.GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "c_in = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, c_in, seq_len)\n",
    "test_eq(RNNPlus(c_in, c_out)(xb).shape, [bs, c_out])\n",
    "test_eq(RNNPlus(c_in, c_out, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5)(xb).shape, [bs, c_out])\n",
    "test_eq(LSTMPlus(c_in, c_out, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5)(xb).shape, [bs, c_out])\n",
    "test_eq(GRUPlus(c_in, c_out, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5)(xb).shape, [bs, c_out])\n",
    "test_eq(RNNPlus(c_in, c_out, seq_len, flatten=True)(xb).shape, [bs, c_out])\n",
    "test_eq(RNNPlus(c_in, c_out, seq_len, flatten=True)(xb).shape, [bs, c_out])\n",
    "test_eq(RNNPlus(c_in, c_out, seq_len, hidden_size=100, n_layers=2, bias=True, rnn_dropout=0.2, bidirectional=True, fc_dropout=0.5, flatten=True)(xb).shape, \n",
    "        [bs, c_out])\n",
    "test_eq(LSTMPlus(c_in, c_out, seq_len, flatten=True)(xb).shape, [bs, c_out])\n",
    "test_eq(GRUPlus(c_in, c_out, seq_len, last_step=False, flatten=True)(xb).shape, [bs, c_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "c_in = 3\n",
    "seq_len = 12\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, c_in, seq_len)\n",
    "custom_head = partial(create_mlp_head, fc_dropout=0.5)\n",
    "test_eq(LSTMPlus(c_in, c_out, seq_len, flatten=True, custom_head=custom_head)(xb).shape, [bs, c_out])\n",
    "custom_head = partial(create_pool_head, concat_pool=True, fc_dropout=0.5)\n",
    "test_eq(LSTMPlus(c_in, c_out, seq_len, last_step=False, flatten=False, custom_head=custom_head)(xb).shape, [bs, c_out])\n",
    "custom_head = partial(create_pool_plus_head, fc_dropout=0.5)\n",
    "test_eq(LSTMPlus(c_in, c_out, seq_len, last_step=False, flatten=False, custom_head=custom_head)(xb).shape, [bs, c_out])\n",
    "custom_head = partial(create_conv_head)\n",
    "test_eq(LSTMPlus(c_in, c_out, seq_len, last_step=False, flatten=False, custom_head=custom_head)(xb).shape, [bs, c_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsai.data.all import *\n",
    "from tsai.models.utils import *\n",
    "dsid = 'NATOPS' \n",
    "bs = 16\n",
    "X, y, splits = get_UCR_data(dsid, return_split=False)\n",
    "tfms  = [None, [Categorize()]]\n",
    "dls = get_ts_dls(X, y, tfms=tfms, splits=splits, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): LastStep()\n",
      "  (1): LinBnDrop(\n",
      "    (0): Linear(in_features=100, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.816546</td>\n",
       "      <td>1.795204</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_ts_model(LSTMPlus, dls=dls)\n",
    "print(model[-1])\n",
    "learn = Learner(dls, model,  metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.313294</td>\n",
       "      <td>0.804326</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LSTMPlus(dls.vars, dls.c, dls.len, last_step=False, flatten=True)\n",
    "learn = Learner(dls, model,  metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.727251</td>\n",
       "      <td>1.642483</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_head = partial(create_pool_head, concat_pool=True)\n",
    "model = LSTMPlus(dls.vars, dls.c, dls.len, last_step=False, flatten=False, custom_head=custom_head)\n",
    "learn = Learner(dls, model,  metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.230085</td>\n",
       "      <td>1.586665</td>\n",
       "      <td>0.588889</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "custom_head = partial(create_pool_plus_head, concat_pool=True)\n",
    "model = LSTMPlus(dls.vars, dls.c, dls.len, last_step=False, flatten=False, custom_head=custom_head)\n",
    "learn = Learner(dls, model,  metrics=accuracy)\n",
    "learn.fit_one_cycle(1, 3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNNPlus(\n",
      "  (backbone): _RNN_Backbone(\n",
      "    (rnn): RNN(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (transpose): Transpose(dims=-1, -2).contiguous()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): LastStep()\n",
      "    (1): LinBnDrop(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=200, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(81802, True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = RNNPlus(c_in, c_out, seq_len, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5, flatten=True)\n",
    "print(m)\n",
    "print(total_params(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMPlus(\n",
      "  (backbone): _RNN_Backbone(\n",
      "    (rnn): LSTM(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (transpose): Transpose(dims=-1, -2).contiguous()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): LastStep()\n",
      "    (1): LinBnDrop(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=200, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(326002, True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = LSTMPlus(c_in, c_out, seq_len, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5, flatten=True)\n",
    "print(m)\n",
    "print(total_params(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRUPlus(\n",
      "  (backbone): _RNN_Backbone(\n",
      "    (rnn): GRU(3, 100, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "    (transpose): Transpose(dims=-1, -2).contiguous()\n",
      "  )\n",
      "  (head): Sequential(\n",
      "    (0): LastStep()\n",
      "    (1): LinBnDrop(\n",
      "      (0): Dropout(p=0.5, inplace=False)\n",
      "      (1): Linear(in_features=200, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "(244602, True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = GRUPlus(c_in, c_out, seq_len, hidden_size=100,n_layers=2,bidirectional=True,rnn_dropout=.5,fc_dropout=.5, flatten=True)\n",
    "print(m)\n",
    "print(total_params(m))\n",
    "m(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "out = create_scripts()\n",
    "beep(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
