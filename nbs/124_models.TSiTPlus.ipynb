{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp models.TSiTPlus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSiT & InceptionTSiT\n",
    "\n",
    "> These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)\n",
    "     \n",
    "Reference: \n",
    "\n",
    "     Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020).\n",
    "     An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
    "\n",
    "     This implementation is a modified version of Vision Transformer that is part of the grat timm library\n",
    "     (https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from tsai.imports import *\n",
    "from tsai.utils import *\n",
    "from tsai.models.layers import *\n",
    "from tsai.models.InceptionTimePlus import InceptionBlockPlus\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "class _TSiTEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_layers:int=6, attn_dropout:float=0, dropout:float=0, drop_path_rate:float=0., \n",
    "                 mlp_ratio:int=1, qkv_bias:bool=True, act:str='reglu', pre_norm:bool=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, n_layers)]\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for i in range(n_layers):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                MultiheadAttention(d_model, n_heads, dropout=attn_dropout, qkv_bias=qkv_bias), nn.LayerNorm(d_model),\n",
    "                PositionwiseFeedForward(d_model, dropout=dropout, act=act, mlp_ratio=mlp_ratio), nn.LayerNorm(d_model),\n",
    "                # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "                DropPath(dpr[i]) if dpr[i] != 0 else nn.Identity(),\n",
    "                # nn.Dropout(drop_path_rate) if drop_path_rate != 0 else nn.Identity()\n",
    "            ]))\n",
    "        self.pre_norm = pre_norm\n",
    "        self.norm = nn.LayerNorm(d_model) if self.pre_norm else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, (mha, attn_norm, pwff, ff_norm, drop_path) in enumerate(self.layers):\n",
    "            if self.pre_norm:\n",
    "                x = drop_path(mha(attn_norm(x))[0]) + x\n",
    "                x = drop_path(pwff(ff_norm(x))) + x\n",
    "            else:\n",
    "                x = attn_norm(drop_path(mha(x)[0]) + x)\n",
    "                x = ff_norm(drop_path(pwff(x)) + x)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _TSiTBackbone(Module):\n",
    "    def __init__(self, c_in:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',\n",
    "                 d_ff:int=256, qkv_bias:bool=True, dropout:float=0., attn_dropout:float=0,drop_path_rate:float=0., \n",
    "                 mlp_ratio:int=1, pre_norm:bool=False, use_token:bool=True, ks:Optional[int]=None, maxpool:bool=True, \n",
    "                 preprocessor:Optional[Callable]=None, device=None, verbose:bool=False):\n",
    "\n",
    "        device = ifnone(device, default_device())\n",
    "        self.preprocessor = nn.Identity()\n",
    "        if preprocessor is not None:\n",
    "            xb = torch.randn(1, c_in, seq_len).to(device)\n",
    "            ori_c_in, ori_seq_len = c_in, seq_len\n",
    "            if not isinstance(preprocessor, nn.Module): preprocessor = preprocessor(c_in, d_model).to(device)\n",
    "            else: preprocessor = preprocessor.to(device)\n",
    "            with torch.no_grad():\n",
    "                # NOTE Most reliable way of determining output dims is to run forward pass\n",
    "                training = preprocessor.training\n",
    "                if training:\n",
    "                    preprocessor.eval()\n",
    "                c_in, seq_len = preprocessor(xb).shape[1:]\n",
    "                preprocessor.train(training)\n",
    "            pv(f'preprocessor: (?, {ori_c_in}, {ori_seq_len}) --> (?, {c_in}, {seq_len})', verbose=verbose)\n",
    "            self.preprocessor = preprocessor\n",
    "        \n",
    "        if ks is not None: \n",
    "            self.to_embedding = nn.Sequential(MultiConcatConv1d(c_in, d_model, kss=ks, maxpool=maxpool),Transpose(1,2))\n",
    "        else: \n",
    "            self.to_embedding = nn.Sequential(Conv1d(c_in, d_model, 1),Transpose(1,2))\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len + use_token, d_model))\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model))\n",
    "        self.use_token = use_token\n",
    "        self.pos_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = _TSiTEncoder(d_model, n_heads, n_layers=n_layers, qkv_bias=qkv_bias, attn_dropout=attn_dropout, dropout=dropout,\n",
    "                                    mlp_ratio=mlp_ratio, drop_path_rate=drop_path_rate, act=act, pre_norm=pre_norm)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # apply preprocessor module if exists\n",
    "        x = self.preprocessor(x)\n",
    "        \n",
    "        # embedding\n",
    "        x = self.to_embedding(x)\n",
    "        if self.use_token:\n",
    "            x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "        x = self.pos_dropout(x + self.pos_embedding)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        x = x.transpose(1,2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exports\n",
    "class TSiTPlus(nn.Sequential):\n",
    "    \"\"\"Time series transformer model based on ViT (Vision Transformer):\n",
    "\n",
    "    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020).\n",
    "    An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\n",
    "\n",
    "    This implementation is a modified version of Vision Transformer that is part of the grat timm library\n",
    "    (https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, c_in:int, c_out:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',\n",
    "                 d_ff:int=256, dropout:float=0., attn_dropout:float=0, drop_path_rate:float=0., mlp_ratio:int=1,\n",
    "                 qkv_bias:bool=True, pre_norm:bool=False, use_token:bool=True, fc_dropout:float=0., bn:bool=False, y_range:Optional[tuple]=None, \n",
    "                 ks:Optional[int]=None, maxpool:bool=True, preprocessor:Optional[Callable]=None, custom_head:Optional[Callable]=None, verbose:bool=False):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        =====\n",
    "\n",
    "        c_in:                   the number of features (aka variables, dimensions, channels) in the time series dataset.\n",
    "        c_out:                  the number of target classes.\n",
    "        seq_len:                number of time steps in the time series.\n",
    "        n_layers:               number of layers (or blocks) in the encoder. Default: 3 (range(1-4))\n",
    "        d_model:                total dimension of the model (number of features created by the model). Default: 128 (range(64-512))\n",
    "        n_heads:                parallel attention heads. Default:16 (range(8-16)).\n",
    "        d_head:                 size of the learned linear projection of queries, keys and values in the MHA. Usual values: 16-512. \n",
    "                                Default: None -> (d_model/n_heads) = 32.\n",
    "        act:                    the activation function of intermediate layer, relu, gelu, geglu, reglu.\n",
    "        d_ff:                   the dimension of the feedforward network model. Default: 512 (range(256-512))\n",
    "        dropout:                dropout applied to to the embedded sequence steps after position embeddings have been added and \n",
    "                                to the mlp sublayer in the encoder.\n",
    "        attn_dropout:         dropout rate applied to the attention sublayer.\n",
    "        drop_path_rate:         stochastic depth rate.\n",
    "        mlp_ratio:              ratio of mlp hidden dim to embedding dim.\n",
    "        qkv_bias:               determines whether bias is applied to the Linear projections of queries, keys and values in the MultiheadAttention\n",
    "        pre_norm:               if True normalization will be applied as the first step in the sublayers. Defaults to False.\n",
    "        use_token:              if True, the output will come from the transformed token. Otherwise a pooling layer will be applied.\n",
    "        fc_dropout:             dropout applied to the final fully connected layer.\n",
    "        bn:                     flag that indicates if batchnorm will be applied to the head.\n",
    "        y_range:                range of possible y values (used in regression tasks).\n",
    "        ks:                     (Optional) kernel sizes that will be applied to a hybrid embedding.\n",
    "        maxpool:                If true and kernel sizes are passed, maxpool will also be added to the hybrid embedding.\n",
    "        preprocessor:           an optional callable (nn.Conv1d with dilation > 1 or stride > 1 for example) that will be used to preprocess the time series before \n",
    "                                the embedding step. It is useful to extract features or resample the time series.\n",
    "        custom_head:            custom head that will be applied to the network. It must contain all kwargs (pass a partial function)\n",
    "\n",
    "        Input shape:\n",
    "            x: bs (batch size) x nvars (aka features, variables, dimensions, channels) x seq_len (aka time steps)\n",
    "        \"\"\"\n",
    "        \n",
    "        backbone = _TSiTBackbone(c_in, seq_len, n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_head=d_head, act=act,\n",
    "                                  d_ff=d_ff, dropout=dropout, attn_dropout=attn_dropout, \n",
    "                                  drop_path_rate=drop_path_rate, pre_norm=pre_norm, mlp_ratio=mlp_ratio, use_token=use_token, \n",
    "                                  ks=ks, maxpool=maxpool, preprocessor=preprocessor, verbose=verbose)\n",
    "\n",
    "        self.head_nf = d_model\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        if custom_head: \n",
    "            head = custom_head(self.head_nf, c_out, self.seq_len) # custom head passed as a partial func with all its kwargs\n",
    "        else:\n",
    "            layers = [TokenLayer(token=use_token)]\n",
    "            layers += [LinBnDrop(d_model, c_out, bn=bn, p=fc_dropout)]\n",
    "            if y_range: layers += [SigmoidRange(*y_range)]\n",
    "            head = nn.Sequential(*layers)\n",
    "        super().__init__(OrderedDict([('backbone', backbone), ('head', head)]))\n",
    "\n",
    "        \n",
    "TSiT = TSiTPlus\n",
    "InceptionTSiTPlus = named_partial(\"InceptionTSiTPlus\", TSiTPlus, preprocessor=partial(InceptionBlockPlus, ks=[3,5,7]))\n",
    "InceptionTSiT = named_partial(\"InceptionTSiT\", TSiTPlus, preprocessor=partial(InceptionBlockPlus, ks=[3,5,7]))\n",
    "ConvTSiT = named_partial(\"ConvTSiT\", TSiTPlus, ks=[1,3,5,7])\n",
    "ConvTSiTPlus = named_partial(\"ConvTSiTPlus\", TSiTPlus, ks=[1,3,5,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TSiTPlus(\n",
       "  (backbone): _TSiTBackbone(\n",
       "    (preprocessor): Identity()\n",
       "    (to_embedding): Sequential(\n",
       "      (0): Conv1d(4, 128, kernel_size=(1,), stride=(1,))\n",
       "      (1): Transpose(1, 2)\n",
       "    )\n",
       "    (pos_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (encoder): _TSiTEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (4): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "        (5): ModuleList(\n",
       "          (0): MultiheadAttention(\n",
       "            (W_Q): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_K): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (W_V): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (sdp_attn): ScaledDotProductAttention()\n",
       "            (to_out): Sequential(\n",
       "              (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (1): Dropout(p=0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (2): PositionwiseFeedForward(\n",
       "            (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "            (1): ReGLU()\n",
       "            (2): Dropout(p=0.0, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "            (4): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (4): Identity()\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "    )\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): TokenLayer()\n",
       "    (1): LinBnDrop(\n",
       "      (0): Linear(in_features=128, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 50\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "model = TSiTPlus(nvars, c_out, seq_len)\n",
    "test_eq(model(xb).shape, (bs, c_out))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 50\n",
    "c_out = 2\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "model = InceptionTSiTPlus(nvars, c_out, seq_len)\n",
    "test_eq(model(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsampling\n",
    "\n",
    "It's a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x36 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TSTensor(samples:8, vars:3, len:5000, device=cpu)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tsai.data.validation import get_splits\n",
    "from tsai.data.core import get_ts_dls\n",
    "X = np.zeros((10, 3, 5000)) \n",
    "y = np.random.randint(0,2,X.shape[0])\n",
    "splits = get_splits(y)\n",
    "dls = get_ts_dls(X, y, splits=splits)\n",
    "xb, yb = dls.train.one_batch()\n",
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to use TSiTPlus, it's likely you'll get an 'out-of-memory' error.\n",
    "\n",
    "To avoid this you can subsample the sequence reducing the input's length. This can be done in multiple ways. Here are a few examples: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separable convolution (to avoid mixing channels)\n",
    "preprocessor = Conv1d(xb.shape[1], xb.shape[1], ks=100, stride=50, padding='same', groups=xb.shape[1]).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convolution (if you want to mix channels or change number of channels)\n",
    "preprocessor = Conv1d(xb.shape[1], 2, ks=100, stride=50, padding='same').to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MaxPool\n",
    "preprocessor = nn.Sequential(Pad1d((0, 50), 0), nn.MaxPool1d(kernel_size=100, stride=50)).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 100])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# AvgPool\n",
    "preprocessor = nn.Sequential(Pad1d((0, 50), 0), nn.AvgPool1d(kernel_size=100, stride=50)).to(default_device())\n",
    "preprocessor(xb).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you decide what type of transform you want to apply, you just need to pass the layer as the preprocessor attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 16\n",
    "nvars = 4\n",
    "seq_len = 1000\n",
    "c_out = 2\n",
    "d_model = 128\n",
    "\n",
    "xb = torch.rand(bs, nvars, seq_len)\n",
    "preprocessor = partial(Conv1d, ks=5, stride=3, padding='same', groups=xb.shape[1])\n",
    "model = TSiTPlus(nvars, c_out, seq_len, d_model=d_model, preprocessor=preprocessor)\n",
    "test_eq(model(xb).shape, (bs, c_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src onerror=\"\n",
       "        this.nextElementSibling.focus();\n",
       "        this.dispatchEvent(new KeyboardEvent('keydown', {key:'s', keyCode: 83, metaKey: true}));\n",
       "        \" style=\"display:none\"><input style=\"width:0;height:0;border:0\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from tsai.imports import create_scripts\n",
    "from tsai.export import get_nb_name\n",
    "nb_name = get_nb_name()\n",
    "create_scripts(nb_name);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
