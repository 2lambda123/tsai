{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created by Ignacio Oguiza - email: oguiza@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:10.453750Z",
     "start_time": "2020-03-07T08:25:10.252668Z"
    },
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "utils.load_extension('collapsible_headings/main')\n",
       "utils.load_extension('hide_input/main')\n",
       "utils.load_extension('autosavetime/main')\n",
       "utils.load_extension('execute_time/ExecuteTime')\n",
       "utils.load_extension('code_prettify/code_prettify')\n",
       "utils.load_extension('scroll_down/main')\n",
       "utils.load_extension('jupyter-js-widgets/extension')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "utils.load_extension('collapsible_headings/main')\n",
    "utils.load_extension('hide_input/main')\n",
    "utils.load_extension('autosavetime/main')\n",
    "utils.load_extension('execute_time/ExecuteTime')\n",
    "utils.load_extension('code_prettify/code_prettify')\n",
    "utils.load_extension('scroll_down/main')\n",
    "utils.load_extension('jupyter-js-widgets/extension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:10.667566Z",
     "start_time": "2020-03-07T08:25:10.462045Z"
    },
    "hide_input": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Purpose üòá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The purpose of this notebook is to show you how you can create a simple, state-of-the-art time series classification model using the great **fastai2** library in 4 steps:\n",
    "1. Import libraries\n",
    "2. Prepare data\n",
    "3. Build learner\n",
    "4. Train model\n",
    "\n",
    "In general, there are 3 main ways to classify time series, based on the input to the neural network:\n",
    "\n",
    "- raw data\n",
    "\n",
    "- image data (encoded from raw data)\n",
    "\n",
    "- feature data (extracted from raw data)\n",
    "\n",
    "In this notebook, we will use the first approach. We will cover other approaches in future notebooks.\n",
    "\n",
    "Throughout the notebook you will see this ‚ú≥Ô∏è. It means there's some value you need to select."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:17.879800Z",
     "start_time": "2020-03-07T08:25:17.726786Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:18.108081Z",
     "start_time": "2020-03-07T08:25:17.952763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai2 : 0.0.12\n",
      "torch   : 1.4.0\n"
     ]
    }
   ],
   "source": [
    "from timeseries.imports import *\n",
    "from timeseries.data import *\n",
    "from timeseries.models import *\n",
    "print('fastai2 :', fastai2.__version__)\n",
    "print('torch   :', torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data üî¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data ‚¨áÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll use one of the most widely used time series classification databases: UEA & UCR Time Series Classification Repository. As of Sep 2019 it contains 129 univariate datasets and 30 multivariate datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:21.744231Z",
     "start_time": "2020-03-07T08:25:21.596962Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(get_UCR_univariate_list()))\n",
    "#pprint.pprint(get_UCR_univariate_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:23.080153Z",
     "start_time": "2020-03-07T08:25:22.933370Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(get_UCR_multivariate_list()))\n",
    "# pprint.pprint(get_UCR_multivariate_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of UCR data it's very easy to get data loaded. Let's select a dataset. You can modify this and select any one from the previous lists (univariate of multivariate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:25:26.656314Z",
     "start_time": "2020-03-07T08:25:26.509005Z"
    }
   },
   "outputs": [],
   "source": [
    "# dataset id\n",
    "dsid = 'ArticularyWordRecognition'   # ‚ú≥Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:26:19.530479Z",
     "start_time": "2020-03-07T08:26:19.358648Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ArticularyWordRecognition\n",
      "X_train: (275, 9, 144)\n",
      "y_train: (275,)\n",
      "X_valid: (300, 9, 144)\n",
      "y_valid: (300,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid, path='..', verbose=True) # indicate path to data/UCR dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:26:49.824442Z",
     "start_time": "2020-03-07T08:26:42.679136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 ACSF1                            100   100    1  1460 10\n",
      "  1 Adiac                            390   391    1   176 37\n",
      "  2 AllGestureWiimoteX               300   700    1   385 10\n",
      "  3 AllGestureWiimoteY               300   700    1   369 10\n",
      "  4 AllGestureWiimoteZ               300   700    1   326 10\n",
      "  5 ArrowHead                         36   175    1   251  3\n",
      "  6 Beef                              30    30    1   470  5\n",
      "  7 BeetleFly                         20    20    1   512  2\n",
      "  8 BirdChicken                       20    20    1   512  2\n",
      "  9 BME                               30   150    1   128  3\n",
      " 10 Car                               60    60    1   577  4\n",
      " 11 CBF                               30   900    1   128  3\n",
      " 12 Chinatown                         20   343    1    24  2\n",
      " 13 ChlorineConcentration            467  3840    1   166  3\n",
      " 14 CinCECGTorso                      40  1380    1  1639  4\n",
      " 15 Coffee                            28    28    1   286  2\n",
      " 16 Computers                        250   250    1   720  2\n",
      " 17 CricketX                         390   390    1   300 12\n",
      " 18 CricketY                         390   390    1   300 12\n",
      " 19 CricketZ                         390   390    1   300 12\n",
      " 20 Crop                            7200 16800    1    46 24\n",
      " 21 DiatomSizeReduction               16   306    1   345  4\n",
      " 22 DistalPhalanxOutlineAgeGroup     400   139    1    80  3\n",
      " 23 DistalPhalanxOutlineCorrect      600   276    1    80  2\n",
      " 24 DistalPhalanxTW                  400   139    1    80  6\n",
      " 25 DodgerLoopDay                     78    80    1   288  7\n",
      " 26 DodgerLoopGame                    20   138    1   288  2\n",
      " 27 DodgerLoopWeekend                 20   138    1   288  2\n",
      " 28 Earthquakes                      322   139    1   512  2\n",
      " 29 ECG200                           100   100    1    96  2\n",
      " 30 ECG5000                          500  4500    1   140  5\n",
      " 31 ECGFiveDays                       23   861    1   136  2\n",
      " 32 ElectricDevices                 8926  7711    1    96  7\n",
      " 33 EOGHorizontalSignal              362   362    1  1250 12\n",
      " 34 EOGVerticalSignal                362   362    1  1250 12\n",
      " 35 EthanolLevel                     504   500    1  1751  4\n",
      " 36 FaceAll                          560  1690    1   131 14\n",
      " 37 FaceFour                          24    88    1   350  4\n",
      " 38 FacesUCR                         200  2050    1   131 14\n",
      " 39 FiftyWords                       450   455    1   270 50\n",
      " 40 Fish                             175   175    1   463  7\n",
      " 41 FordA                           3601  1320    1   500  2\n",
      " 42 FordB                           3636   810    1   500  2\n",
      " 43 FreezerRegularTrain              150  2850    1   301  2\n",
      " 44 FreezerSmallTrain                 28  2850    1   301  2\n",
      " 45 Fungi                             18   186    1   201 18\n",
      " 46 GestureMidAirD1                  208   130    1   360 26\n",
      " 47 GestureMidAirD2                  208   130    1   360 26\n",
      " 48 GestureMidAirD3                  208   130    1   360 26\n",
      " 49 GesturePebbleZ1                  132   172    1   455  6\n",
      " 50 GesturePebbleZ2                  146   158    1   455  6\n",
      " 51 GunPoint                          50   150    1   150  2\n",
      " 52 GunPointAgeSpan                  135   316    1   150  2\n",
      " 53 GunPointMaleVersusFemale         135   316    1   150  2\n",
      " 54 GunPointOldVersusYoung           136   315    1   150  2\n",
      " 55 Ham                              109   105    1   431  2\n",
      " 56 HandOutlines                    1000   370    1  2709  2\n",
      " 57 Haptics                          155   308    1  1092  5\n",
      " 58 Herring                           64    64    1   512  2\n",
      " 59 HouseTwenty                       40   119    1  2000  2\n",
      " 60 InlineSkate                      100   550    1  1882  7\n",
      " 61 InsectEPGRegularTrain             62   249    1   601  3\n",
      " 62 InsectEPGSmallTrain               17   249    1   601  3\n",
      " 63 InsectWingbeatSound              220  1980    1   256 11\n",
      " 64 ItalyPowerDemand                  67  1029    1    24  2\n",
      " 65 LargeKitchenAppliances           375   375    1   720  3\n",
      " 66 Lightning2                        60    61    1   637  2\n",
      " 67 Lightning7                        70    73    1   319  7\n",
      " 68 Mallat                            55  2345    1  1024  8\n",
      " 69 Meat                              60    60    1   448  3\n",
      " 70 MedicalImages                    381   760    1    99 10\n",
      " 71 MelbournePedestrian             1194  2439    1    24 10\n",
      " 72 MiddlePhalanxOutlineAgeGroup     400   154    1    80  3\n",
      " 73 MiddlePhalanxOutlineCorrect      600   291    1    80  2\n",
      " 74 MiddlePhalanxTW                  399   154    1    80  6\n",
      " 75 MixedShapesRegularTrain          500  2425    1  1024  5\n",
      " 76 MixedShapesSmallTrain            100  2425    1  1024  5\n",
      " 77 MoteStrain                        20  1252    1    84  2\n",
      " 78 NonInvasiveFetalECGThorax1      1800  1965    1   750 42\n",
      " 79 NonInvasiveFetalECGThorax2      1800  1965    1   750 42\n",
      " 80 OliveOil                          30    30    1   570  4\n",
      " 81 OSULeaf                          200   242    1   427  6\n",
      " 82 PhalangesOutlinesCorrect        1800   858    1    80  2\n",
      " 83 Phoneme                          214  1896    1  1024 39\n",
      " 84 PickupGestureWiimoteZ             50    50    1   361 10\n",
      " 85 PigAirwayPressure                104   208    1  2000 52\n",
      " 86 PigArtPressure                   104   208    1  2000 52\n",
      " 87 PigCVP                           104   208    1  2000 52\n",
      " 88 PLAID                            537   537    1  1344 11\n",
      " 89 Plane                            105   105    1   144  7\n",
      " 90 PowerCons                        180   180    1   144  2\n",
      " 91 ProximalPhalanxOutlineAgeGroup   400   205    1    80  3\n",
      " 92 ProximalPhalanxOutlineCorrect    600   291    1    80  2\n",
      " 93 ProximalPhalanxTW                400   205    1    80  6\n",
      " 94 RefrigerationDevices             375   375    1   720  3\n",
      " 95 Rock                              20    50    1  2844  4\n",
      " 96 ScreenType                       375   375    1   720  3\n",
      " 97 SemgHandGenderCh2                300   600    1  1500  2\n",
      " 98 SemgHandMovementCh2              450   450    1  1500  6\n",
      " 99 SemgHandSubjectCh2               450   450    1  1500  5\n",
      "100 ShakeGestureWiimoteZ              50    50    1   385 10\n",
      "101 ShapeletSim                       20   180    1   500  2\n",
      "102 ShapesAll                        600   600    1   512 60\n",
      "103 SmallKitchenAppliances           375   375    1   720  3\n",
      "104 SmoothSubspace                   150   150    1    15  3\n",
      "105 SonyAIBORobotSurface1             20   601    1    70  2\n",
      "106 SonyAIBORobotSurface2             27   953    1    65  2\n",
      "107 StarLightCurves                 1000  8236    1  1024  3\n",
      "108 Strawberry                       613   370    1   235  2\n",
      "109 SwedishLeaf                      500   625    1   128 15\n",
      "110 Symbols                           25   995    1   398  6\n",
      "111 SyntheticControl                 300   300    1    60  6\n",
      "112 ToeSegmentation1                  40   228    1   277  2\n",
      "113 ToeSegmentation2                  36   130    1   343  2\n",
      "114 Trace                            100   100    1   275  4\n",
      "115 TwoLeadECG                        23  1139    1    82  2\n",
      "116 TwoPatterns                     1000  4000    1   128  4\n",
      "117 UMD                               36   144    1   150  3\n",
      "118 UWaveGestureLibraryAll           896  3582    1   945  8\n",
      "119 UWaveGestureLibraryX             896  3582    1   315  8\n",
      "120 UWaveGestureLibraryY             896  3582    1   315  8\n",
      "121 UWaveGestureLibraryZ             896  3582    1   315  8\n",
      "122 Wafer                           1000  6164    1   152  2\n",
      "123 Wine                              57    54    1   234  2\n",
      "124 WordSynonyms                     267   638    1   270 25\n",
      "125 Worms                            181    77    1   900  5\n",
      "126 WormsTwoClass                    181    77    1   900  2\n",
      "127 Yoga                             300  3000    1   426  2\n"
     ]
    }
   ],
   "source": [
    "for i,dsid in enumerate(get_UCR_univariate_list()):\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test = get_UCR_data(dsid, path='..', verbose=False)\n",
    "        print(f'{i:3} {dsid:30} {len(y_train):5} {len(y_test):5} {X_train.shape[1]:4} {X_train.shape[-1]:5} {len(np.unique(y_train)):2}')\n",
    "        del X_train, y_train, X_test, y_test\n",
    "    except:\n",
    "        print(f'{i:3} {dsid:30}***')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-07T08:26:59.944131Z",
     "start_time": "2020-03-07T08:26:58.134789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0 ArticularyWordRecognition        275   300    9   144 25\n",
      "  1 AtrialFibrillation                15    15    2   640  3\n",
      "  2 BasicMotions                      40    40    6   100  4\n",
      "  3 CharacterTrajectories           1422  1436    3   180 20\n",
      "  4 Cricket                          108    72    6  1197 12\n",
      "  5 DuckDuckGeese                 ***\n",
      "  6 EigenWorms                       128   131    6 17984  5\n",
      "  7 Epilepsy                         137   138    3   206  4\n",
      "  8 ERing                             30   270    4    65  6\n",
      "  9 EthanolConcentration             261   263    3  1751  4\n",
      " 10 FaceDetection                   5890  3524  144    62  2\n",
      " 11 FingerMovements                  316   100   28    50  2\n",
      " 12 HandMovementDirection            160    74   10   400  4\n",
      " 13 Handwriting                      150   850    3   152 26\n",
      " 14 Heartbeat                        204   205   61   405  2\n",
      " 15 InsectWingbeat                ***\n",
      " 16 JapaneseVowels                   270   370   12    26  9\n",
      " 17 Libras                           180   180    2    45 15\n",
      " 18 LSST                            2459  2466    6    36 14\n",
      " 19 MotorImagery                     278   100   64  3000  2\n",
      " 20 NATOPS                           180   180   24    51  6\n",
      " 21 PEMS-SF                          267   173  963   144  7\n",
      " 22 PenDigits                       7494  3498    2     8 10\n",
      " 23 PhonemeSpectra                  3315  3353   11   217 39\n",
      " 24 RacketSports                     151   152    6    30  4\n",
      " 25 SelfRegulationSCP1               268   293    6   896  2\n",
      " 26 SelfRegulationSCP2               200   180    7  1152  2\n",
      " 27 SpokenArabicDigits              6599  2199   13    93 10\n",
      " 28 StandWalkJump                     12    15    4  2500  3\n",
      " 29 UWaveGestureLibrary              120   320    3   315  8\n"
     ]
    }
   ],
   "source": [
    "for i,dsid in enumerate(get_UCR_multivariate_list()):\n",
    "    try:\n",
    "        X_train, y_train, X_test, y_test = get_UCR_data(dsid, path='..', verbose=False)\n",
    "        print(f'{i:3} {dsid:30} {len(y_train):5} {len(y_test):5} {X_train.shape[1]:4} {X_train.shape[-1]:5} {len(np.unique(y_train)):2}')\n",
    "        del X_train, y_train, X_test, y_test\n",
    "    except:\n",
    "        print(f'{i:3} {dsid:30}***')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ò£Ô∏è **Something very important when you prepare your own data is that data needs to be in a 3-d array with the following format:**\n",
    "\n",
    "1. Number of samples\n",
    "2. Dimensions\n",
    "3. Time series length (aka time steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All UEA & UCR Time Series Classification data have already been split between train and valid. When you use your own data, you'll have to split it yourself. We'll see examples of this in future notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare databunch üíø"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You always need to define the bs at the time of creating the databunch, the object that contains all data required.\n",
    "\n",
    "It's also best practice to scale the data using the train stats. There are several options available: \n",
    "\n",
    "1. standardization or normalization.\n",
    "\n",
    "2. calculate them based on all samples, per channel or per sample. \n",
    "\n",
    "3. scale range (for normalization only).\n",
    "\n",
    "The most common practice is to standardize data per channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64                            # ‚ú≥Ô∏è\n",
    "seed = 1234                        # ‚ú≥Ô∏è\n",
    "scale_type = 'standardize'         # ‚ú≥Ô∏è 'standardize', 'normalize'\n",
    "scale_by_channel = True            # ‚ú≥Ô∏è \n",
    "scale_by_sample  = False           # ‚ú≥Ô∏è \n",
    "scale_range = (-1, 1)              # ‚ú≥Ô∏è for normalization only: usually left to (-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last step in data preparation is to prepare a databunch.\n",
    "Time series data may come as numpy arrays, pandas dataframes, etc.\n",
    "The 2 most common ways to load data into a databunch will be from a numpy array/ torch tensors or a pandas dataframe. Let's see how we'd work in either case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From 3D numpy arrays/ torch tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You need to first create ItemLists from TimeSeriesList (custom type of ItemList built for Time Series)\n",
    "\n",
    "2) You need to label the ItemLists. You'll find a lot of information [here](https://docs.fast.ai/data_block.html)\n",
    "\n",
    "3) You enter the train bs and val_bs and crate the databunch object. \n",
    "\n",
    "4) You add features and seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "db = (ItemLists('.', TimeSeriesList(X_train), TimeSeriesList(X_valid))\n",
    "      .label_from_lists(y_train, y_valid)\n",
    "      .databunch(bs=min(bs, len(X_train)), val_bs=min(len(X_valid), bs * 2), num_workers=cpus, device=device)\n",
    "      .scale(scale_type=scale_type, scale_by_channel=scale_by_channel, \n",
    "             scale_by_sample=scale_by_sample,scale_range=scale_range)\n",
    "     )\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From pandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extract data from a pandas dataframe. Since we don't have the UCR data available as a dataframe, we'll first need to create and save it. You won't need to do this when you have a time series dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "dsid = 'NATOPS' \n",
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)\n",
    "for ch in range(X_train.shape[-2]):\n",
    "    data_ch = np.concatenate((np.full((len(np.concatenate((X_train, X_valid))), 1), ch),\n",
    "                              np.concatenate((X_train, X_valid))[:, ch], \n",
    "                              np.concatenate((y_train, y_valid))[:, None]), axis=-1)\n",
    "    if ch == 0: data = data_ch\n",
    "    else: data = np.concatenate((data, data_ch))\n",
    "df = pd.DataFrame(data, columns=['feat'] + list(np.arange(X_train.shape[-1]).astype('str')) + ['target'])\n",
    "df.to_csv(path/f'data/UCR/{dsid}/{dsid}.csv', index=False)\n",
    "pd.read_csv(path/f'data/UCR/{dsid}/{dsid}.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You would actually start here, loading an existing dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsid = 'NATOPS'   # ‚ú≥Ô∏è\n",
    "df = pd.read_csv(path/f'data/UCR/{dsid}/{dsid}.csv')\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîé To create the TimeSeriesList, you need to select the columns that contain the time series only, neither the target, not the feature (for multivariate TS).\n",
    "\n",
    "üîé You should use **label_cls=CategoryList** when labels are floats but it is a classification problem. Otherwise, the fastai library would take it as a regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) You need to first TimeSeriesList (custom type of ItemList built for Time Series) from the dataframe. As cols you should only enter the data from the time series (X values, not y).\n",
    "\n",
    "2) Then you split the TimeSeriesList into 2 lists (traina and valid). There are multiple ways to do that. More info [here](https://docs.fast.ai/data_block.html)\n",
    "\n",
    "3) You need to label the ItemLists. You'll find a lot of information [here](https://docs.fast.ai/data_block.html)\n",
    "\n",
    "4) You enter the train bs and val_bs and crate the databunch object. \n",
    "\n",
    "5) You add features and seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = (TimeSeriesList.from_df(df, '.', cols=df.columns.values[1:-1], feat='feat')\n",
    "      .split_by_rand_pct(valid_pct=0.2, seed=seed)\n",
    "      .label_from_df(cols='target', label_cls=CategoryList)\n",
    "      .databunch(bs=bs,  val_bs=bs * 2,  num_workers=cpus,  device=device)\n",
    "      .scale(scale_type=scale_type, scale_by_channel=scale_by_channel, \n",
    "             scale_by_sample=scale_by_sample,scale_range=scale_range)\n",
    "     )\n",
    "db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "db.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Build learner üèó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from torchtimeseries.models import *\n",
    "# Select one arch from these state-of-the-art time series/ 1D models:\n",
    "# ResCNN, FCN, InceptionTime, ResNet\n",
    "arch = InceptionTime                     # ‚ú≥Ô∏è   \n",
    "arch_kwargs = dict()                     # ‚ú≥Ô∏è \n",
    "opt_func=Ranger                          # ‚ú≥Ô∏è a state-of-the-art optimizer\n",
    "loss_func = LabelSmoothingCrossEntropy() # ‚ú≥Ô∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "hide_input": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = arch(db.features, db.c, **arch_kwargs).to(device)\n",
    "learn = Learner(db, model, opt_func=opt_func, loss_func=loss_func)\n",
    "learn.save('stage_0')\n",
    "print(learn.model)\n",
    "print(learn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Train model üöµüèº‚Äç"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### LR find üîé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.load('stage_0')\n",
    "learn.lr_find()\n",
    "learn.recorder.plot(suggestion=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Train üèÉüèΩ‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "epochs = 100         # ‚ú≥Ô∏è \n",
    "max_lr = 1e-2        # ‚ú≥Ô∏è \n",
    "warmup = False       # ‚ú≥Ô∏è\n",
    "pct_start = .7       # ‚ú≥Ô∏è\n",
    "metrics = [accuracy] # ‚ú≥Ô∏è\n",
    "wd = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learn.metrics = metrics\n",
    "learn.load('stage_0')\n",
    "learn.fit_one_cycle(epochs, max_lr=max_lr, pct_start=pct_start, moms=(.95, .85) if warmup else (.95, .95),\n",
    "                    div_factor=25.0 if warmup else 1., wd=wd)\n",
    "learn.save('stage_1')\n",
    "learn.recorder.plot_lr()\n",
    "learn.recorder.plot_losses()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "archs_names, acc_, acces_, acc5_, n_params_,  = [], [], [], [], []\n",
    "archs_names.append(arch.__name__)\n",
    "early_stop = math.ceil(np.argmin(learn.recorder.losses) / len(learn.data.train_dl))\n",
    "acc_.append('{:.5}'.format(learn.recorder.metrics[-1][0].item()))\n",
    "acces_.append('{:.5}'.format(learn.recorder.metrics[early_stop - 1][0].item()))\n",
    "acc5_.append('{:.5}'.format(np.mean(np.max(learn.recorder.metrics))))\n",
    "n_params_.append(count_params(learn))\n",
    "clear_output()\n",
    "df = (pd.DataFrame(np.stack((archs_names, acc_, acces_, acc5_, n_params_)).T,\n",
    "                   columns=['arch', 'accuracy', 'accuracy train loss', 'max_accuracy','n_params'])\n",
    "      .sort_values('accuracy train loss', ascending=False).reset_index(drop=True))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
