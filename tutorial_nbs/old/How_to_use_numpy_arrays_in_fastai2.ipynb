{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work with numpy arrays in fastai2: time series classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd like to share how you can work with (very large) numpy arrays in fastai2 through a time series classification example. In this case we'll use a multivariate time series dataset.\n",
    "\n",
    "**High level requirements:**\n",
    "\n",
    "- Be able to work with *numpy arrays with any number of dimensions*. \n",
    "- Data may be *larger than RAM*, so it may be in memory or on disk.\n",
    "- Use data on disk with similar *performance* to data in memory.\n",
    "- Data is often *split*: \n",
    "    - X, y\n",
    "    - X_train, X_valid, y_train, y_valid\n",
    "- Add an *unlabeled dataset* (for example for semi-supervised/ self-supervised learning).\n",
    "- Add test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T23:37:37.177222Z",
     "start_time": "2020-03-14T23:37:34.454537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T23:37:49.643903Z",
     "start_time": "2020-03-14T23:37:37.183371Z"
    }
   },
   "outputs": [],
   "source": [
    "from fastai2.torch_core import *\n",
    "from fastai2.data.all import *\n",
    "from fastai2.learner import *\n",
    "from fastai2.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timeseries.imports import *\n",
    "from timeseries.utils import *\n",
    "from timeseries.data import *\n",
    "# from timeseries.core import *\n",
    "from timeseries.models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TSTensor(TensorBase):\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, o): \n",
    "        return cls(To2DPlusTensor(o)) # creates a TSTensor with at least 2 dimensions of type float\n",
    "\n",
    "    @property\n",
    "    def vars(self): return self.shape[-2]\n",
    "\n",
    "    @property\n",
    "    def len(self): return self.shape[-1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        return retain_type(res, self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'TSTensor(size:{list(self.size())})'\n",
    "\n",
    "    def show(self, ax=None, ctx=None, title=None, **kwargs):\n",
    "        ax = ifnone(ax,ctx)\n",
    "        if ax is None: fig, ax = plt.subplots(**kwargs)\n",
    "        ax.plot(self.T)\n",
    "        ax.axis(xmin=0, xmax=self.shape[-1] - 1)\n",
    "        ax.set_title(title, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        return ax\n",
    "\n",
    "@Transform\n",
    "def ToTSTensor(o:np.ndarray): \n",
    "    \"\"\" np.ndarray to tensor of dtype torch.float32\"\"\"\n",
    "    return TSTensor.create(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TSTensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-48fb8a9f1b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TSTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TSTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TSTensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TSTensor' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.random.rand(16, 6, 12)\n",
    "ts = TSTensor.create(a)\n",
    "test_eq(type(ts).__name__, 'TSTensor')\n",
    "test_eq(type(ts[0]).__name__, 'TSTensor')\n",
    "test_eq(type(ts[0][0]).__name__, 'TSTensor')\n",
    "test_eq(isinstance(ts, torch.Tensor), True)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 3).double()\n",
    "t = ToTSTensor(a)\n",
    "test_eq(list(t.shape), [2, 3])\n",
    "test_eq(t.dtype, torch.float64)\n",
    "test_eq(ToType(torch.float32)(t).dtype, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(2, 3).astype('float64')\n",
    "t = ToTSTensor(a)\n",
    "test_eq(list(t.shape), [2, 3])\n",
    "test_eq(t.dtype, torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(2, 3).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit TSTensor.create(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(16, 1, 12)\n",
    "ts = TSTensor.create(b)\n",
    "test_eq(ts.ndim, 3)\n",
    "test_eq(ts[0].ndim, 2)\n",
    "test_eq(ts[0][0].ndim, 1)\n",
    "ts, ts[0], ts[0][0], ts[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit TSTensor.create(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.rand(16, 12)\n",
    "c = np.random.randint(0, 3, 100)\n",
    "labels = L(['a', 'b', 'c'])[c]\n",
    "items = itemify(b, labels)\n",
    "t = ToTSTensor(items[0])\n",
    "test_eq(list(t[0].data.shape), [1, 12])\n",
    "tl = TfmdLists(items, ToTSTensor)\n",
    "test_eq(list(tl[0][0].data.shape), [1, 12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit tl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TSTfmdDL(TfmdDL): \n",
    "\n",
    "    @property\n",
    "    def vars(self): return self.dataset[0][0].shape[-2]\n",
    "    \n",
    "    @property\n",
    "    def len(self): return self.dataset[0][0].shape[-1]\n",
    "\n",
    "    @delegates(plt.subplots)\n",
    "    def show_batch(self, b=None, max_n=9, nrows=3, ncols=3, figsize=(12, 10), **kwargs):\n",
    "        if b is None: b = self.one_batch()\n",
    "        db = self.decode_batch(b, max_n=max_n)\n",
    "        if nrows is None: \n",
    "            sqrt = math.sqrt(len(db))\n",
    "            rows = min(math.ceil(sqrt), len(db)) \n",
    "        if ncols is None: ncols = len(db) // rnows\n",
    "        fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize,  **kwargs)\n",
    "        for tup, ax in zip(db[:nrows ** 2], [axs] if nrows == 1 else axs.flatten()): \n",
    "            show_tuple(tup, ax=ax)\n",
    "        plt.tight_layout()\n",
    "\n",
    "@delegates(plt.subplots)\n",
    "def show_tuple(tup, ax=None, **kwargs):\n",
    "    \"Display a timeseries plot from a tuple\"\n",
    "    tup[0].show(title='unlabeled' if len(tup) == 1 else tup[1], ax=ax, **kwargs)\n",
    "    \n",
    "    \n",
    "def cycle_dl(dl):\n",
    "    for x,y in iter(dl): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:06:22.335635Z",
     "start_time": "2020-03-15T00:06:22.062289Z"
    }
   },
   "outputs": [],
   "source": [
    "dsid = 'StarLightCurves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:06:34.880402Z",
     "start_time": "2020-03-15T00:06:26.590284Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid, path='..', verbose=False)\n",
    "X = np.concatenate((X_train, X_valid))\n",
    "y = np.concatenate((y_train, y_valid))\n",
    "folder = 'data/UCR'\n",
    "np.save(str(PATH.parent/f'{folder}/{dsid}/X.npy'), X) # cannot use pathlib.PosixPath as filename \n",
    "np.save(str(PATH.parent/f'{folder}/{dsid}/y.npy'), y)\n",
    "del X, y\n",
    "X = np.load(str(PATH.parent/f'{folder}/{dsid}/X.npy'), mmap_mode='r')\n",
    "y = np.load(str(PATH.parent/f'{folder}/{dsid}/y.npy'), mmap_mode='r')\n",
    "split_idx = (L(list(np.arange(len(X_train)))), L(list(np.arange(len(X_train), len(X)))))\n",
    "X.shape, y.shape, X.__class__.__name__, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process_TL(tfmdlists, process=True, **kwargs): \n",
    "    if process: return TfmdLists(tfmdlists[:], None, **kwargs)\n",
    "    else: return tfmdlists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "# tfms = None\n",
    "tls = None\n",
    "kwargs = {'splits':split_idx}\n",
    "pre_process = True\n",
    "it = 0\n",
    "\n",
    "items = itemify(np.array(X), np.array(y))\n",
    "n_items = len(items[0])\n",
    "assert (tfms is None or n_items == len(tfms)), f\"n_tfms ({len(tfms)}) doesn't match n_items ({n_items})\"\n",
    "process = [False] * (n_items - 1) + [not pre_process] if X.__class__.__name__ == 'memmap' else [not pre_process] * (n_items)\n",
    "tfms = [None] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(n_items)]\n",
    "tls = L(tls if tls else [pre_process_TL(TfmdLists(items, t, **kwargs), p, **kwargs) for p, t in zip(process, tfms)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "tfms = [[ToTSTensor]]\n",
    "# tfms = None\n",
    "items = itemify(X,)\n",
    "# items = itemify(X,y)\n",
    "n_items = len(items[0])\n",
    "assert (tfms is None or n_items == len(tfms)), f\"n_tfms ({len(tfms)}) doesn't match n_items ({n_items})\"\n",
    "process = [False] * (n_items - 1) + [not pre_process] if items[0][0].__class__.__name__ == 'memmap' else [not pre_process] * (n_items)\n",
    "process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _test(it):\n",
    "    if len(tls) == 1: return tls[0][it]\n",
    "    res = tuple([tl[it] for tl in tls])\n",
    "    return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "def _test_train(it):\n",
    "    if len(tls) == 1: return tls[0].train[it]\n",
    "    res = tuple([tl.train[it] for tl in tls])\n",
    "    return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "def _test_valid(it):\n",
    "    if len(tls) == 1: return tls[0].valid[it]\n",
    "    res = tuple([tl.valid[it] for tl in tls])\n",
    "    return res if is_indexer(it) else list(zip(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test(0), _test([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_train(0), _test_train([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_valid(0), _test_valid([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyDatasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, X=None, y=None, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, pre_process=True, **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        \n",
    "        \n",
    "        if tls is None:\n",
    "            if items is None: items = itemify(X,) if y is None else itemify(X,y)\n",
    "            n_items = len(items[0])\n",
    "            assert (tfms is None or n_items == len(tfms)), f\"n_tfms ({len(tfms)}) doesn't match n_items ({n_items})\"\n",
    "            process = [False] * (n_items - 1) + [pre_process] if items[0][0].__class__.__name__ == 'memmap' else [pre_process] * (n_items)\n",
    "            tfms = [None] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(n_items)]\n",
    "        self.tls = L(tls if tls else [pre_process_TL(TfmdLists(items, t, **kwargs), p, **kwargs) for p, t in zip(process, tfms)])\n",
    "#         self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        if len(self.tls) == 1: return self.tls[0][it]\n",
    "        res = tuple([tl[it] for tl in self.tls])\n",
    "        return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "# tfms = None\n",
    "dsets = NumpyDatasets(X, y, tfms=tfms, splits=split_idx, pre_process=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dsets.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = TSTfmdDL(dsets.valid, bs=128)\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "# tfms = None\n",
    "dsets = NumpyDatasets(np.array(X), y, tfms=tfms, splits=split_idx, pre_process=False)\n",
    "dls = dsets.dataloaders(bs=64, val_bs=128, num_workers=None)\n",
    "valid_dl = TSTfmdDL(dsets.valid, bs=128, num_workers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = dls\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dsets.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = dsets.dataloaders(bs=64, val_bs=128, num_workers=None)\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dls.dataloaders())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "# tfms = None\n",
    "dsets = NumpyDatasets(np.array(X), y, tfms=tfms, splits=split_idx, pre_process=True)\n",
    "dls = TSTfmdDL(dsets, bs=64, val_bs=128, num_workers=None)\n",
    "valid_dl = TSTfmdDL(dsets.valid, bs=128, num_workers=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.valid))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        res = tuple([tl[it] for tl in self.tls])\n",
    "        return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = processTL(TfmdLists(items, ItemGetter(0)), splits=split_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ORIGINAL lazy=True\n",
    "items = itemify(X,y)\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "kwargs = {}\n",
    "tls = None\n",
    "\n",
    "assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range_of(items[0])]\n",
    "tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "\n",
    "def _test(it):\n",
    "    res = tuple([tl[it] for tl in tls])\n",
    "    return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "_test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ORIGINAL - UNLABELED lazy=True\n",
    "# items = itemify(X)\n",
    "# tfms = [ToTSTensor]\n",
    "# kwargs = {}\n",
    "# tls = None\n",
    "\n",
    "# assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "# tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range_of(items[0])]\n",
    "# tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "\n",
    "# def _test(it):\n",
    "#     res = tuple([tl[it] for tl in tls])\n",
    "#     return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "# _test([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test([0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NON-LAZY lazy=False\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "kwargs = {}\n",
    "tls = None\n",
    "\n",
    "items = itemify(X,y)\n",
    "assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range_of(items[0])]\n",
    "tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))]).zip()\n",
    "tls = L(zip(*tls))\n",
    "\n",
    "def _test2(it):\n",
    "    return tls[it] if is_indexer(it) else list(tls[it])\n",
    "def _test2(it):\n",
    "    res = tuple([L(tl)[it] for tl in tls])\n",
    "    return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "_test2([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls[0][]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test2(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test2([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MIXED lazy=False in_memory=True (memmap)\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "kwargs = {}\n",
    "tls = None\n",
    "lazy = True\n",
    "\n",
    "if not tls:\n",
    "    items = itemify(X,y)\n",
    "    items_len = len(items[0])\n",
    "    xtfms = None\n",
    "    assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "    if items[0][0].__class__.__name__ == 'memmap': # if data in memory split tfms\n",
    "        lazy = False\n",
    "        if tfms is not None: xtfms, tfms[0] = tfms[0], None\n",
    "    tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(items_len)]\n",
    "    tls = L([TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "    if not lazy: tls = tls.zip()\n",
    "\n",
    "def _test3(it):\n",
    "    res = tuple(TfmdLists(tls[it], xtfms, **kwargs)[:items_len])\n",
    "    return res if is_indexer(it) else list(res)\n",
    "\n",
    "_test3([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test3(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit _test3([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "kwargs = {}\n",
    "tls = None\n",
    "lazy = False\n",
    "\n",
    "if not tls:\n",
    "#     items = itemify(X,y)\n",
    "    items = itemify(np.array(X), np.array(y))\n",
    "    items_len = len(items[0])                \n",
    "    assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "    if items[0][0].__class__.__name__ == 'memmap': lazy = False # if data on disk split tfms\n",
    "    if not lazy and tfms is not None: xtfms, tfms[0] = tfms[0], None\n",
    "    tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(items_len)]\n",
    "    tls = L([TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "#     if not lazy: # By applying this we perform all tfms- is there an alternative?\n",
    "#         tls = L(tls.zip())\n",
    "#                 tls = L(zip(*tls))\n",
    "\n",
    "# def _test4(it):\n",
    "#     if xtfms is not None: \n",
    "#         print(1)\n",
    "#         res = tuple(TfmdLists(tls[it], xtfms, **kwargs)[:items_len])\n",
    "#         return res if is_indexer(it) else list(res)\n",
    "#     elif lazy:\n",
    "#         print(2)\n",
    "#         res = tuple([tl[it] for tl in tls])\n",
    "#         return res if is_indexer(it) else list(zip(*res))\n",
    "#     else:\n",
    "#         print(3)\n",
    "#         return tls[it] if is_indexer(it) else list(tls[it])\n",
    "\n",
    "# _test4([0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "tls = L([TfmdLists(X, None), TfmdLists(y, tfms[1])]).zip()\n",
    "tls2 = L([TfmdLists(tls, tfms[0], splits=split_idx)])\n",
    "tls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = itemify(np.array(X), np.array(y))\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(items_len)]\n",
    "tls0 = TfmdLists(TfmdLists(items, tfms[0]), None, splits=split_idx)\n",
    "tls1 = TfmdLists(TfmdLists(items, tfms[1])[:len(items)], None, splits=split_idx)\n",
    "tls  = L([tls0, tls1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls2[0].subset(0)[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls2 = L(tls.zip())\n",
    "tls3 = L(zip(*tls2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TfmdLists(tls.zip(), None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tls is None:\n",
    "            if items is None:\n",
    "                if y is None: items = itemify(X)\n",
    "                else: items = itemify(X,y)\n",
    "            items_len = len(items[0])                \n",
    "            assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "            if items[0][0].__class__.__name__ == 'memmap': lazy = False # if data on disk split tfms\n",
    "            if not lazy and tfms is not None: xtfms, tfms[0] = tfms[0], None\n",
    "            tfms = [ItemGetter(i) for i in range_of(items[0])] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(items_len)]\n",
    "            tls = L([TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "            if not lazy: # By applying this we perform all tfms- is there an alternative?\n",
    "                tls = L(tls.zip())\n",
    "#                 tls = L(zip(*tls))\n",
    "        else: \n",
    "            tls = L(tls)\n",
    "            items_len = len(tls[0])\n",
    "        self.tls = tls\n",
    "        self.xtfms = xtfms\n",
    "        self.items_len = items_len\n",
    "        self.lazy=lazy\n",
    "        print(lazy, xtfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:12:22.571139Z",
     "start_time": "2020-03-15T00:12:22.507625Z"
    }
   },
   "outputs": [],
   "source": [
    "itemify(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "# items = itemify(np.array(X), np.array(y))\n",
    "dsets = TSDatasets(X, y, tfms=tfms, splits=split_idx, lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dsets.tls[0]), len(dsets.tls[0].subset(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = TSTfmdDL(dsets.train, bs=128)\n",
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TfmdDL(dsets, bs=64, val_bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.train))\n",
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = L(TfmdLists(L(dsets.tls).zip(), None))\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = L(TfmdLists(L(dsets.tls).zip(), ToTSTensor))\n",
    "t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dsets.train\n",
    "train_ds[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = dsets.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = L([TfmdLists(tl, None) for tl in dsets.tls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0].subset(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple((Pipeline(ToTSTensor)(X)[0], TfmdLists(y, Categorize())[:len(y)][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls = dsets.tls.zip()\n",
    "tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cycle_dl(dl):\n",
    "    for x,y in iter(dl): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilteredBase??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfmdLists(FilteredBase, L, GetAttr):\n",
    "    \"A `Pipeline` of `tfms` applied to a collection of `items`\"\n",
    "    _default='tfms'\n",
    "    def __init__(self, items, tfms, use_list=None, do_setup=True, split_idx=None, train_setup=True,\n",
    "                 splits=None, types=None, verbose=False):\n",
    "        super().__init__(items, use_list=use_list)\n",
    "        self.splits = L([slice(None),[]] if splits is None else splits).map(mask2idxs)\n",
    "        if isinstance(tfms,TfmdLists): tfms = tfms.tfms\n",
    "        if isinstance(tfms,Pipeline): do_setup=False\n",
    "        self.tfms = Pipeline(tfms, split_idx=split_idx)\n",
    "        self.types = types\n",
    "        if do_setup:\n",
    "            pv(f\"Setting up {self.tfms}\", verbose)\n",
    "            self.setup(train_setup=train_setup)\n",
    "\n",
    "    def _new(self, items, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, types=self.types, **kwargs)\n",
    "    def subset(self, i): return self._new(self._get(self.splits[i]), split_idx=i)\n",
    "    def _after_item(self, o): return self.tfms(o)\n",
    "    def __repr__(self): return f\"{self.__class__.__name__}: {self.items}\\ntfms - {self.tfms.fs}\"\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def show(self, o, **kwargs): return self.tfms.show(o, **kwargs)\n",
    "    def decode(self, o, **kwargs): return self.tfms.decode(o, **kwargs)\n",
    "    def __call__(self, o, **kwargs): return self.tfms.__call__(o, **kwargs)\n",
    "    def overlapping_splits(self): return L(Counter(self.splits.concat()).values()).filter(gt(1))\n",
    "\n",
    "    def setup(self, train_setup=True):\n",
    "        self.tfms.setup(self, train_setup)\n",
    "        if len(self) != 0:\n",
    "            x = super().__getitem__(0) if self.splits is None else super().__getitem__(self.splits[0])[0]\n",
    "            self.types = []\n",
    "            for f in self.tfms.fs:\n",
    "                self.types.append(getattr(f, 'input_types', type(x)))\n",
    "                x = f(x)\n",
    "            self.types.append(type(x))\n",
    "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
    "        self.pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
    "\n",
    "    def infer_idx(self, x):\n",
    "        idx = 0\n",
    "        for t in self.types:\n",
    "            if isinstance(x, t): break\n",
    "            idx += 1\n",
    "        types = L(t if is_listy(t) else [t] for t in self.types).concat().unique()\n",
    "        pretty_types = '\\n'.join([f'  - {t}' for t in types])\n",
    "        assert idx < len(self.types), f\"Expected an input of type in \\n{pretty_types}\\n but got {type(x)}\"\n",
    "        return idx\n",
    "\n",
    "    def infer(self, x):\n",
    "        return compose_tfms(x, tfms=self.tfms.fs[self.infer_idx(x):], split_idx=self.split_idx)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        if self._after_item is None: return res\n",
    "        return self._after_item(res) if is_indexer(idx) else res.map(self._after_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSDatasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, X=None, y=None, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, xtfms=None, lazy=True, **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        if tls is None:\n",
    "            if items is None: items = itemify(X) if y is None else itemify(X,y)\n",
    "            n_items = len(items[0])                \n",
    "            assert tfms is None or len(tfms) == len(items[0]), 'len tfms == len items[0]'\n",
    "#             if items[0][0].__class__.__name__ == 'memmap': \n",
    "#                 lazy = False # if data on disk split tfms\n",
    "#                 if tfms is not None: xtfms, tfms[0] = tfms[0], None\n",
    "            tfms = [ItemGetter(i) for i in range(n_items)] if tfms is None else [[ItemGetter(i)] + L(tfms[i]) for i in range(n_items)]\n",
    "            tls = L([TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "            if not lazy: # By applying this we perform all tfms - is there an alternative?\n",
    "#                 tls  = L([TfmdLists(X, None), TfmdLists(y, tfms[1])]).zip()\n",
    "#                 tls2 = L([TfmdLists(tls, tfms[0], splits=split_idx)])\n",
    "                tls = L(tls.zip())\n",
    "#                 tls = L(zip(*tls))\n",
    "#                 tls = L([TfmdLists(tls.zip(), xtfms)])\n",
    "                tls = L([TfmdLists(L([tls.zip()]), xtfms, **kwargs)])\n",
    "        else: \n",
    "            tls = L(tls)\n",
    "            n_items = len(tls)\n",
    "        self.tls = tls\n",
    "        self.xtfms = xtfms\n",
    "        self.n_items = n_items\n",
    "        self.lazy=lazy\n",
    "        \n",
    "#         self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        if self.lazy:\n",
    "#             print(1)\n",
    "            res = tuple([tl[it] for tl in self.tls])\n",
    "            return res if is_indexer(it) else list(zip(*res))\n",
    "        else:\n",
    "#             print(3)\n",
    "            return self.tls[it] if is_indexer(it) else list(self.tls[it])\n",
    "#         if self.lazy and self.xtfms is None:\n",
    "# #             print(1)\n",
    "#             res = tuple([tl[it] for tl in self.tls])\n",
    "#             return res if is_indexer(it) else list(zip(*res))\n",
    "#         elif self.xtfms is not None: \n",
    "# #             print(2)\n",
    "#             res = tuple(TfmdLists(self.tls[it], self.xtfms, **kwargs)[:self.n_itemsitems_len])\n",
    "#             return res if is_indexer(it) else list(res)\n",
    "#         else:\n",
    "# #             print(3)\n",
    "#             return self.tls[it] if is_indexer(it) else list(self.tls[it])\n",
    "    \n",
    "#         res = tuple([tl[it] for tl in self.tls])\n",
    "#         return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): \n",
    "        print('subset:', i, len(L(self.tls)))\n",
    "#         return type(self)(tls=L(tl.subset(i) for tl in self.tls),\n",
    "        return type(self)(tls=L([self.tls.subset(i)]) if not self.lazy else L(tl.subset(i) for tl in self.tls), \n",
    "                          n_inp=self.n_inp, xtfms=self.xtfms, lazy=self.lazy)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx\n",
    "\n",
    "    _docs=dict(\n",
    "        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n",
    "        show=\"Show item `o` in `ctx`\",\n",
    "        dataloaders=\"Get a `DataLoaders`\",\n",
    "        overlapping_splits=\"All splits that are in more than one split\",\n",
    "        subset=\"New `Datasets` that only includes subset `i`\",\n",
    "        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n",
    "        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Datasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        res = tuple([tl[it] for tl in self.tls])\n",
    "        return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T23:37:49.711350Z",
     "start_time": "2020-03-14T23:37:49.704981Z"
    }
   },
   "outputs": [],
   "source": [
    "# class TSTuple(tuple):\n",
    "    \n",
    "#     @delegates(plt.subplots)\n",
    "#     def show(self, ax=None, **kwargs):\n",
    "#         if ax is None: fig, ax = plt.subplots(**kwargs)\n",
    "#         ax.plot(self[0].T)\n",
    "#         ax.axis(xmin=0, xmax=self[0].shape[-1] - 1)\n",
    "#         ax.set_title('unlabeled' if len(self) == 1 else self[1], weight='bold')\n",
    "#         if ax is None: \n",
    "#             plt.tight_layout()\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-14T23:37:49.725364Z",
     "start_time": "2020-03-14T23:37:49.717505Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: datablock --> dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:06:15.470608Z",
     "start_time": "2020-03-15T00:06:15.433987Z"
    }
   },
   "outputs": [],
   "source": [
    "# @ToTensor\n",
    "# def encodes(self, o:np.ndarray): return TSTensor.create(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:26:33.545935Z",
     "start_time": "2020-03-15T00:26:33.536059Z"
    }
   },
   "outputs": [],
   "source": [
    "def TSTensorBlock(): return TransformBlock(item_tfms=ToTensor)\n",
    "\n",
    "# #export\n",
    "# class TSTransformBlock():\n",
    "#     \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "#     def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "#         self.type_tfms  =              L(type_tfms)\n",
    "#         self.item_tfms  = ToTSTensor + L(item_tfms)\n",
    "#         self.batch_tfms =              L(batch_tfms)\n",
    "#         self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:31:45.764719Z",
     "start_time": "2020-03-15T00:31:45.290897Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(TSTensorBlock(), CategoryBlock(vocab=None, add_na=False)),\n",
    "                   get_x=ItemGetter(0), get_y=ItemGetter(1), \n",
    "                   splitter=IndexSplitter(split_idx[1]))\n",
    "dls = dblock.dataloaders(source=itemify(X,y), bs=64, val_bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(dls.valid))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:27:12.261381Z",
     "start_time": "2020-03-15T00:27:06.397183Z"
    }
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:12:48.582789Z",
     "start_time": "2020-03-15T00:12:48.574575Z"
    }
   },
   "outputs": [],
   "source": [
    "# unlabeled\n",
    "# udblock = DataBlock(blocks=(TSTensorBlock(type_tfms=None, item_tfms=None, batch_tfms=None)),get_x=ItemGetter(0))\n",
    "# dls    = udblock.dataloaders(source=itemify(X,), bs=64, val_bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:25:55.782347Z",
     "start_time": "2020-03-15T00:25:55.471244Z"
    }
   },
   "outputs": [],
   "source": [
    "dblock.summary(dblock.source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _,axs = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\n",
    "# dls.show_batch(ctxs=axs.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xb = next(iter(dls.valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:13:26.855367Z",
     "start_time": "2020-03-15T00:13:21.266195Z"
    }
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(dls.valid))\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-15T00:18:38.873215Z",
     "start_time": "2020-03-15T00:18:23.251823Z"
    }
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls.valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2:  datasets --> dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Datasets(FilteredBase):\n",
    "#     \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "#     def __init__(self, items=None, tfms=None, tls=None, n_inp=None, dl_type=None, **kwargs):\n",
    "#         super().__init__(dl_type=dl_type)\n",
    "#         self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "#         self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "#     def __getitem__(self, it):\n",
    "#         res = tuple([tl[it] for tl in self.tls])\n",
    "#         return res if is_indexer(it) else list(zip(*res))\n",
    "\n",
    "#     def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "#     def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "#     def __len__(self): return len(self.tls[0])\n",
    "#     def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "#     def __repr__(self): return coll_repr(self)\n",
    "#     def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "#     def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), n_inp=self.n_inp)\n",
    "#     def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "#     def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "#     @property\n",
    "#     def splits(self): return self.tls[0].splits\n",
    "#     @property\n",
    "#     def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "#     @property\n",
    "#     def items(self): return self.tls[0].items\n",
    "#     @items.setter\n",
    "#     def items(self, v):\n",
    "#         for tl in self.tls: tl.items = v\n",
    "\n",
    "#     def show(self, o, ctx=None, **kwargs):\n",
    "#         for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "#         return ctx\n",
    "\n",
    "#     def new_empty(self):\n",
    "#         tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "#         return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "#     @contextmanager\n",
    "#     def set_split_idx(self, i):\n",
    "#         old_split_idx = self.split_idx\n",
    "#         for tl in self.tls: tl.tfms.split_idx = i\n",
    "#         yield self\n",
    "#         for tl in self.tls: tl.tfms.split_idx = old_split_idx\n",
    "\n",
    "#     _docs=dict(\n",
    "#         decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n",
    "#         show=\"Show item `o` in `ctx`\",\n",
    "#         dataloaders=\"Get a `DataLoaders`\",\n",
    "#         overlapping_splits=\"All splits that are in more than one split\",\n",
    "#         subset=\"New `Datasets` that only includes subset `i`\",\n",
    "#         new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n",
    "#         set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class TSDatasets(FilteredBase):\n",
    "    \"A dataset that creates a tuple from each `tfms`, passed thru `item_tfms`\"\n",
    "    def __init__(self, X=None, y=None, items=None, tfms=None, xtfms=None, tls=None, n_inp=None, dl_type=None,  **kwargs):\n",
    "        super().__init__(dl_type=dl_type)\n",
    "        self.xtfms = xtfms\n",
    "        if items is None: \n",
    "            if y is not None: items = itemify(X, y)\n",
    "            else: items = itemify(X,)\n",
    "#         if tfms is not None and self.xtfms is None and items[0][0].__class__.__name__ == 'memmap': \n",
    "#             self.xtfms = Pipeline([ToTSTensor] + tfms[0])\n",
    "#             tfms[0] = []\n",
    "#             if tfms is not None: tfms = [[ItemGetter(i)] + t for i,t in enumerate(tfms)]\n",
    "        self.tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "        if tls is not None: self.tls = self.tls.zip()\n",
    "#         if tls is not None and self.xtfms is not None: self.tls = self.tls.zip()\n",
    "        self.n_inp = (1 if len(self.tls)==1 else len(self.tls)-1) if n_inp is None else n_inp\n",
    "\n",
    "    def __getitem__(self, it):\n",
    "        return tuple(self.tls[it])\n",
    "        res = tuple(self.tls[it])\n",
    "        return res if is_indexer(it) else list(zip(*res))\n",
    "        \n",
    "\n",
    "    def __getattr__(self,k): return gather_attrs(self, k, 'tls')\n",
    "    def __dir__(self): return super().__dir__() + gather_attr_names(self, 'tls')\n",
    "    def __len__(self): return len(self.tls[0])\n",
    "    def __iter__(self): return (self[i] for i in range(len(self)))\n",
    "    def __repr__(self): return coll_repr(self)\n",
    "    def decode(self, o, full=True): return tuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))\n",
    "    def subset(self, i): return type(self)(tls=L(tl.subset(i) for tl in self.tls), xtfms=self.xtfms, n_inp=self.n_inp)\n",
    "    def _new(self, items, *args, **kwargs): return super()._new(items, tfms=self.tfms, do_setup=False, **kwargs)\n",
    "    def overlapping_splits(self): return self.tls[0].overlapping_splits()\n",
    "    @property\n",
    "    def splits(self): return self.tls[0].splits\n",
    "    @property\n",
    "    def split_idx(self): return self.tls[0].tfms.split_idx\n",
    "    @property\n",
    "    def items(self): return self.tls[0].items\n",
    "    @items.setter\n",
    "    def items(self, v):\n",
    "        for tl in self.tls: tl.items = v\n",
    "\n",
    "    def show(self, o, ctx=None, **kwargs):\n",
    "        for o_,tl in zip(o,self.tls): ctx = tl.show(o_, ctx=ctx, **kwargs)\n",
    "        return ctx\n",
    "\n",
    "    def new_empty(self):\n",
    "        tls = [tl._new([], split_idx=tl.split_idx) for tl in self.tls]\n",
    "        return type(self)(tls=tls, n_inp=self.n_inp)\n",
    "\n",
    "    @contextmanager\n",
    "    def set_split_idx(self, i):\n",
    "        old_split_idx = self.split_idx\n",
    "        for tl in self.tls: tl.tfms.split_idx = i\n",
    "        yield self\n",
    "        for tl in self.tls: tl.tfms.split_idx = old_split_idx\n",
    "\n",
    "    _docs=dict(\n",
    "        decode=\"Compose `decode` of all `tuple_tfms` then all `tfms` on `i`\",\n",
    "        show=\"Show item `o` in `ctx`\",\n",
    "        dataloaders=\"Get a `DataLoaders`\",\n",
    "        overlapping_splits=\"All splits that are in more than one split\",\n",
    "        subset=\"New `Datasets` that only includes subset `i`\",\n",
    "        new_empty=\"Create a new empty version of the `self`, keeping only the transforms\",\n",
    "        set_split_idx=\"Contextmanager to use the same `Datasets` with another `split_idx`\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Transform\n",
    "def ToTSTensor(x:np.ndarray): return TSTensor.create(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms   =  [[ItemGetter(0), ToTSTensor], [ItemGetter(1),Categorize()]]\n",
    "dsets = TSDatasets(X,y,tfms=tfms,splits=split_idx)\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.train[0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl = TfmdDL(dsets.valid, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl, valid_dl = dls.dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dl[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms   =  [[add(10)], [Categorize()]]\n",
    "xtfms = tfms[0]\n",
    "tfms[0] = []\n",
    "if tfms is not None: tfms = [[ItemGetter(i)] + t for i,t in enumerate(tfms)]\n",
    "tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets.tls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms   =  [[add(10)], [Categorize()]]\n",
    "if tfms is not None: tfms = [[ItemGetter(i)] + t for i,t in enumerate(tfms)]\n",
    "tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(L(dsets.tls[0, 2, 3]).map(ToTSTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TSDatasets(Datasets):\n",
    "    \n",
    "#     @delegates(plt.subplots)\n",
    "#     def show(self, idx, **kwargs):\n",
    "#         for i in idx if isinstance(idx, list) else [idx]: self.decode(self[i]).show(**kwargs)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     def decode(self, o, full=True): return TSTuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #export\n",
    "# class TSTfmdDL(TfmdDL): \n",
    "\n",
    "#     @property\n",
    "#     def vars(self): return self.dataset[0][0].shape[-2]\n",
    "    \n",
    "#     @property\n",
    "#     def len(self): return self.dataset[0][0].shape[-1]\n",
    "\n",
    "#     @delegates(plt.subplots)\n",
    "#     def show_batch(self, b=None, max_n=9, nrows=3, ncols=3, figsize=(12, 10), **kwargs):\n",
    "#         if b is None: b = self.one_batch()\n",
    "#         db = self.decode_batch(b, max_n=max_n)\n",
    "#         if nrows is None: \n",
    "#             sqrt = math.sqrt(len(db))\n",
    "#             rows = min(math.ceil(sqrt), len(db)) \n",
    "#         if ncols is None: ncols = len(db) // rnows\n",
    "#         fig, axs = plt.subplots(nrows=nrows, ncols=ncols, figsize=figsize,  **kwargs)\n",
    "#         for tup, ax in zip(db[:nrows ** 2], [axs] if nrows == 1 else axs.flatten()): \n",
    "#             show_tuple(tup, ax=ax)\n",
    "#         plt.tight_layout()\n",
    "\n",
    "# @delegates(plt.subplots)\n",
    "# def show_tuple(tup, ax=None, **kwargs):\n",
    "#     \"Display a timeseries plot from a tuple\"\n",
    "#     tup[0].show(title='unlabeled' if len(tup) == 1 else tup[1], ax=ax, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class TSTensor(TensorBase):\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, x): \n",
    "        return cls(x)\n",
    "        return cls(To2dplusTensor(x)) # creates a TSTensor with at least 2 dimensions\n",
    "\n",
    "    @property\n",
    "    def vars(self): return self.shape[-2]\n",
    "\n",
    "    @property\n",
    "    def len(self): return self.shape[-1]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        res = super().__getitem__(idx)\n",
    "        return retain_type(res, self)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.ndim == 2:   return f'TSTensor(vars={self.shape[0]}, len={self.shape[1]})'\n",
    "        elif self.ndim > 2:  return f'TSTensor(samples={self.shape[-3]}, vars={self.shape[-2]}, len={self.shape[-1]})'\n",
    "\n",
    "    def show(self, ax=None, ctx=None, title=None, **kwargs):\n",
    "        ax = ifnone(ax,ctx)\n",
    "        if ax is None: fig, ax = plt.subplots(**kwargs)\n",
    "        ax.plot(self.T)\n",
    "        ax.axis(xmin=0, xmax=self.shape[-1] - 1)\n",
    "        ax.set_title(title, weight='bold')\n",
    "        plt.tight_layout()\n",
    "        return ax\n",
    "    \n",
    "@Transform\n",
    "def ToTSTensor(x:np.ndarray): return TSTensor.create(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms   =  [[ItemGetter(0), ], [ItemGetter(1), Categorize()]]\n",
    "items  =  itemify(X,y)\n",
    "splits =  IndexSplitter(split_idx[1])(items)\n",
    "dsets  =  Datasets(items, tfms=tfms, splits=splits)\n",
    "dls    =  TfmdDL(dsets, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms   =  [[ItemGetter(0), ], [ItemGetter(1), Categorize()]]\n",
    "TSDatasets(X,y, tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tuple(L(dsets[0]).map(ToTSTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ToTSTensor(dsets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = None\n",
    "tls = None\n",
    "\n",
    "kwargs = {}\n",
    "tfms   =  [[ItemGetter(0), ], [ItemGetter(1), Categorize()]]\n",
    "\n",
    "if items is None: \n",
    "    if y is not None: items = itemify(X, y)\n",
    "    else: items = itemify(X,)\n",
    "\n",
    "tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))])\n",
    "\n",
    "res = tuple([tl[it] for tl in tls])\n",
    "res if is_indexer(it) else list(zip(*res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tls = L(tls if tls else [TfmdLists(items, t, **kwargs) for t in L(ifnone(tfms,[None]))]).zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "\n",
    "if is_indexer(it) : res = ToTSTensor(tls[it])\n",
    "else: res = list(zip(*res))\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit ToTSTensor(tls[it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = itemify(X,y)\n",
    "items.map(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## New heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_eq(items, pickle.loads(pickle.dumps(items)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls = TSTfmdDL(dsets, bs=128)\n",
    "dls.vars, dls.len, dls.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls = TfmdDL(dsets, bs=128, )\n",
    "xb, yb = next(iter(dls.train))\n",
    "xb, yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = dls.one_batch()\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai2.data.all import *\n",
    "dblock = DataBlock(blocks=(TSTensorBlock, CategoryBlock),\n",
    "                   get_x=ItemGetter(0), get_y=ItemGetter(1), \n",
    "                   splitter=IndexSplitter(split_idx[1])\n",
    "                  )\n",
    "dsets1 =  dblock.datasets(itemify(X,y))\n",
    "dls1 =    TSTfmdDL(dsets, bs=128, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms   =  [[ItemGetter(0), ToTSTensor], [ItemGetter(1), Categorize()]]\n",
    "items  =  itemify(X,y)\n",
    "splits =  IndexSplitter(split_idx[1])(items)\n",
    "dsets2  =  TSDatasets(items, tfms=tfms, splits=splits)\n",
    "dls2    =  TSTfmdDL(dsets, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms    =  [[ItemGetter(0)], [ItemGetter(1)]]\n",
    "items   =  itemify(X,y)\n",
    "splits  =  IndexSplitter(split_idx[1])(items)\n",
    "dsets3  =  Datasets(items, tfms=tfms, splits=splits)\n",
    "dls3    =  TfmdDL(dsets, bs=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls3.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls2.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%time cycle_dl(dls3.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def unzip(tup):  return list(zip(*tup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit TupleGetter(0)(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tls = L([TfmdLists(items, [ItemGetter(0), ToTSTensor]), TfmdLists(items, [ItemGetter(1), Categorize()])]).zipped()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delegates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit tuple([tls[0][0], tls[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tls[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit ToTSTensor(items[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit tls[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit (tls[it] for tl in tls]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit ([tl[it] for tl in tls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit tuple([tl[it] for tl in tls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%timeit TfmdLists(items, ItemGetter(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb,yb = dls1.one_batch()\n",
    "xb[0], yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb,yb = dls2.one_batch()\n",
    "xb[0], yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls2.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai2.callback.all import *\n",
    "# model =  ResNet(dls.vars, dls.c)\n",
    "model  = inception_time(dls.vars, dls.c)\n",
    "learn =  Learner(dls1.dataloaders(),\n",
    "                model,\n",
    "                loss_func=nn.CrossEntropyLoss(),\n",
    "                metrics=accuracy)\n",
    "learn.fit_one_cycle(25, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# New heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# class TSTuple(tuple):\n",
    "    \n",
    "#     @delegates(plt.subplots)\n",
    "#     def show(self, ax=None, **kwargs):\n",
    "#         if ax is None: fig, ax = plt.subplots(**kwargs)\n",
    "#         ax.plot(self[0].T)\n",
    "#         ax.axis(xmin=0, xmax=self[0].shape[-1] - 1)\n",
    "#         ax.set_title('unlabeled' if len(self) == 1 else self[1], weight='bold')\n",
    "#         if ax is None: \n",
    "#             plt.tight_layout()\n",
    "#             plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "# class TSDatasets(Datasets):\n",
    "    \n",
    "#     @delegates(plt.subplots)\n",
    "#     def show(self, idx, **kwargs):\n",
    "#         for i in idx if isinstance(idx, list) else [idx]: self.decode(self[i]).show(**kwargs)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "    \n",
    "#     def decode(self, o, full=True): return TSTuple(tl.decode(o_, full=full) for o_,tl in zip(o,tuplify(self.tls, match=o)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#export\n",
    "class TSTfmdDL(TfmdDL): \n",
    "\n",
    "    @property\n",
    "    def vars(self): return self.dataset[0][0].shape[-2]\n",
    "    \n",
    "    @property\n",
    "    def len(self): return self.dataset[0][0].shape[-1]\n",
    "\n",
    "    @delegates(plt.subplots)\n",
    "    def show_batch(self, b=None, max_n=9, rows=None, cols=None, figsize=(10, 10), **kwargs):\n",
    "        if b is None: b = self.one_batch()\n",
    "        db = self.decode_batch(b, max_n=max_n)\n",
    "        sqrt = math.sqrt(len(db))\n",
    "        if rows is None: rows = min(math.ceil(sqrt), len(db)) \n",
    "        if cols is None: cols = len(db) // rows\n",
    "        fig, axs = plt.subplots(rows, cols, figsize=figsize,  **kwargs)\n",
    "        for tup, ax in zip(db[:rows ** 2], [axs] if rows == 1 else axs.flatten()): tup.show(ax=ax, **kwargs)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've used 2 datasets to test the enw functionality.\n",
    "\n",
    "1. A large dataset that don't fit in memory (using np.memmap arrays)\n",
    "2. A smaller dataset that fits in memory (using regular np.ndarrays)\n",
    "\n",
    "As you can see, the only difference is really how you load the data:\n",
    "\n",
    "1. To create a normal array load is with mmap_mode = None (default value). \n",
    "2. To create an array on disk (np.memmap) you'll need to select a mmap_mode. I normally use c, since I don't want to modify data on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsid = 'StarLightCurves'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if dsid == 'memmap':\n",
    "    X = np.load('X_on_disk.npy', mmap_mode='c')\n",
    "    y = np.load('y_on_disk.npy', mmap_mode='c')\n",
    "    sel_arr_class = X.__class__.__name__\n",
    "\n",
    "elif dsid == 'memmap_small':\n",
    "    X = np.load('X_on_disk_small.npy', mmap_mode='c')\n",
    "    y = np.load('y_on_disk_small.npy', mmap_mode='c')\n",
    "    sel_arr_class = X.__class__.__name__\n",
    "\n",
    "elif dsid == 'numeric':\n",
    "    X = np.zeros((100, 5, 10)) + np.arange(100).reshape(100, 1, 1)\n",
    "    y = np.arange(100)\n",
    "    sel_arr_class = X.__class__.__name__\n",
    "    \n",
    "else:\n",
    "    X_train, y_train, X_valid, y_valid = get_UCR_data(dsid, path='..', verbose=False)\n",
    "    X = np.concatenate((X_train, X_valid))\n",
    "    y = np.concatenate((y_train, y_valid))\n",
    "    folder = 'data/UCR'\n",
    "    np.save(str(PATH.parent/f'{folder}/{dsid}/X.npy'), X) # cannot use pathlib.PosixPath as filename \n",
    "    np.save(str(PATH.parent/f'{folder}/{dsid}/y.npy'), y)\n",
    "    del X, y\n",
    "    X = np.load(str(PATH.parent/f'{folder}/{dsid}/X.npy'), mmap_mode='c')\n",
    "    y = np.load(str(PATH.parent/f'{folder}/{dsid}/y.npy'), mmap_mode='c')\n",
    "    sel_arr_class = X.__class__.__name__\n",
    "    \n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# class TSTransformBlock():\n",
    "#     \"A basic wrapper that links defaults transforms for the data block API\"\n",
    "#     def __init__(self, type_tfms=None, item_tfms=None, batch_tfms=None, dl_type=None, dls_kwargs=None):\n",
    "#         self.type_tfms  =            L(type_tfms)\n",
    "#         self.item_tfms  = ToTensor + L(item_tfms)\n",
    "#         self.batch_tfms =            L(batch_tfms)\n",
    "#         self.dl_type,self.dls_kwargs = dl_type,({} if dls_kwargs is None else dls_kwargs)\n",
    "        \n",
    "# def TSTensorBlock(cls=TSTensor): return TransformBlock(type_tfms=cls.create)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "items = L(X, y).zip()\n",
    "items[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "item = L(X, y).zip()[0]\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ToTSTensor(ItemGetter(0)(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "get_items_from_numpy = itemify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fastai2.data.all import *\n",
    "dblock = DataBlock(blocks=(TSTensorBlock, CategoryBlock),\n",
    "                   get_x=ItemGetter(0), get_y=ItemGetter(1), \n",
    "                   splitter=RandomSplitter()\n",
    "                  )\n",
    "dsets = dblock.datasets(L(X, y).zip())\n",
    "t = dsets[0]\n",
    "dsets.show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = dsets[0]\n",
    "type(dsets.decode(t)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.show(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.show(dsets.train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(dsets, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai2.data.all import *\n",
    "dblock = DataBlock(blocks=(TSTensorBlock, CategoryBlock),\n",
    "                   get_items=itemify,\n",
    "                   get_x=ItemGetter(0), get_y=ItemGetter(1), \n",
    "                   splitter=RandomSplitter()\n",
    "                  )\n",
    "dsets = dblock.datasets((X, y))\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.decode(dsets.train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(L(X, y).zip())\n",
    "# dls.show_batch(rows=3, cols=3, sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms = [[ItemGetter(0), ToTSTensor], [ItemGetter(1), Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = L(X, y).zip()\n",
    "dsets = Datasets(items, tfms=tfms, splits=splits)\n",
    "t = dsets[0]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t[0].data, t[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.show(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### No tfms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms = [[ItemGetter(0), ], [ItemGetter(1), ]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X, y)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets[0], (X[0], y if y is None else y[0]))\n",
    "test_eq(dsets[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# ALL DATA\n",
    "tfms = [[ItemGetter(0), ], [ItemGetter(1), ]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X, y)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets[0], (X[0], y if y is None else y[0]))\n",
    "test_eq(dsets[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "tfms = [[ItemGetter(0), ], [ItemGetter(1), ]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X, y)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets.train[0], (X[splits[0][0]], y if y is None else y[splits[0][0]]))\n",
    "test_eq(dsets.train[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# VALID\n",
    "tfms = [[ItemGetter(0), ], [ItemGetter(1), ]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X, y)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets.valid[0], (X[splits[1][0]], y if y is None else y[splits[1][0]]))\n",
    "test_eq(dsets.valid[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets.valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UNLABELED ALL\n",
    "tfms = [[ItemGetter(0), ToTSTensor], []]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X,)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "# test_eq(dsets[0], (X[0], ))\n",
    "# test_eq(dsets[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# UNLABELED TRAIN\n",
    "tfms = None\n",
    "splits = RandomSplitter()(X)\n",
    "items = tuple((samplify(X), ))\n",
    "dsets = NumpyDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets.train[0], (X[splits[0][0]], ))\n",
    "test_eq(dsets.train[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets.train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# UNLABELED VALID\n",
    "tfms = None\n",
    "splits = RandomSplitter()(X)\n",
    "items = tuple((samplify(X), ))\n",
    "dsets = NumpyDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets.valid[0], (X[splits[1][0]], ))\n",
    "test_eq(dsets.valid[0][0].__class__.__name__, sel_arr_class)\n",
    "dsets.valid[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Transforms & decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TFMS ALL\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = tuple((samplify(X), samplify(y)))\n",
    "dsets = NumpyDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets[0][0].__class__.__name__, 'TSTensor')\n",
    "test_eq(dsets[0][1].__class__.__name__, 'TensorCategory')\n",
    "dsets[0], dsets[0][0].data, dsets[0][1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TFMS TRAIN\n",
    "tfms = [[ToTSTensor], [Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = tuple((samplify(X), samplify(y)))\n",
    "dsets = NumpyDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets.train[0][0].__class__.__name__, 'TSTensor')\n",
    "test_eq(dsets.train[0][1].__class__.__name__, 'TensorCategory')\n",
    "test_eq(dsets.decode(dsets.train[0])[0].data, tensor(X[splits[0][0]]))\n",
    "test_eq(dsets.decode(dsets.train[0])[1], str(y[splits[0][0]]))\n",
    "dsets.train[0], dsets.train[0][0].data, dsets.train[0][1].data, dsets.decode(dsets.train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# TFMS VALID\n",
    "tfms = [[ToTSTensor], [Categorize(add_na=True)]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = tuple((samplify(X), samplify(y)))\n",
    "dsets = NumpyDatasets(items, tfms=tfms, splits=splits)\n",
    "test_eq(dsets[0][0].__class__.__name__, 'TSTensor')\n",
    "test_eq(dsets[0][1].__class__.__name__, 'TensorCategory')\n",
    "if dsid != 'numeric': \n",
    "    test_eq(dsets.decode(dsets.valid[0])[0].data, tensor(X[splits[1][0]]))\n",
    "    test_eq(dsets.decode(dsets.valid[0])[1], str(y[splits[1][0]]))\n",
    "dsets.valid[0], dsets.valid[0][0].data, dsets.valid[0][1].data, dsets.decode(dsets.valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decode\n",
    "dec_x, dec_y = dsets.decode(dsets[0])\n",
    "test_eq(dec_y, str(y[0]))\n",
    "dsets[0], dec_x, dec_y, type(dec_x), type(dec_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms = [[ItemGetter(0), ToTSTensor], [ItemGetter(1), Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = Lzip(X, y)\n",
    "dsets = TSDatasets(items, tfms=tfms, splits=splits)\n",
    "dsets.show(idx=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsets.valid.show(idx=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms = [[ItemGetter(0), ToTSTensor], [ItemGetter(1), Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items = itemify(X, y)\n",
    "dsets = Datasets(items, tfms=tfms, splits=splits)\n",
    "dls = TSTfmdDL(dsets, bs=16)\n",
    "xb,yb = dls.one_batch()\n",
    "(xb, yb), (xb[0], yb[0]), dls.decode_batch((xb, yb))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dl = TSTfmdDL(dsets.train, bs=16)\n",
    "xb,yb = train_dl.one_batch()\n",
    "if dsid == 'numeric': test_eq(train_dl.decode_batch((xb, yb))[0][1], str(y[splits[0][0]]))\n",
    "(xb[0], yb[0]), train_dl.decode_batch((xb, yb))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_dl = TSTfmdDL(dsets.valid, bs=16)\n",
    "xb,yb = valid_dl.one_batch()\n",
    "(xb[0], yb[0]), valid_dl.decode_batch((xb, yb))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dls.show_batch(max_n=9, figsize=(10,10), sharey=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dl.show_batch(max_n=9, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "valid_dl.show_batch(max_n=9, figsize=(10, 10), sharey=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from fastai2.torch_core import *\n",
    "from fastai2.layers import *\n",
    "from fastai2.imports import *\n",
    "\n",
    "# Iception Time paper: https://arxiv.org/abs/1909.04939\n",
    "\n",
    "class AdaptiveConcatPool1d(nn.Module):\n",
    "    \"Layer that concats `AdaptiveAvgPool1d` and `AdaptiveMaxPool1d`\"\n",
    "    def __init__(self, size=None):\n",
    "        super().__init__()\n",
    "        self.size = size or 1\n",
    "        self.ap = nn.AdaptiveAvgPool1d(self.size)\n",
    "        self.mp = nn.AdaptiveMaxPool1d(self.size)\n",
    "    def forward(self, x): return torch.cat([self.mp(x), self.ap(x)], 1)\n",
    "\n",
    "\n",
    "act_fn = nn.ReLU(inplace=True)\n",
    "def conv(ni, nf, ks=3, stride=1, bias=False):\n",
    "    return nn.Conv1d(ni, nf, kernel_size=ks, stride=stride, padding=ks//2, bias=bias)\n",
    "\n",
    "class Shortcut(Module):\n",
    "    \"Merge a shortcut with the result of the module by adding them. Adds Conv, BN and ReLU\"\n",
    "    def __init__(self, ni, nf, act_fn=act_fn): \n",
    "        self.act_fn=act_fn\n",
    "        self.conv=conv(ni, nf, 1)\n",
    "        self.bn=nn.BatchNorm1d(nf)\n",
    "    def forward(self, x): return act_fn(x + self.bn(self.conv(x.orig)))\n",
    "\n",
    "class InceptionModule(Module):\n",
    "    def __init__(self, ni, nb_filters=32, kss=[39, 19, 9], bottleneck_size=32, stride=1):\n",
    "        self.bottleneck = nn.Conv1d(ni, bottleneck_size, 1) if (bottleneck_size>0 and ni>1) else noop\n",
    "        self.convs = nn.ModuleList([conv(bottleneck_size if (bottleneck_size>1 and ni>1) else ni, nb_filters, ks) for ks in kss])\n",
    "        self.maxpool_bottleneck = nn.Sequential(nn.MaxPool1d(3, stride, padding=1), conv(ni, nb_filters, 1))\n",
    "        self.bn_relu = nn.Sequential(nn.BatchNorm1d((len(kss)+1)*nb_filters), nn.ReLU())\n",
    "    def forward(self, x):\n",
    "        bottled = self.bottleneck(x)\n",
    "        return self.bn_relu(torch.cat([c(bottled) for c in self.convs]+[self.maxpool_bottleneck(x)], dim=1))\n",
    "\n",
    "def inception_time(ni, nout, ks=40, depth=6, bottleneck_size=32, nb_filters=32, head=True):\n",
    "    layers = []\n",
    "    \n",
    "    # compute kernel sizes: eg for ks=40 => kss=[39, 19, 9] \n",
    "    kss = [ks // (2**i) for i in range(3)]\n",
    "    # ensure odd kss until nn.Conv1d with padding='same' is available in pytorch 1.3\n",
    "    kss = [ksi if ksi % 2 != 0 else ksi - 1 for ksi in kss]\n",
    "    n_ks = len(kss) + 1\n",
    "    for d in range(depth):\n",
    "        # Farid\n",
    "      # im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size))\n",
    "        im = SequentialEx(InceptionModule(ni if d==0 else n_ks*nb_filters, kss=kss, bottleneck_size=bottleneck_size if d > 0 else 0))\n",
    "        if d%3==2: im.append(Shortcut(n_ks*nb_filters, n_ks*nb_filters))      \n",
    "        layers.append(im)\n",
    "    head = [AdaptiveConcatPool1d(), Flatten(), nn.Linear(2*n_ks*nb_filters, nout)] if head else []\n",
    "    return  nn.Sequential(*layers, *head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "tfms =   [[ItemGetter(0), ToTSTensor], [ItemGetter(1), Categorize()]]\n",
    "splits = RandomSplitter()(X)\n",
    "items =  Lzip(X,y)\n",
    "dsets =  TSDatasets(items, tfms=tfms, splits=splits)\n",
    "dls =    TSTfmdDL(dsets, bs=16)\n",
    "model =  ResNet(dls.vars, dls.c)\n",
    "# model  = inception_time(dls.vars, dls.c)\n",
    "learn =  Learner(dls.dataloaders(),\n",
    "                model,\n",
    "                loss_func=nn.CrossEntropyLoss(),\n",
    "                metrics=accuracy,\n",
    "#                 cbs=VerboseCallback()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_dl, valid_dl = dls.dataloaders()\n",
    "train_ds, valid_ds = dls.train, dls.valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xb,yb=dls.one_batch()\n",
    "learn.loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fastai2.callback.all import *\n",
    "model =  ResNet(dls.vars, dls.c)\n",
    "# model  = inception_time(dls.vars, dls.c)\n",
    "learn =  Learner(dls.dataloaders(),\n",
    "                model,\n",
    "                loss_func=nn.CrossEntropyLoss(),\n",
    "                metrics=accuracy,\n",
    "#                 cbs=VerboseCallback()\n",
    "               )\n",
    "learn.fit_one_cycle(25, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# delegates(Learner.__init__)\n",
    "\n",
    "\n",
    "# # def cnn_learner(dls, arch, loss_func=None, pretrained=True, cut=None, splitter=None,\n",
    "# # y_range=None, config=None, n_in=3, n_out=None, normalize=True, **kwargs):\n",
    "# def ts_learner(dls,\n",
    "#                model=None,\n",
    "#                #opt_func=Ranger,\n",
    "#                loss_func=None,\n",
    "#                cbs=None,\n",
    "#                metrics=None,\n",
    "#                **kwargs):\n",
    "#     \"Build a ts learner with default settings if None is passed\"\n",
    "#     n_in = dls.vars #get_n_channels(dls.train)  # data.n_channels\n",
    "#     n_out = dls.c  # Number of classes\n",
    "\n",
    "#     if model is None:\n",
    "#         model = inception_time(n_in, n_out).to(device=default_device())\n",
    "#     #     if opt_func is None: opt_func = Ranger\n",
    "#     if loss_func is None: loss_func = LabelSmoothingCrossEntropy()\n",
    "#     if cbs is None: cbs = L(cbs)\n",
    "#     if metrics is None: metrics = accuracy\n",
    "\n",
    "#     learn = Learner(dls,\n",
    "#                     model,\n",
    "#                     #opt_func=opt_func,\n",
    "#                     loss_func=loss_func,\n",
    "# #                     metrics=metrics,\n",
    "# #                     **kwargs\n",
    "#                    )\n",
    "\n",
    "#     return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai2.data.all import *\n",
    "dblock = DataBlock(blocks=(TSTensorBlock, CategoryBlock),\n",
    "                   get_x=ItemGetter(0), get_y=ItemGetter(1), \n",
    "                   splitter=RandomSplitter()\n",
    "                  )\n",
    "dsets = dblock.datasets(itemify(X,y))\n",
    "dls =    TSTfmdDL(dsets, bs=16)\n",
    "# model =  ResNet(dls.vars, dls.c)\n",
    "model  = inception_time(dls.vars, dls.c)\n",
    "learn =  Learner(dls.dataloaders(),\n",
    "                model,\n",
    "                loss_func=nn.CrossEntropyLoss(),\n",
    "                metrics=accuracy,\n",
    "#                 cbs=VerboseCallback()\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from fastai2.callback.all import *\n",
    "learn.fit_one_cycle(25, lr_max=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
