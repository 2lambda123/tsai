{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "created by Ignacio Oguiza - email: oguiza@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/Rocket.svg\"  width=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROCKET (RandOm Convolutional KErnel Transform) is a new Time Series Classification (TSC) method that has just been released (Oct 29th, 2019), and has achieved **state-of-the-art performance on the UCR univariate time series classification datasets, surpassing HIVE-COTE (the previous state of the art since 2017) in accuracy, with exceptional speed compared to other traditional DL methods.** \n",
    "\n",
    "To achieve these 2 things at once is **VERY IMPRESSIVE**. ROCKET is certainly a new TSC method you should try.\n",
    "\n",
    "Authors:\n",
    "Dempster, A., Petitjean, F., & Webb, G. I. (2019). ROCKET: Exceptionally fast and accurate time series classification using random convolutional kernels. arXiv preprint arXiv:1910.13051.\n",
    "\n",
    "[paper](https://arxiv.org/pdf/1910.13051)\n",
    "\n",
    "There are 2 main limitations to the original ROCKET method though:\n",
    "- The release code doesn't handle multivariate data\n",
    "- It doesn't run on a GPU, so it's slow when used with a large datasets\n",
    "\n",
    "In this notebook you will learn: \n",
    "- how you can use the original ROCKET method\n",
    "- you will also learn about a new ROCKET version I have developed in Pytorch, that handles both **univariate and multivariate** data, and uses **GPU**\n",
    "- you will see how you can integrate the ROCKET features with fastai or other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries üìö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:00.570810Z",
     "start_time": "2020-04-11T18:44:48.106661Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "ISCOLAB = 'google.colab' in sys.modules\n",
    "if ISCOLAB:\n",
    "    # for bleeding edge\n",
    "    !pip install git+https://github.com/fastai/fastcore.git@master\n",
    "    !pip install git+https://github.com/fastai/fastai2.git@master\n",
    "    #!pip install git+https://github.com/timeseriesAI/timeseriesAI.git@master \n",
    "    \n",
    "    # for latest stable version\n",
    "    !pip install tsai \n",
    "    \n",
    "import tsai\n",
    "from tsai.all import *\n",
    "display(HTML(\"<style>.container {width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:00.600648Z",
     "start_time": "2020-04-11T18:45:00.577029Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsai       : 0.0.5\n",
      "fastai2    : 0.0.17\n",
      "fastcore   : 0.1.17\n",
      "torch      : 1.4.0\n",
      "scipy      : 1.3.1\n",
      "numpy      : 1.18.1\n",
      "pandas     : 0.25.1\n",
      "Total RAM  :  8.00 GB\n",
      "Used RAM   :  3.45 GB\n",
      "n_cpus     : 8\n",
      "device     : cpu\n"
     ]
    }
   ],
   "source": [
    "print('tsai       :', tsai.__version__)\n",
    "print('fastai2    :', fastai2.__version__)\n",
    "print('fastcore   :', fastcore.__version__)\n",
    "print('torch      :', torch.__version__)\n",
    "print('scipy      :', sp.__version__)\n",
    "print('numpy      :', np.__version__)\n",
    "print('pandas     :', pd.__version__)\n",
    "print(f'Total RAM  : {bytes2GB(psutil.virtual_memory().total):5.2f} GB')\n",
    "print(f'Used RAM   : {bytes2GB(psutil.virtual_memory().used):5.2f} GB')\n",
    "print('n_cpus     :', cpus)\n",
    "iscuda = torch.cuda.is_available()\n",
    "if iscuda: print('device     : {} ({})'.format(device, torch.cuda.get_device_name(0)))\n",
    "else: print('device     :', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the original ROCKET method? üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROCKET is applied in 2 phases:\n",
    "\n",
    "1. Generate features from each time series: ROCKET calculates 20k features from each time series, independently of the sequence length. \n",
    "2. Apply a classifier to those calculated features. Those features are then used by the classifier of your choice. In the original code they use 2 simple linear classifiers: RidgeClassifierCV and Logistic Regression, but you can use any classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "Let's first generate the features. We'll import data from a UCR Time Series dataset.\n",
    "\n",
    "The original method requires the time series to be in a 2d array of shape (samples, len)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:29.788393Z",
     "start_time": "2020-04-12T16:45:29.700527Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = get_UCR_data('OliveOil')\n",
    "seq_len = X_train.shape[-1]\n",
    "X_train = X_train[:, 0]\n",
    "X_valid = X_valid[:, 0]\n",
    "labels = np.unique(y_train)\n",
    "transform = {}\n",
    "for i, l in enumerate(labels): transform[l] = i\n",
    "y_train = np.vectorize(transform.get)(y_train)\n",
    "y_valid = np.vectorize(transform.get)(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we normalize the data to mean 0 and std 1 **'per sample'** (recommended by the authors), that is they set each sample to mean 0 and std 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:32.510279Z",
     "start_time": "2020-04-12T16:45:32.483975Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = (X_train - X_train.mean(axis = 1, keepdims = True)) / (X_train.std(axis = 1, keepdims = True) + 1e-8)\n",
    "X_valid = (X_valid - X_valid.mean(axis = 1, keepdims = True)) / (X_valid.std(axis = 1, keepdims = True) + 1e-8)\n",
    "X_train.mean(axis = 1, keepdims = True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To generate the features, we first need to create the 10k random kernels that will be used to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:37.986657Z",
     "start_time": "2020-04-12T16:45:35.515425Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kernels = generate_kernels(seq_len, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now we apply those ramdom kernels to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:48.648056Z",
     "start_time": "2020-04-12T16:45:38.388086Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train_tfm = apply_kernels(X_train, kernels)\n",
    "X_valid_tfm = apply_kernels(X_valid, kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 2Ô∏è‚É£ Apply a classifier\n",
    "\n",
    "So now we have the features, and we are ready to apply a classifier. \n",
    "\n",
    "Let's use a simple, linear RidgeClassifierCV as they propose in the paper. We first instantiate it. \n",
    "\n",
    "Note:\n",
    "alphas: Array of alpha values to try. Regularization strength; must be a positive float. Regularization improves the conditioning of the problem and reduces the variance of the estimates. Larger values specify stronger regularization. Alpha corresponds to C^-1 in other linear models such as LogisticRegression or LinearSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:48.662920Z",
     "start_time": "2020-04-12T16:45:48.654003Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:48.732489Z",
     "start_time": "2020-04-12T16:45:48.672074Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifierCV(alphas=array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]),\n",
       "                  class_weight=None, cv=None, fit_intercept=True,\n",
       "                  normalize=True, scoring=None, store_cv_values=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " classifier.fit(X_train_tfm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:45:48.757533Z",
     "start_time": "2020-04-12T16:45:48.739936Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_valid_tfm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚ò£Ô∏è **This is pretty impressive! It matches or exceeds the state-of-the-art performance without any fine tuning in <2 seconds!!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:50:37.953778Z",
     "start_time": "2020-04-11T18:50:32.183385Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernels = generate_kernels(seq_len, 10000)\n",
    "X_train_tfm = apply_kernels(X_train, kernels)\n",
    "X_valid_tfm = apply_kernels(X_valid, kernels)\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)\n",
    "classifier.fit(X_train_tfm, y_train)\n",
    "classifier.score(X_valid_tfm, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "‚ö†Ô∏è Bear in mind that this process is not deterministic since there is randomness involved in the kernels. In thiis case, performance may vary between .9 to .933"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use ROCKET with large and/ or multivariate datasets on GPU? ‚ùì"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated before, the current ROCKET method doesn't support multivariate time series or GPU. This may be a drawback in some cases. \n",
    "\n",
    "To overcome both limitations I've created a multivariate ROCKET on GPU in Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way to use ROCKET in Pytorch is the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1Ô∏è‚É£ Generate features\n",
    "\n",
    "First you prepare the input data and normalize it per sample. The input to ROCKET Pytorch is a 3d tensor of shape (samples, vars, len), preferrable on gpu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:48:58.122839Z",
     "start_time": "2020-04-12T16:48:57.970079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([160, 10, 400]) torch.Size([74, 10, 400]) (160,) (74,)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_valid, y_valid = get_UCR_data('HandMovementDirection')\n",
    "_, features, seq_len = X_train.shape\n",
    "X_train = (X_train - X_train.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_train.std(axis=(1, 2), keepdims=True) + 1e-8)\n",
    "X_valid = (X_valid - X_valid.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_valid.std(axis=(1, 2), keepdims=True) + 1e-8)\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32, device=device)\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you instantiate the Pytorch model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:49:15.792973Z",
     "start_time": "2020-04-12T16:49:05.311270Z"
    }
   },
   "outputs": [],
   "source": [
    "n_kernels=10_000\n",
    "kss=[7, 9, 11]\n",
    "model = ROCKET(features, seq_len, n_kernels=n_kernels, kss=kss).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we now transform the original data, creating 20k features per sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:54:26.992839Z",
     "start_time": "2020-04-12T16:49:19.384599Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-06facfa6e9ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train_tfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_valid_tfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-5d6b1c3a7191>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_kernels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0m_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0m_ppv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m    201\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 202\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_tfm = model(X_train)\n",
    "X_valid_tfm = model(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Apply a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you apply a classifier of your choice. \n",
    "With RidgeClassifierCV in particular, there's no need to normalize the calculated features before passing them to the classifier, as it does it internally (if normalize is set to True as recommended by the authors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.359201Z",
     "start_time": "2020-04-11T18:44:48.172Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "ridge = RidgeClassifierCV(alphas=np.logspace(-8, 8, 17), normalize=True)\n",
    "ridge.fit(X_train_tfm, y_train)\n",
    "print('alpha: {:.2E}  train: {:.5f}  valid: {:.5f}'.format(ridge.alpha_, \n",
    "                                                           ridge.score(X_train_tfm, y_train), \n",
    "                                                           ridge.score(X_valid_tfm, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the case of other classifiers (like Logistic Regression), the authors recommend a per-feature normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.363437Z",
     "start_time": "2020-04-11T18:44:48.183Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "eps = 1e-6\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i, C in enumerate(Cs):\n",
    "    f_mean = X_train_tfm.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train_tfm.std(axis=0, keepdims=True) + eps  # epsilon to avoid dividing by 0\n",
    "    X_train_tfm2 = (X_train_tfm - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid_tfm - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2} eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This result is amazing!! The previous state of the art (Inceptiontime) was .37837\n",
    "\n",
    "‚ò£Ô∏è Note: Epsilon has a large impact on the result. You can actually test several values to find the one that best fits your problem, but bear in mind you can only select C and epsilon based on train data!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "##### RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One way to do this would be to performa a random search using several epsilon and C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.367687Z",
     "start_time": "2020-04-11T18:44:48.194Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_tests = 10\n",
    "epss = np.logspace(-8, 0, 9)\n",
    "Cs = np.logspace(-5, 5, 11)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "best_loss = np.inf\n",
    "for i in range(n_tests):\n",
    "    eps = np.random.choice(epss)\n",
    "    C = np.random.choice(Cs)\n",
    "    f_mean = X_train_tfm.mean(axis=0, keepdims=True)\n",
    "    f_std = X_train_tfm.std(axis=0, keepdims=True) + eps  # epsilon\n",
    "    X_train_tfm2 = (X_train_tfm - f_mean) / f_std\n",
    "    X_valid_tfm2 = (X_valid_tfm - f_mean) / f_std\n",
    "    classifier = LogisticRegression(penalty='l2', C=C, n_jobs=-1)\n",
    "    classifier.fit(X_train_tfm2, y_train)\n",
    "    probas = classifier.predict_proba(X_train_tfm2)\n",
    "    loss = nn.CrossEntropyLoss()(torch.tensor(probas), torch.tensor(y_train)).item()\n",
    "    train_score = classifier.score(X_train_tfm2, y_train)\n",
    "    val_score = classifier.score(X_valid_tfm2, y_valid)\n",
    "    if loss < best_loss:\n",
    "        best_eps = eps\n",
    "        best_C = C\n",
    "        best_loss = loss\n",
    "        best_train_score = train_score\n",
    "        best_val_score = val_score\n",
    "    print('{:2}  eps: {:.2E}  C: {:.2E}  loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        i, eps, C, loss, train_score, val_score))\n",
    "print('\\nBest result:')\n",
    "print('eps: {:.2E}  C: {:.2E}  train_loss: {:.5f}  train_acc: {:.5f}  valid_acc: {:.5f}'.format(\n",
    "        best_eps, best_C, best_loss, best_train_score, best_val_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Results: accuracy and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've checked the performance for all 85 univariate datasets in the UCR archive, and performance is the same as the published one. \n",
    "\n",
    "**Average performance accross all datasets is 85.100% (original) vs\t85.085% (with the Pytorch version).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I've also run some tests to and compared performance (time) of ROCKET original code vs Pytorch. These are the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"./images/Time_ROCKET_comp.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, as you can see, in small datasets, the original method may be a bit faster than the GPU method, but for larger datasets, there's a great benefit in using the GPU version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In addition to this, I have also run the code on the TSC UCR multivariate datasets (all the ones that don't contain nan values), and the results are also very good, beating the previous state-of-the-art in this category as well by a large margin. For example, ROCKET reduces InceptionTime errors by 26% on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.372357Z",
     "start_time": "2020-04-11T18:44:48.209Z"
    },
    "hidden": true,
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "iterations = 5\n",
    "datasets = sorted(get_UCR_multivariate_list())\n",
    "n_kernels=10_000\n",
    "kss=[7, 9, 11]\n",
    "\n",
    "ds_ = []\n",
    "ds_nl_ = []\n",
    "ds_di_ = []\n",
    "ds_type_ = []\n",
    "iters_ = []\n",
    "means_ = []\n",
    "stds_ = []\n",
    "times_ = []\n",
    "alphas_ = []\n",
    "\n",
    "datasets = listify(datasets)\n",
    "for dsid in datasets:\n",
    "    try: \n",
    "        print(f'\\nProcessing {dsid} dataset...\\n')\n",
    "        if dsid in get_UCR_univariate_list(): scale_type=None\n",
    "        else: scale_type='standardize'\n",
    "        data = create_UCR_databunch(dsid, bs=2048, scale_type=scale_type, verbose=True)\n",
    "        X_train = data.train_ds.x.items.astype(np.float32)\n",
    "        X_valid = data.valid_ds.x.items.astype(np.float32)\n",
    "        y_train = data.train_ds.y.items.astype(np.int64)\n",
    "        y_valid = data.valid_ds.y.items.astype(np.int64)\n",
    "    except: \n",
    "        ds_nl_.append(dsid)\n",
    "        print(f'\\n...{dsid} cannot be loaded\\n')\n",
    "        continue\n",
    "    if np.isinf(X_train).sum() + np.isnan(X_train).sum() > 0: \n",
    "        ds_di_.append(dsid)\n",
    "        print(f'...{dsid} contains inf or nan\\n')\n",
    "        continue\n",
    "        \n",
    "    if data.features == 1: ds_type_.append('univariate')\n",
    "    else: ds_type_.append('multivariate')\n",
    "    \n",
    "    score_ = []\n",
    "    elapsed_times_ = []\n",
    "    for i in range(iterations):\n",
    "        start_time = time.time()\n",
    "        model = ROCKET(data.features, data.seq_len, n_kernels=n_kernels,kss=kss).to(device)\n",
    "        for j, (xb,yb) in enumerate(data.train_dl.new(shuffle=False, drop_last=False)):\n",
    "            with torch.no_grad(): out = model(xb)\n",
    "            X_train_tfm = out if j == 0 else torch.cat((X_train_tfm, out))\n",
    "        for j, (xb,yb) in enumerate(data.valid_dl.new(shuffle=False, drop_last=False)):\n",
    "            with torch.no_grad(): out = model(xb)\n",
    "            X_valid_tfm = out if j == 0 else torch.cat((X_valid_tfm, out))\n",
    "        classifier = RidgeClassifierCV(alphas=np.logspace(-3, 3, 7), normalize=True)\n",
    "        classifier.fit(X_train_tfm, y_train)\n",
    "        score = classifier.score(X_valid_tfm, y_valid)\n",
    "        score_.append(score)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        elapsed_times_.append(elapsed_time)\n",
    "        alphas_.append(classifier.alpha_)\n",
    "        print('   {:2} - score: {:.5f}  alpha: {:.2E}  time(s): {:4.0f}'.\\\n",
    "              format(i + 1, score, classifier.alpha_, elapsed_time))\n",
    "    ds_.append(dsid)\n",
    "    iters_.append(iterations)\n",
    "    means_.append('{:.6f}'.format(np.mean(score_)))\n",
    "    stds_.append('{:.6f}'.format(np.std(score_)))\n",
    "    times_.append('{:.0f}'.format(np.mean(elapsed_times_)))\n",
    "    if len(datasets) >1: clear_output()\n",
    "    df = pd.DataFrame(np.stack((ds_, ds_type_, iters_,  means_, stds_, times_)).T, \n",
    "             columns=['dataset', 'type', 'iterations',  'mean_accuracy', 'std_accuracy',  'time(s)'])\n",
    "    pd.set_option('display.max_columns', 999)\n",
    "    pd.set_option('display.max_rows', 999)\n",
    "    display(df)\n",
    "    pd.set_option('display.max_columns', 20)\n",
    "    pd.set_option('display.max_rows', 60)\n",
    "    if ds_nl_ != []: print('\\n(*) datasets not loaded      :', ds_nl_)\n",
    "    if ds_di_ != []:print('(*) datasets with data issues:', ds_di_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This is the comparison with InceptionTime:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"./images/ROCKET_InceptionTime_multi.png\"  width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And how to use it with other classifiers? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you build the 20k features per sample, you can use them to train any classifier of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use fastai, for example, with a logistic regression, you could should then build a databunch and a logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the steps to use ROCKET with a classifier in fastai:\n",
    "\n",
    "1. Normalize the data 'per sample' (if not previously normalized)\n",
    "2. Calculate features\n",
    "3. Normalize calculated features 'per feature' (you'll get 20k means and stds)\n",
    "4. Create the databunch passing the normalized calculated features as 3d tensor/arrays to a TSList."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-12T16:54:40.737427Z",
     "start_time": "2020-04-12T16:54:32.680039Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-d435741227df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# calculate 20k features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mROCKET\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_kernels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mX_train_tfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mX_valid_tfm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-5d6b1c3a7191>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, c_in, seq_len, n_kernels, kss)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mks\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdilation\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mweight\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dsid = 'OliveOil'\n",
    "bs = 30\n",
    "eps = 1e-6\n",
    "\n",
    "# extract data\n",
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)\n",
    "\n",
    "# normalize data 'per sample'\n",
    "X_train = (X_train - X_train.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_train.std(axis=(1, 2), keepdims=True) + eps)\n",
    "X_valid = (X_valid - X_valid.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_valid.std(axis=(1, 2), keepdims=True) + eps)\n",
    "\n",
    "# calculate 20k features\n",
    "_, features, seq_len = X_train.shape\n",
    "model = ROCKET(features, seq_len, n_kernels=10000, kss=[7, 9, 11]).to(device)\n",
    "X_train_tfm = model(torch.tensor(X_train, device=device)).unsqueeze(1)\n",
    "X_valid_tfm = model(torch.tensor(X_valid, device=device)).unsqueeze(1)\n",
    "\n",
    "# normalize 'per feature'\n",
    "f_mean = X_train_tfm.mean(dim=0, keepdims=True)\n",
    "f_std = X_train_tfm.std(dim=0, keepdims=True) + eps\n",
    "X_train_tfm_norm = (X_train_tfm - f_mean) / f_std\n",
    "X_valid_tfm_norm = (X_valid_tfm - f_mean) / f_std\n",
    "\n",
    "# To use fastai v2 we concatenate train and valid, and calculate the splits:\n",
    "X = np.concatenate((X_train_tfm_norm, X_valid_tfm_norm))\n",
    "y = np.concatenate((y_train, y_valid))\n",
    "splits = (L(list(np.arange(len(X_train)))), L(list(np.arange(len(X_train), len(X)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now need ot create datasets and dataloaders in fastai v2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.380891Z",
     "start_time": "2020-04-11T18:44:48.231Z"
    }
   },
   "outputs": [],
   "source": [
    "tfms  = [None, [Categorize()]]\n",
    "dsets = TSDatasets(X, y, splits=splits, inplace=True)\n",
    "dls   = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[64, 128], num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output of ROCKET."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.385793Z",
     "start_time": "2020-04-11T18:44:48.241Z"
    }
   },
   "outputs": [],
   "source": [
    "dls.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we'll use a very simple: a Logistic Regression with 20k input features and 2 classes in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.389576Z",
     "start_time": "2020-04-11T18:44:48.250Z"
    }
   },
   "outputs": [],
   "source": [
    "def init(layer):\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.constant_(layer.weight.data, 0.)\n",
    "        nn.init.constant_(layer.bias.data, 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.393633Z",
     "start_time": "2020-04-11T18:44:48.258Z"
    }
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(20_000, data.c))\n",
    "model.apply(init)\n",
    "learn = Learner(dls, model, metrics=accuracy)\n",
    "learn.save('stage-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.398628Z",
     "start_time": "2020-04-11T18:44:48.267Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.lr_find()\n",
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.403252Z",
     "start_time": "2020-04-11T18:44:48.276Z"
    }
   },
   "outputs": [],
   "source": [
    "learn.load('stage-0')\n",
    "learn.fit_one_cycle(10, max_lr=3e-3, wd=1e2)\n",
    "learn.recorder.plot_losses()\n",
    "learn.recorder.plot_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an excellent result we have achieved with a very simple classifier (Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Boosted trees: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.408479Z",
     "start_time": "2020-04-11T18:44:48.286Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dsid = 'OliveOil'\n",
    "eps = 1e-6\n",
    "\n",
    "# extract data\n",
    "X_train, y_train, X_valid, y_valid = get_UCR_data(dsid)\n",
    "\n",
    "# normalize data 'per sample'\n",
    "X_train = (X_train - X_train.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_train.std(axis=(1, 2), keepdims=True) + eps)\n",
    "X_valid = (X_valid - X_valid.mean(axis=(1, 2), keepdims=True)) / (\n",
    "    X_valid.std(axis=(1, 2), keepdims=True) + eps)\n",
    "\n",
    "# calculate 20k features\n",
    "_, features, seq_len = X_train.shape\n",
    "model = ROCKET(features, seq_len, n_kernels=10000, kss=[7, 9, 11]).to(device)\n",
    "X_train_tfm = model(torch.tensor(X_train, device=device))\n",
    "X_valid_tfm = model(torch.tensor(X_valid, device=device))\n",
    "\n",
    "# normalize 'per feature'\n",
    "f_mean = X_train_tfm.mean(dim=0, keepdims=True)\n",
    "f_std = X_train_tfm.std(dim=0, keepdims=True) + eps\n",
    "X_train_tfm_norm = (X_train_tfm - f_mean) / f_std\n",
    "X_valid_tfm_norm = (X_valid_tfm - f_mean) / f_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-11T18:45:04.412683Z",
     "start_time": "2020-04-11T18:44:48.294Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "classifier = xgb.XGBClassifier(max_depth=3,\n",
    "                               learning_rate=0.1,\n",
    "                               n_estimators=100,\n",
    "                               verbosity=1,\n",
    "                               objective='binary:logistic',\n",
    "                               booster='gbtree',\n",
    "                               tree_method='auto',\n",
    "                               n_jobs=1,\n",
    "                               gpu_id=-1,\n",
    "                               gamma=0,\n",
    "                               min_child_weight=1,\n",
    "                               max_delta_step=0,\n",
    "                               subsample=.5,\n",
    "                               colsample_bytree=1,\n",
    "                               colsample_bylevel=1,\n",
    "                               colsample_bynode=1,\n",
    "                               reg_alpha=0,\n",
    "                               reg_lambda=1,\n",
    "                               scale_pos_weight=1,\n",
    "                               base_score=0.5,\n",
    "                               random_state=0,\n",
    "                               missing=None)\n",
    "\n",
    "classifier.fit(X_train_tfm, y_train)\n",
    "preds = classifier.predict(X_valid_tfm)\n",
    "(preds == y_valid).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "ROCKET is a great method for TSC that has established a new level of performance both in terms of accuracy and time. It does it by successfully applying an approach quite different from the traditional DL approaches. The method uses 10k random kernels to generate features that are then classified by linear classifiers (although you may use a classifier of your choice).\n",
    "The original method has 2 limitations (lack of multivariate and lack of GPU support) that are overcome by the Pytorch implementation shared in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
