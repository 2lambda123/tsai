---

title: Optimizers


keywords: fastai
sidebar: home_sidebar

summary: "This contains a set of optimizers."
description: "This contains a set of optimizers."
nb_path: "nbs/009_optimizer.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/009_optimizer.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LookAhead" class="doc_header"><code>class</code> <code>LookAhead</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L18" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LookAhead</code>(<strong><code>base_optimizer</code></strong>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>6</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Ralamb" class="doc_header"><code>class</code> <code>Ralamb</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L88" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Ralamb</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>betas</code></strong>=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="ralamb" class="doc_header"><code>ralamb</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L165" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>ralamb</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="RangerLars" class="doc_header"><code>RangerLars</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L168" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>RangerLars</code>(<strong><code>params</code></strong>, <strong>*<code>args</code></strong>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>6</code></em>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rangerlars" class="doc_header"><code>rangerlars</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L172" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>rangerlars</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Madam" class="doc_header"><code>class</code> <code>Madam</code><a href="" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Madam</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.01</code></em>, <strong><code>p_scale</code></strong>=<em><code>3.0</code></em>, <strong><code>g_bound</code></strong>=<em><code>10.0</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="Ranger" class="doc_header"><code>class</code> <code>Ranger</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L252" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>Ranger</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>alpha</code></strong>=<em><code>0.5</code></em>, <strong><code>k</code></strong>=<em><code>6</code></em>, <strong><code>N_sma_threshhold</code></strong>=<em><code>5</code></em>, <strong><code>betas</code></strong>=<em><code>(0.95, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-05</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>use_gc</code></strong>=<em><code>True</code></em>, <strong><code>gc_conv_only</code></strong>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Base class for all optimizers.</p>
<p>.. warning::
    Parameters need to be specified as collections that have a deterministic
    ordering that is consistent between runs. Examples of objects that don't
    satisfy those properties are sets and iterators over values of dictionaries.</p>
<p>Arguments:
    params (iterable): an iterable of :class:<code>torch.Tensor</code> s or
        :class:<code>dict</code> s. Specifies what Tensors should be optimized.
    defaults: (dict): a dict containing default values of optimization
        options (used when a parameter group doesn't specify them).</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="rangergc" class="doc_header"><code>rangergc</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L400" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>rangergc</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="AdaBelief" class="doc_header"><code>class</code> <code>AdaBelief</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L409" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>AdaBelief</code>(<strong><code>params</code></strong>, <strong><code>lr</code></strong>=<em><code>0.001</code></em>, <strong><code>betas</code></strong>=<em><code>(0.9, 0.999)</code></em>, <strong><code>eps</code></strong>=<em><code>1e-08</code></em>, <strong><code>weight_decay</code></strong>=<em><code>0</code></em>, <strong><code>amsgrad</code></strong>=<em><code>False</code></em>, <strong><code>weight_decouple</code></strong>=<em><code>False</code></em>, <strong><code>fixed_decay</code></strong>=<em><code>False</code></em>, <strong><code>rectify</code></strong>=<em><code>False</code></em>) :: <code>Optimizer</code></p>
</blockquote>
<p>Implements AdaBelief algorithm. Modified from Adam in PyTorch
Arguments:
    params (iterable): iterable of parameters to optimize or dicts defining
        parameter groups
    lr (float, optional): learning rate (default: 1e-3)
    betas (Tuple[float, float], optional): coefficients used for computing
        running averages of gradient and its square (default: (0.9, 0.999))
    eps (float, optional): term added to the denominator to improve
        numerical stability (default: 1e-8)
    weight<em>decay (float, optional): weight decay (L2 penalty) (default: 0)
    amsgrad (boolean, optional): whether to use the AMSGrad variant of this
        algorithm from the paper <code>On the Convergence of Adam and Beyond</code></em>
        (default: False)
    weight_decouple (boolean, optional): ( default: False) If set as True, then
        the optimizer uses decoupled weight decay as in AdamW
    fixed_decay (boolean, optional): (default: False) This is used when weight_decouple
        is set as True.
        When fixed<em>decay == True, the weight decay is performed as
        $W</em>{new} = W<em>{old} - W</em>{old} \times decay$.
        When fixed_decay == False, the weight decay is performed as
        $W<em>{new} = W</em>{old} - W_{old} \times decay \times lr$. Note that in this case, the
        weight decay ratio decreases with learning rate (lr).
    rectify (boolean, optional): (default: False) If set as True, then perform the rectified
        update similar to RAdam
reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients
           NeurIPS 2020 Spotlight</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="adabelief" class="doc_header"><code>adabelief</code><a href="https://github.com/timeseriesAI/tsai/tree/master/tsai/optimizer.py#L583" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>adabelief</code>(<strong>*<code>args</code></strong>, <strong>**<code>kwargs</code></strong>)</p>
</blockquote>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

