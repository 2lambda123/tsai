---

title: TSTransformerPlus


keywords: fastai
sidebar: home_sidebar

summary: "This is a PyTorch implementation created by Ignacio Oguiza (timeseriesAI@gmail.com)."
description: "This is a PyTorch implementation created by Ignacio Oguiza (timeseriesAI@gmail.com)."
nb_path: "nbs/124_models.TSTransformerPlus.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/124_models.TSTransformerPlus.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSTransformerPlus" class="doc_header"><code>class</code> <code>TSTransformerPlus</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/models/TSTransformerPlus.py#L56" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSTransformerPlus</code>(<strong><code>c_in</code></strong>:<code>int</code>, <strong><code>c_out</code></strong>:<code>int</code>, <strong><code>seq_len</code></strong>:<code>int</code>, <strong><code>n_layers</code></strong>:<code>int</code>=<em><code>3</code></em>, <strong><code>d_model</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>d_head</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>act</code></strong>:<code>str</code>=<em><code>'reglu'</code></em>, <strong><code>d_ff</code></strong>:<code>int</code>=<em><code>256</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>fc_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>res_attention</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>pre_norm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>use_cls_token</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>custom_head</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>) :: <a href="/models.TabFusionTransformer.html#Sequential"><code>Sequential</code></a></p>
</blockquote>
<p>A sequential container.
Modules will be added to it in the order they are passed in the constructor.
Alternatively, an ordered dict of modules can also be passed in.</p>
<p>To make it easier to understand, here is a small example::</p>

<pre><code># Example of using Sequential
model = nn.Sequential(
          nn.Conv2d(1,20,5),
          nn.ReLU(),
          nn.Conv2d(20,64,5),
          nn.ReLU()
        )

# Example of using Sequential with OrderedDict
model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1,20,5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20,64,5)),
          ('relu2', nn.ReLU())
        ]))</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TSTransformerPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
<span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TSTransformerPlus(
  (backbone): _TSTransformerBackbone(
    (lin): Linear(in_features=4, out_features=128, bias=True)
    (layers): ModuleList(
      (0): ModuleList(
        (0): MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=False)
          (W_K): Linear(in_features=128, out_features=128, bias=False)
          (W_V): Linear(in_features=128, out_features=128, bias=False)
          (fc): Linear(in_features=128, out_features=128, bias=False)
          (sdp_attn): ScaledDotProductAttention()
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): PositionwiseFeedForward(
          (0): Linear(in_features=128, out_features=1024, bias=True)
          (1): ReGLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=512, out_features=128, bias=True)
        )
        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (1): ModuleList(
        (0): MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=False)
          (W_K): Linear(in_features=128, out_features=128, bias=False)
          (W_V): Linear(in_features=128, out_features=128, bias=False)
          (fc): Linear(in_features=128, out_features=128, bias=False)
          (sdp_attn): ScaledDotProductAttention()
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): PositionwiseFeedForward(
          (0): Linear(in_features=128, out_features=1024, bias=True)
          (1): ReGLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=512, out_features=128, bias=True)
        )
        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
      (2): ModuleList(
        (0): MultiheadAttention(
          (W_Q): Linear(in_features=128, out_features=128, bias=False)
          (W_K): Linear(in_features=128, out_features=128, bias=False)
          (W_V): Linear(in_features=128, out_features=128, bias=False)
          (fc): Linear(in_features=128, out_features=128, bias=False)
          (sdp_attn): ScaledDotProductAttention()
          (dropout): Dropout(p=0.0, inplace=False)
        )
        (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
        (2): PositionwiseFeedForward(
          (0): Linear(in_features=128, out_features=1024, bias=True)
          (1): ReGLU()
          (2): Dropout(p=0.0, inplace=False)
          (3): Linear(in_features=512, out_features=128, bias=True)
        )
        (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (head): Sequential(
    (0): TokenLayer()
    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (2): Linear(in_features=128, out_features=2, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

