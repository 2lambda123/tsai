---

title: TSiT & InceptionTSiT


keywords: fastai
sidebar: home_sidebar

summary: "These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)"
description: "These are PyTorch implementations created by Ignacio Oguiza (timeseriesAI@gmail.com) based on ViT (Vision Transformer)"
nb_path: "nbs/124_models.TSiTPlus.ipynb"
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: nbs/124_models.TSiTPlus.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="TSiTPlus" class="doc_header"><code>class</code> <code>TSiTPlus</code><a href="https://github.com/timeseriesAI/tsai/tree/main/tsai/models/TSiTPlus.py#L97" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>TSiTPlus</code>(<strong><code>c_in</code></strong>:<code>int</code>, <strong><code>c_out</code></strong>:<code>int</code>, <strong><code>seq_len</code></strong>:<code>int</code>, <strong><code>n_layers</code></strong>:<code>int</code>=<em><code>6</code></em>, <strong><code>d_model</code></strong>:<code>int</code>=<em><code>128</code></em>, <strong><code>n_heads</code></strong>:<code>int</code>=<em><code>16</code></em>, <strong><code>d_head</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>act</code></strong>:<code>str</code>=<em><code>'reglu'</code></em>, <strong><code>d_ff</code></strong>:<code>int</code>=<em><code>256</code></em>, <strong><code>dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>attn_dropout</code></strong>:<code>float</code>=<em><code>0</code></em>, <strong><code>drop_path_rate</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>mlp_ratio</code></strong>:<code>int</code>=<em><code>1</code></em>, <strong><code>qkv_bias</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>pre_norm</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>use_token</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>fc_dropout</code></strong>:<code>float</code>=<em><code>0.0</code></em>, <strong><code>bn</code></strong>:<code>bool</code>=<em><code>False</code></em>, <strong><code>y_range</code></strong>:<code>Optional</code>[<code>tuple</code>]=<em><code>None</code></em>, <strong><code>ks</code></strong>:<code>Optional</code>[<code>int</code>]=<em><code>None</code></em>, <strong><code>maxpool</code></strong>:<code>bool</code>=<em><code>True</code></em>, <strong><code>preprocessor</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>custom_head</code></strong>:<code>Optional</code>[<code>Callable</code>]=<em><code>None</code></em>, <strong><code>verbose</code></strong>:<code>bool</code>=<em><code>False</code></em>) :: <a href="/models.TabFusionTransformer.html#Sequential"><code>Sequential</code></a></p>
</blockquote>
<p>Time series transformer model based on ViT (Vision Transformer):</p>
<p>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020).
An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</p>
<p>This implementation is a modified version of Vision Transformer that is part of the grat timm library
(<a href="https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py">https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py</a>)</p>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">TSiTPlus</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Time series transformer model based on ViT (Vision Transformer):</span>

<span class="sd">    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... &amp; Houlsby, N. (2020).</span>
<span class="sd">    An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.</span>

<span class="sd">    This implementation is a modified version of Vision Transformer that is part of the grat timm library</span>
<span class="sd">    (https://github.com/rwightman/pytorch-image-models/blob/72b227dcf57c0c62291673b96bdc06576bb90457/timm/models/vision_transformer.py)</span>
<span class="sd">    &quot;&quot;&quot;</span>


    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">c_in</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">c_out</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">:</span><span class="nb">int</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">d_head</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">act</span><span class="p">:</span><span class="nb">str</span><span class="o">=</span><span class="s1">&#39;reglu&#39;</span><span class="p">,</span>
                 <span class="n">d_ff</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">drop_path_rate</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="p">:</span><span class="nb">int</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">qkv_bias</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">pre_norm</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">use_token</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fc_dropout</span><span class="p">:</span><span class="nb">float</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">bn</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">y_range</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">tuple</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
                 <span class="n">ks</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">maxpool</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">preprocessor</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">custom_head</span><span class="p">:</span><span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">]</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="p">:</span><span class="nb">bool</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">        =====</span>

<span class="sd">        c_in:                   the number of features (aka variables, dimensions, channels) in the time series dataset.</span>
<span class="sd">        c_out:                  the number of target classes.</span>
<span class="sd">        seq_len:                number of time steps in the time series.</span>
<span class="sd">        n_layers:               number of layers (or blocks) in the encoder. Default: 3 (range(1-4))</span>
<span class="sd">        d_model:                total dimension of the model (number of features created by the model). Default: 128 (range(64-512))</span>
<span class="sd">        n_heads:                parallel attention heads. Default:16 (range(8-16)).</span>
<span class="sd">        d_head:                 size of the learned linear projection of queries, keys and values in the MHA. Usual values: 16-512. </span>
<span class="sd">                                Default: None -&gt; (d_model/n_heads) = 32.</span>
<span class="sd">        act:                    the activation function of intermediate layer, relu, gelu, geglu, reglu.</span>
<span class="sd">        d_ff:                   the dimension of the feedforward network model. Default: 512 (range(256-512))</span>
<span class="sd">        dropout:                dropout applied to to the embedded sequence steps after position embeddings have been added and </span>
<span class="sd">                                to the mlp sublayer in the encoder.</span>
<span class="sd">        attn_dropout:         dropout rate applied to the attention sublayer.</span>
<span class="sd">        drop_path_rate:         stochastic depth rate.</span>
<span class="sd">        mlp_ratio:              ratio of mlp hidden dim to embedding dim.</span>
<span class="sd">        qkv_bias:               determines whether bias is applied to the Linear projections of queries, keys and values in the MultiheadAttention</span>
<span class="sd">        pre_norm:               if True normalization will be applied as the first step in the sublayers. Defaults to False.</span>
<span class="sd">        use_token:              if True, the output will come from the transformed token. Otherwise a pooling layer will be applied.</span>
<span class="sd">        fc_dropout:             dropout applied to the final fully connected layer.</span>
<span class="sd">        bn:                     flag that indicates if batchnorm will be applied to the head.</span>
<span class="sd">        y_range:                range of possible y values (used in regression tasks).</span>
<span class="sd">        ks:                     (Optional) kernel sizes that will be applied to a hybrid embedding.</span>
<span class="sd">        maxpool:                If true and kernel sizes are passed, maxpool will also be added to the hybrid embedding.</span>
<span class="sd">        preprocessor:           an optional callable (nn.Conv1d with dilation &gt; 1 or stride &gt; 1 for example) that will be used to preprocess the time series before </span>
<span class="sd">                                the embedding step. It is useful to extract features or resample the time series.</span>
<span class="sd">        custom_head:            custom head that will be applied to the network. It must contain all kwargs (pass a partial function)</span>

<span class="sd">        Input shape:</span>
<span class="sd">            x: bs (batch size) x nvars (aka features, variables, dimensions, channels) x seq_len (aka time steps)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        
        <span class="n">backbone</span> <span class="o">=</span> <span class="n">_TSiTBackbone</span><span class="p">(</span><span class="n">c_in</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">d_head</span><span class="o">=</span><span class="n">d_head</span><span class="p">,</span> <span class="n">act</span><span class="o">=</span><span class="n">act</span><span class="p">,</span>
                                  <span class="n">d_ff</span><span class="o">=</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">attn_dropout</span><span class="o">=</span><span class="n">attn_dropout</span><span class="p">,</span> 
                                  <span class="n">drop_path_rate</span><span class="o">=</span><span class="n">drop_path_rate</span><span class="p">,</span> <span class="n">pre_norm</span><span class="o">=</span><span class="n">pre_norm</span><span class="p">,</span> <span class="n">mlp_ratio</span><span class="o">=</span><span class="n">mlp_ratio</span><span class="p">,</span> <span class="n">use_token</span><span class="o">=</span><span class="n">use_token</span><span class="p">,</span> 
                                  <span class="n">ks</span><span class="o">=</span><span class="n">ks</span><span class="p">,</span> <span class="n">maxpool</span><span class="o">=</span><span class="n">maxpool</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">head_nf</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">c_out</span> <span class="o">=</span> <span class="n">c_out</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span> <span class="o">=</span> <span class="n">seq_len</span>
        <span class="k">if</span> <span class="n">custom_head</span><span class="p">:</span> 
            <span class="n">head</span> <span class="o">=</span> <span class="n">custom_head</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">head_nf</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">seq_len</span><span class="p">)</span> <span class="c1"># custom head passed as a partial func with all its kwargs</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span><span class="n">TokenLayer</span><span class="p">(</span><span class="n">token</span><span class="o">=</span><span class="n">use_token</span><span class="p">)]</span>
            <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">LinBnDrop</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">bn</span><span class="o">=</span><span class="n">bn</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">fc_dropout</span><span class="p">)]</span>
            <span class="k">if</span> <span class="n">y_range</span><span class="p">:</span> <span class="n">layers</span> <span class="o">+=</span> <span class="p">[</span><span class="n">SigmoidRange</span><span class="p">(</span><span class="o">*</span><span class="n">y_range</span><span class="p">)]</span>
            <span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">OrderedDict</span><span class="p">([(</span><span class="s1">&#39;backbone&#39;</span><span class="p">,</span> <span class="n">backbone</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;head&#39;</span><span class="p">,</span> <span class="n">head</span><span class="p">)]))</span>

        
<span class="n">TSiT</span> <span class="o">=</span> <span class="n">TSiTPlus</span>
<span class="n">InceptionTSiTPlus</span> <span class="o">=</span> <span class="n">named_partial</span><span class="p">(</span><span class="s2">&quot;InceptionTSiTPlus&quot;</span><span class="p">,</span> <span class="n">TSiTPlus</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">InceptionBlockPlus</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]))</span>
<span class="n">InceptionTSiT</span> <span class="o">=</span> <span class="n">named_partial</span><span class="p">(</span><span class="s2">&quot;InceptionTSiT&quot;</span><span class="p">,</span> <span class="n">TSiTPlus</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">partial</span><span class="p">(</span><span class="n">InceptionBlockPlus</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">]))</span>
<span class="n">ConvTSiT</span> <span class="o">=</span> <span class="n">named_partial</span><span class="p">(</span><span class="s2">&quot;ConvTSiT&quot;</span><span class="p">,</span> <span class="n">TSiTPlus</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">])</span>
<span class="n">ConvTSiTPlus</span> <span class="o">=</span> <span class="n">named_partial</span><span class="p">(</span><span class="s2">&quot;ConvTSiTPlus&quot;</span><span class="p">,</span> <span class="n">TSiTPlus</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">7</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
<span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TSiTPlus(
  (backbone): _TSiTBackbone(
    (preprocessor): Identity()
    (to_embedding): Sequential(
      (0): Conv1d(4, 128, kernel_size=(1,), stride=(1,))
      (1): Transpose(1, 2)
    )
    (pos_dropout): Dropout(p=0.0, inplace=False)
    (encoder): _TSiTEncoder(
      (layers): ModuleList(
        (0): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (1): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (2): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (3): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (4): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
        (5): ModuleList(
          (0): MultiheadAttention(
            (W_Q): Linear(in_features=128, out_features=128, bias=True)
            (W_K): Linear(in_features=128, out_features=128, bias=True)
            (W_V): Linear(in_features=128, out_features=128, bias=True)
            (sdp_attn): ScaledDotProductAttention()
            (to_out): Sequential(
              (0): Linear(in_features=128, out_features=128, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (2): PositionwiseFeedForward(
            (0): Linear(in_features=128, out_features=256, bias=True)
            (1): ReGLU()
            (2): Dropout(p=0.0, inplace=False)
            (3): Linear(in_features=128, out_features=128, bias=True)
            (4): Dropout(p=0.0, inplace=False)
          )
          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (4): Identity()
        )
      )
      (norm): Identity()
    )
  )
  (head): Sequential(
    (0): TokenLayer()
    (1): LinBnDrop(
      (0): Linear(in_features=128, out_features=2, bias=True)
    )
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">InceptionTSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>[W NNPACK.cpp:79] Could not initialize NNPACK! Reason: Unsupported hardware.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Subsampling">Subsampling<a class="anchor-link" href="#Subsampling"> </a></h3><p>It's a known fact that transformers cannot be directly applied to long sequences. To avoid this, we have included a way to subsample the sequence to generate a more manageable input.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">tsai.data.validation</span> <span class="kn">import</span> <span class="n">get_splits</span>
<span class="kn">from</span> <span class="nn">tsai.data.core</span> <span class="kn">import</span> <span class="n">get_ts_dls</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5000</span><span class="p">))</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">splits</span> <span class="o">=</span> <span class="n">get_splits</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">get_ts_dls</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">splits</span><span class="o">=</span><span class="n">splits</span><span class="p">)</span>
<span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">dls</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">xb</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>TSTensor(samples:8, vars:3, len:5000, device=cpu)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>If you try to use TSiTPlus, it's likely you'll get an 'out-of-memory' error.</p>
<p>To avoid this you can subsample the sequence reducing the input's length. This can be done in multiple ways. Here are a few examples:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">ks</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">Conv1d</span><span class="p">(</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">2</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 2, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Pad1d</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">MaxPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">preprocessor</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">Pad1d</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">AvgPool1d</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">50</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">default_device</span><span class="p">())</span>
<span class="n">preprocessor</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([8, 3, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Once you decide what type of transform you want to apply, you just need to pass the layer as the preprocessor attribute:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">bs</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">nvars</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">seq_len</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">c_out</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">128</span>

<span class="n">xb</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">nvars</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
<span class="n">preprocessor</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">Conv1d</span><span class="p">,</span> <span class="n">ks</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">stride</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;same&#39;</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">xb</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TSiTPlus</span><span class="p">(</span><span class="n">nvars</span><span class="p">,</span> <span class="n">c_out</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">preprocessor</span><span class="o">=</span><span class="n">preprocessor</span><span class="p">)</span>
<span class="n">test_eq</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="p">(</span><span class="n">bs</span><span class="p">,</span> <span class="n">c_out</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

