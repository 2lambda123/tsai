#AUTOGENERATED! DO NOT EDIT! file to edit: ./TSCallbacks.ipynb (unless otherwise specified)

from fastai.torch_core import *
from fastai.callback import *
#from fastai.callbacks.mixup import MixUpLoss
from fastai.basic_train import Learner, LearnerCallback

class MixUpCallback1D(LearnerCallback):
    "Callback that creates the mixed-up input and target."
    def __init__(self, learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True):
        super().__init__(learn)
        self.alpha,self.stack_x,self.stack_y = alpha,stack_x,stack_y

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = MixUpLoss1D(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies mixup to `last_input` and `last_target` if `train`."
        if not train: return

        lambd = np.random.beta(self.alpha, self.alpha, last_target.size(0))
        lambd = np.concatenate([lambd[:,None], 1-lambd[:,None]], 1).max(1)
        lambd = last_input.new(lambd)
        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)
        x1, y1 = last_input[shuffle], last_target[shuffle]

        if self.stack_x:
            new_input = [last_input, last_input[shuffle], lambd]
        else:
            if last_input.ndim == 4:
                new_input = (last_input * lambd.view(lambd.size(0),1,1,1) + x1 * (1-lambd).view(lambd.size(0),1,1,1))
            elif last_input.ndim == 3:
                new_input = (last_input * lambd.view(lambd.size(0),1,1) + x1 * (1-lambd).view(lambd.size(0),1,1))

        if self.stack_y:
            new_target = torch.cat([last_target[:,None].float(), y1[:,None].float(), lambd[:,None].float()], 1)
        else:
            if len(last_target.shape) == 2:
                lambd = lambd.unsqueeze(1).float()
            new_target = last_target.float() * lambd + y1.float() * (1-lambd)

        return {'last_input': new_input, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()


class MixUpLoss1D(nn.Module):
    "Adapt the loss function `crit` to go with mixup."

    def __init__(self, crit, reduction='mean'):
        super().__init__()
        if hasattr(crit, 'reduction'):
            self.crit = crit
            self.old_red = crit.reduction
            setattr(self.crit, 'reduction', 'none')
        else:
            self.crit = partial(crit, reduction='none')
            self.old_crit = crit
        self.reduction = reduction

    def forward(self, output, target):
        if len(target.size()) == 2:
            loss1, loss2 = self.crit(output,target[:,0].long()), self.crit(output,target[:,1].long())
            d = (loss1 * target[:,2] + loss2 * (1-target[:,2])).mean()
        else:  d = self.crit(output, target)
        if self.reduction == 'mean': return d.mean()
        elif self.reduction == 'sum':            return d.sum()
        return d

    def get_old(self):
        if hasattr(self, 'old_crit'):  return self.old_crit
        elif hasattr(self, 'old_red'):
            setattr(self.crit, 'reduction', self.old_red)
            return self.crit


def mixup1D(learn:Learner, alpha:float=0.4, stack_x:bool=False, stack_y:bool=True) -> Learner:
    "Add mixup https://arxiv.org/abs/1710.09412 to `learn`."
    learn.callback_fns.append(partial(MixUpCallback1D, alpha=alpha, stack_x=stack_x, stack_y=stack_y))
    return learn

Learner.mixup1D = mixup1D


from fastai.torch_core import *
from fastai.callback import *
from fastai.callbacks.mixup import MixUpLoss
from fastai.basic_train import Learner, LearnerCallback

class CutXCallback1D(LearnerCallback):
    "Callback that creates the cut mixed-up input and target."
    def __init__(self, learn:Learner, alpha:float=1., stack_y:bool=True, mix:bool=True,
                 ch:bool=False, α:float=.125, true_λ:bool=True):
        super().__init__(learn)

        self.alpha,self.stack_y,self.mix,self.ch,self.α,self.true_λ = alpha,stack_y,mix,ch,α,true_λ

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies mixup to `last_input` and `last_target` if `train`."
        if not train: return
        last_input_ndim = last_input.ndim
        λ = np.random.beta(self.alpha, self.alpha)
        λ = max(λ, 1- λ)
        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)
        x1, y1 = last_input[shuffle], last_target[shuffle]
        bbx1, bby1, bbx2, bby2 = rand_bbox2(last_input.size(), λ, last_input_ndim)
        new_input = last_input.clone()
        λ = last_input.new([λ])
        if self.ch:
            all_samples = tuple(np.arange(last_input.shape[0]))
            if shuffle.is_cuda: shuffle = tuple(shuffle.cpu().numpy())
            else: shuffle = tuple(shuffle.numpy())
            input_ch = new_input.shape[-2]
            out_ch = tuple(np.random.choice(np.arange(input_ch),
                                            int(np.random.beta(self.α, 1) * input_ch),
                                            replace=False))
            subseq = tuple(np.arange(last_input.shape[-1])[bbx1:bbx2])
        if self.mix:
            if self.ch:
                new_input[np.ix_(all_samples, out_ch, subseq)] = last_input[np.ix_(shuffle, out_ch, subseq)]
            else:
                new_input[:, ..., bby1:bby2, bbx1:bbx2] = last_input[shuffle, ..., bby1:bby2, bbx1:bbx2]
        else:
            if self.ch:
                new_input[np.ix_(all_samples, out_ch, subseq)] = 0
            else:
                new_input[:, ..., bby1:bby2, bbx1:bbx2] = 0

        if self.mix:
            if self.true_λ:
                if self.ch:
                    λ_ = 1 - ((bbx2 - bbx1) * len(out_ch) / (last_input.size(-1) * last_input.size(-2)))
                else:
                    λ_ = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (last_input.size(-1) * last_input.size(-2)))
                λ = last_input.new([λ_])

            else: λ = last_input.new([λ])


            if self.stack_y:
                new_target = torch.cat([last_target[:,None].float(), y1[:,None].float(),
                                    λ.repeat(last_input.shape[0])[:,None].float()], 1)
            else:
                if len(last_target.shape) == 2:
                    λ = λ.unsqueeze(1).float()
                new_target = last_target.float() * λ + y1.float() * (1-λ)
        else:
            new_target = last_target

        return {'last_input': new_input, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()


def rand_bbox2(last_input_size, λ, ndim):
    '''lambd is always between .5 and 1'''

    W = last_input_size[-1]
    H = last_input_size[-2]
    if ndim == 4:
        cut_rat = np.sqrt(1. - λ) # 0. - .707
        cut_w = np.int(W * cut_rat)
        cut_h = np.int(H * cut_rat)

        # uniform
        cx = np.random.randint(W)
        cy = np.random.randint(H)

        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = np.clip(cy - cut_h // 2, 0, H)
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = np.clip(cy + cut_h // 2, 0, H)

    elif ndim == 3:
        cut_rat = (1. - λ) # 0. - .5
        cut_w = np.int(W * cut_rat)

        # uniform
        cx = np.random.randint(W)

        bbx1 = np.clip(cx - cut_w // 2, 0, W)
        bby1 = 0
        bbx2 = np.clip(cx + cut_w // 2, 0, W)
        bby2 = H

    return bbx1, bby1, bbx2, bby2


def cutmix1D(learn:Learner, alpha:float=1., stack_y:bool=True, true_λ:bool=True) -> Learner:
    "Add mixup https://arxiv.org/abs/1710.09412 to `learn`."
    learn.callback_fns.append(partial(CutXCallback1D, alpha=alpha, stack_y=stack_y, mix=True, true_λ=true_λ))
    return learn

def cutout1D(learn:Learner, alpha:float=1., stack_y:bool=True, true_λ:bool=True) -> Learner:
    "Add mixup https://arxiv.org/abs/1710.09412 to `learn`."
    learn.callback_fns.append(partial(CutXCallback1D, alpha=alpha, stack_y=stack_y, mix=False, true_λ=true_λ))
    return learn

def channelcutmix1D(learn:Learner, alpha:float=1., stack_y:bool=True, ch:bool=True, α:float=.125, true_λ:bool=True) -> Learner:
    "Add mixup https://arxiv.org/abs/1710.09412 to `learn`."
    learn.callback_fns.append(partial(CutXCallback1D, alpha=alpha, stack_y=stack_y, mix=True, ch=ch, α=α, true_λ=true_λ))
    return learn

def channelcutout1D(learn:Learner, alpha:float=1., stack_y:bool=True, ch:bool=True, α:float=.125, true_λ:bool=True) -> Learner:
    "Add mixup https://arxiv.org/abs/1710.09412 to `learn`."
    learn.callback_fns.append(partial(CutXCallback1D, alpha=alpha, stack_y=stack_y, mix=False, ch=ch, α=α, true_λ=true_λ))
    return learn

Learner.cutmix1D = cutmix1D
Learner.cutout1D = cutout1D
Learner.chcutmix1D = channelcutmix1D
Learner.chcutout1D = channelcutout1D


from fastai.torch_core import *
from fastai.callback import *
from fastai.callbacks.mixup import MixUpLoss
from fastai.basic_train import Learner, LearnerCallback

class CutMixCallback(LearnerCallback):
    "Callback that creates the cut mixed-up input and target."
    def __init__(self, learn:Learner, α:float=1., stack_y:bool=True, true_λ:bool=True):
        super().__init__(learn)
        self.α,self.stack_y,self.true_λ = α,stack_y,true_λ

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = MixUpLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies cutmix to `last_input` and `last_target` if `train`."
        if not train: return
        λ = np.random.beta(self.α, self.α)
        λ = max(λ, 1- λ)
        shuffle = torch.randperm(last_target.size(0)).to(last_input.device)
        x1, y1 = last_input[shuffle], last_target[shuffle]
        #Get new input
        last_input_size = last_input.shape
        bbx1, bby1, bbx2, bby2 = rand_bbox(last_input.size(), λ)
        new_input = last_input.clone()
        new_input[:, ..., bby1:bby2, bbx1:bbx2] = last_input[shuffle, ..., bby1:bby2, bbx1:bbx2]
        λ = last_input.new([λ])
        if self.true_λ:
            λ = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (last_input_size[-1] * last_input_size[-2]))
            λ = last_input.new([λ])
        if self.stack_y:
            new_target = torch.cat([last_target.unsqueeze(1).float(), y1.unsqueeze(1).float(),
                                    λ.repeat(last_input_size[0]).unsqueeze(1).float()], 1)
        else:
            if len(last_target.shape) == 2:
                λ = λ.unsqueeze(1).float()
            new_target = last_target.float() * λ + y1.float() * (1-λ)
        return {'last_input': new_input, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()


def rand_bbox(last_input_size, λ):
    '''lambd is always between .5 and 1'''

    W = last_input_size[-1]
    H = last_input_size[-2]
    cut_rat = np.sqrt(1. - λ) # 0. - .707
    cut_w = np.int(W * cut_rat)
    cut_h = np.int(H * cut_rat)

    # uniform
    cx = np.random.randint(W)
    cy = np.random.randint(H)

    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)

    return bbx1, bby1, bbx2, bby2


def cutmix(learn:Learner, α:float=1., stack_x:bool=False, stack_y:bool=True, true_λ:bool=True) -> Learner:
    "Add mixup https://arxiv.org/pdf/1905.04899.pdf to `learn`."
    learn.callback_fns.append(partial(CutMixCallback, α=α, stack_y=stack_y, true_λ=true_λ))
    return learn

Learner.cutmix = cutmix


from fastai.torch_core import *
from fastai.callback import *
from fastai.basic_train import Learner, LearnerCallback

class RicapLoss(nn.Module):
    "Adapt the loss function `crit` to go with ricap data augmentations."

    def __init__(self, crit, reduction='mean'):
        super().__init__()
        if hasattr(crit, 'reduction'):
            self.crit = crit
            self.old_red = crit.reduction
            setattr(self.crit, 'reduction', 'none')
        else:
            self.crit = partial(crit, reduction='none')
            self.old_crit = crit
        self.reduction = reduction

    def forward(self, output, target):
        if target.ndim == 2:
            c_ = target[:, 1:5]
            W_ = target[:, 5:]
            loss = [W_[:, k] * self.crit(output, c_[:, k].long()) for k in range(4)]
            d = torch.mean(torch.stack(loss))
        else: d = self.crit(output, target)
        if self.reduction == 'mean': return d.mean()
        elif self.reduction == 'sum': return d.sum()
        return d

    def get_old(self):
        if hasattr(self, 'old_crit'): return self.old_crit
        elif hasattr(self, 'old_red'):
            setattr(self.crit, 'reduction', self.old_red)
            return self.crit

class RicapCallback(LearnerCallback):
    "Callback that creates the ricap input and target."
    def __init__(self, learn:Learner, β:float=.3, stack_y:bool=True):
        super().__init__(learn)
        self.β,self.stack_y = β,stack_y

    def on_train_begin(self, **kwargs):
        if self.stack_y: self.learn.loss_func = RicapLoss(self.learn.loss_func)

    def on_batch_begin(self, last_input, last_target, train, **kwargs):
        "Applies ricap to `last_input` and `last_target` if `train`."
        if not train: return
        I_x, I_y = last_input.size()[2:]
        w = int(np.round(I_x * np.random.beta(self.β, self.β)))
        h = int(np.round(I_y * np.random.beta(self.β, self.β)))
        w_ = [w, I_x - w, w, I_x - w]
        h_ = [h, h, I_y - h, I_y - h]
        cropped_images = {}
        bs = last_input.size(0)
        c_ = torch.zeros((bs, 4)).float().to(last_input.device)
        W_ = torch.zeros(4).float().to(last_input.device)
        for k in range(4):
            idx = torch.randperm(bs)
            x_k = np.random.randint(0, I_x - w_[k] + 1)
            y_k = np.random.randint(0, I_y - h_[k] + 1)
            cropped_images[k] = last_input[idx][:, :, x_k:x_k + w_[k], y_k:y_k + h_[k]]
            c_[:, k] = last_target[idx].float()
            W_[k] = w_[k] * h_[k] / (I_x * I_y)
        patched_images = torch.cat(
            (torch.cat((cropped_images[0], cropped_images[1]), 2),
             torch.cat((cropped_images[2], cropped_images[3]), 2)), 3)  #.cuda()
        if self.stack_y:
                new_target = torch.cat((last_target[:,None].float(), c_,
                                        W_[None].repeat(last_target.size(0), 1)), dim=1)
        else:
            new_target = c_ * W_
        return {'last_input': patched_images, 'last_target': new_target}

    def on_train_end(self, **kwargs):
        if self.stack_y: self.learn.loss_func = self.learn.loss_func.get_old()


def ricap(learn:Learner, stack_y:bool=True) -> Learner:
    "Add ricap https://arxiv.org/pdf/1811.09030.pdf to `learn`."
    learn.callback_fns.append(partial(RicapCallback, stack_y=stack_y))
    return learn

Learner.ricap = ricap

from torch.utils.data.sampler import WeightedRandomSampler


class OverSamplingCallback(LearnerCallback):
    def __init__(self, learn: Learner):
        super().__init__(learn)
        self.labels = self.learn.data.train_dl.dataset.y.items
        _, counts = np.unique(self.labels, return_counts=True)
        self.weights = torch.DoubleTensor((1 / counts)[self.labels])
        self.old_batch_sampler = self.learn.data.train_dl.dl.batch_sampler

    def on_train_begin(self, **kwargs):
        self.learn.data.train_dl.dl.batch_sampler = BatchSampler(
            WeightedRandomSampler(self.weights,
                                  len(self.learn.data.train_dl.dataset)),
            self.learn.data.train_dl.batch_size, False)

    def on_train_end(self, **kwargs):
        self.learn.data.train_dl.dl.batch_sampler = self.old_batch_sampler


def oversampling(learn: Learner) -> Learner:
    learn.callback_fns.append(OverSamplingCallback)
    return learn


Learner.oversampling = oversampling


from fastai.callbacks.tracker import TrackerCallback
class ReduceLROnPlateau(TrackerCallback):
    "A `TrackerCallback` that reduces learning rate when a metric has stopped improving."
    def __init__(self, learn:Learner, monitor:str='valid_loss', mode:str='auto',
                 patience:int=0, factor:float=0.2, min_delta:int=0, min_lr:float=1e-6, verbose=False):
        super().__init__(learn, monitor=monitor, mode=mode)
        self.patience,self.factor,self.min_delta,self.min_lr,self.verbose = patience,factor,min_delta,min_lr,verbose
        if self.operator == np.less:  self.min_delta *= -1

    def on_train_begin(self, **kwargs:Any)->None:
        "Initialize inner arguments."
        self.wait, self.opt = 0, self.learn.opt
        super().on_train_begin(**kwargs)

    def on_epoch_end(self, epoch, **kwargs:Any)->None:
        "Compare the value monitored to its best and maybe reduce lr."
        current = self.get_monitor_value()
        if current is None: return
        if self.operator(current - self.min_delta, self.best): self.best,self.wait = current,0
        elif self.opt.lr > self.min_lr:
            self.wait += 1
            if self.wait > self.patience:
                self.opt.lr = max(self.min_lr, self.opt.lr * self.factor)
                self.wait = 0
                if self.verbose: print(f'Epoch {epoch}: reducing lr to {self.opt.lr}')


def reduce_lr_on_plateau(learn: Learner, monitor='valid_loss', mode='auto',
                         patience=0, factor=0.2, min_delta=0, min_lr:float=1e-6, verbose=False) -> Learner:

    learn.callback_fns.append(
        partial(ReduceLROnPlateau, monitor=monitor, mode=mode,
                patience=patience, factor=factor, min_delta=min_delta, min_lr=min_lr, verbose=verbose))
    return learn

Learner.reduce_lr_on_plateau = reduce_lr_on_plateau