# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/124_models.TSTransformerPlus.ipynb (unless otherwise specified).

__all__ = ['TSTransformerPlus']

# Cell
from ..imports import *
from .layers import *
from typing import Callable

# Cell

class _TransformerEncoder(nn.Module):
    def __init__(self, d_model, n_heads, n_layers:int=6, encoder_dropout:float=0., act:str='reglu', pre_norm:bool=False):
        super().__init__()
        self.layers = nn.ModuleList([])
        for _ in range(n_layers):
            self.layers.append(nn.ModuleList([
                MultiheadAttention(d_model, n_heads, dropout=encoder_dropout), nn.LayerNorm(d_model),
                PositionwiseFeedForward(d_model, dropout=encoder_dropout, act=act), nn.LayerNorm(d_model),
            ]))
        self.pre_norm = pre_norm

    def forward(self, x):
        for i, (mha, attn_norm, pwff, ff_norm) in enumerate(self.layers):
            if self.pre_norm:
                x = mha(attn_norm(x))[0] + x
                x = pwff(ff_norm(x)) + x
            else:
                x = attn_norm(mha(x)[0] + x)
                x = ff_norm(pwff(x) + x)
        return x


class _TSTransformerBackbone(Module):
    def __init__(self, c_in:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',
                 d_ff:int=256, emb_dropout:float=0., encoder_dropout:float=0., pre_norm:bool=False, pct_random_steps:float=1., use_cls_token:bool=True,
                 custom_subsampling:Optional[Callable]=None, verbose:bool=True):

        self.pct_random_steps = pct_random_steps
        self.input_layer = custom_subsampling
        if custom_subsampling is not None:
            xb = torch.randn(1, c_in, seq_len).to(default_device())
            ori_c_in, ori_seq_len = c_in, seq_len
            c_in, seq_len = custom_subsampling.to(default_device())(xb).shape[1:]
            del xb
            pv(f'custom_subsampling: (?, {ori_c_in}, {ori_seq_len}) --> (?, {c_in}, {seq_len})', verbose=verbose)
        self.to_embedding = nn.Sequential(Transpose(1,2), nn.Linear(c_in, d_model))
        self.pos_embedding = nn.Parameter(torch.zeros(1, seq_len + use_cls_token, d_model))
        self.emb_dropout = nn.Dropout(emb_dropout)
        self.cls_token = nn.Parameter(torch.zeros(1, 1, d_model)) if use_cls_token else None

        self.encoder = _TransformerEncoder(d_model, n_heads, n_layers=n_layers, encoder_dropout=encoder_dropout, act=act, pre_norm=pre_norm)

    def forward(self, x):

        # apply custom_input_layer (if available)
        if self.input_layer is not None:
            x = self.input_layer(x)

        B, _, S = x.shape

        x = self.to_embedding(x)
        if self.training and self.pct_random_steps < 1.:
            idxs = np.tile(np.random.choice(S, round(S * self.pct_random_steps), False), math.ceil(1 / self.pct_random_steps))[:S]
            x = x[:, idxs]
        if self.cls_token is not None:
            x = torch.cat((self.cls_token.repeat(B, 1, 1), x), dim=1)
        x += self.pos_embedding
        x = self.emb_dropout(x)

        x = self.encoder(x)

        x = x.transpose(1,2)
        return x


class TSTransformerPlus(nn.Sequential):
    """Time series transformer model based on ViT (Vision Transformer):
    Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2020).
    An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.

        Args:
            c_in: the number of features (aka variables, dimensions, channels) in the time series dataset.
            c_out: the number of target classes.
            seq_len: number of time steps in the time series.
            n_layers: number of layers (or blocks) in the encoder. Default: 3 (range(1-4))
            d_model: total dimension of the model (number of features created by the model). Default: 128 (range(64-512))
            n_heads:  parallel attention heads. Default:16 (range(8-16)).
            d_head: size of the learned linear projection of queries, keys and values in the MHA. Usual values: 16-512. Default: None -> (d_model/n_heads) = 32.
            d_ff: the dimension of the feedforward network model. Default: 512 (range(256-512))
            act: the activation function of intermediate layer, relu, gelu, geglu, reglu.
            pre_norm: if True normalization will be applied as the first step in the sublayers. Defaults to False
            emb_dropout: dropout applied to to the embedded sequence steps.
            encoder_dropout: dropout applied to the encoder (MultheadAttention and PositionwiseFeedForward layers).
            bn: indicates if batchnorm will be applied to the head.
            fc_dropout: dropout applied to the final fully connected layer.
            y_range: range of possible y values (used in regression tasks).
            custom_head: custom head that will be applied to the network. It must contain all kwargs (pass a partial function)

        Input shape:
            x: bs (batch size) x nvars (aka features, variables, dimensions, channels) x seq_len (aka time steps)
    """


    def __init__(self, c_in:int, c_out:int, seq_len:int, n_layers:int=6, d_model:int=128, n_heads:int=16, d_head:Optional[int]=None, act:str='reglu',
                 d_ff:int=256, emb_dropout:float=0., encoder_dropout:float=0., pre_norm:bool=False, use_cls_token:bool=True, pct_random_steps:float=1.,
                 fc_dropout:float=0., bn:bool=True, y_range:Optional[tuple]=None, custom_subsampling:Optional[Callable]=None,
                 custom_head:Optional[Callable]=None, verbose:bool=True):

        backbone = _TSTransformerBackbone(c_in, seq_len, n_layers=n_layers, d_model=d_model, n_heads=n_heads, d_head=d_head, act=act,
                                          d_ff=d_ff, emb_dropout=emb_dropout, encoder_dropout=encoder_dropout,
                                          pre_norm=pre_norm, pct_random_steps=pct_random_steps, use_cls_token=use_cls_token,
                                          custom_subsampling=custom_subsampling, verbose=verbose)

        self.head_nf = d_model
        self.c_out = c_out
        self.seq_len = seq_len
        if custom_head: head = custom_head(self.head_nf, c_out, self.seq_len) # custom head passed as a partial func with all its kwargs
        else:
            layers = [TokenLayer(token=use_cls_token)]
            layers += [LinBnDrop(d_model, c_out, bn=bn, p=fc_dropout)]
            if y_range: layers += [SigmoidRange(*y_range)]
        super().__init__(OrderedDict([('backbone', backbone), ('head', nn.Sequential(*layers))]))